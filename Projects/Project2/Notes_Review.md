### Fully-connected layer and output layer

- output layer only have `linear activation``
- fully connected layer and other convolution layers should have a  `non-linear activation`.

### Further Reading

- [RELU activation](https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning)
- [Gradiant Vanish problem](https://www.quora.com/What-is-the-vanishing-gradient-problem)
- **[Good practical details in CNN](http://cs231n.github.io/neural-networks-1/)**

### Tensorflow document

- [flatten](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)
- [fully connection](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)
    - default: ReLU activation