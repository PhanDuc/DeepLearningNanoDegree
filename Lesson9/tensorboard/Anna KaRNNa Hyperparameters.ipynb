{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70, 81, 13,  3, 76, 40, 55, 58, 74, 60, 60, 60, 14, 13,  3,  3, 47,\n",
       "       58, 71, 13, 45, 52, 75, 52, 40, 79, 58, 13, 55, 40, 58, 13, 75, 75,\n",
       "       58, 13, 75, 52,  5, 40, 78, 58, 40, 51, 40, 55, 47, 58, 22, 61, 81,\n",
       "       13,  3,  3, 47, 58, 71, 13, 45, 52, 75, 47, 58, 52, 79, 58, 22, 61,\n",
       "       81, 13,  3,  3, 47, 58, 52, 61, 58, 52, 76, 79, 58,  8, 32, 61, 60,\n",
       "       32, 13, 47, 64, 60, 60,  2, 51, 40, 55, 47, 76, 81, 52, 61], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 81, 13,  3, 76, 40, 55, 58, 74, 60],\n",
       "       [16, 61, 50, 58, 81, 40, 58, 45,  8, 51],\n",
       "       [58, 80, 13, 76, 80, 81, 52, 61, 41, 58],\n",
       "       [ 8, 76, 81, 40, 55, 58, 32,  8, 22, 75],\n",
       "       [58, 76, 81, 40, 58, 75, 13, 61, 50, 82],\n",
       "       [58, 29, 81, 55,  8, 22, 41, 81, 58, 75],\n",
       "       [76, 58, 76,  8, 60, 50,  8, 64, 60, 60],\n",
       "       [ 8, 58, 81, 40, 55, 79, 40, 75, 71, 24],\n",
       "       [81, 13, 76, 58, 52, 79, 58, 76, 81, 40],\n",
       "       [40, 55, 79, 40, 75, 71, 58, 13, 61, 50]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, file_writer):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3  Iteration 1/534 Training loss: 4.4137 2.3591 sec/batch\n",
      "Epoch 1/3  Iteration 2/534 Training loss: 4.4020 1.3317 sec/batch\n",
      "Epoch 1/3  Iteration 3/534 Training loss: 4.3884 1.2256 sec/batch\n",
      "Epoch 1/3  Iteration 4/534 Training loss: 4.3692 1.5423 sec/batch\n",
      "Epoch 1/3  Iteration 5/534 Training loss: 4.3313 1.0276 sec/batch\n",
      "Epoch 1/3  Iteration 6/534 Training loss: 4.2513 0.6485 sec/batch\n",
      "Epoch 1/3  Iteration 7/534 Training loss: 4.1613 0.6037 sec/batch\n",
      "Epoch 1/3  Iteration 8/534 Training loss: 4.0809 0.6714 sec/batch\n",
      "Epoch 1/3  Iteration 9/534 Training loss: 4.0111 0.7136 sec/batch\n",
      "Epoch 1/3  Iteration 10/534 Training loss: 3.9499 0.5657 sec/batch\n",
      "Epoch 1/3  Iteration 11/534 Training loss: 3.8943 0.5690 sec/batch\n",
      "Epoch 1/3  Iteration 12/534 Training loss: 3.8480 0.7104 sec/batch\n",
      "Epoch 1/3  Iteration 13/534 Training loss: 3.8074 0.5345 sec/batch\n",
      "Epoch 1/3  Iteration 14/534 Training loss: 3.7726 0.7056 sec/batch\n",
      "Epoch 1/3  Iteration 15/534 Training loss: 3.7415 0.6787 sec/batch\n",
      "Epoch 1/3  Iteration 16/534 Training loss: 3.7136 0.6766 sec/batch\n",
      "Epoch 1/3  Iteration 17/534 Training loss: 3.6880 0.7965 sec/batch\n",
      "Epoch 1/3  Iteration 18/534 Training loss: 3.6663 0.7451 sec/batch\n",
      "Epoch 1/3  Iteration 19/534 Training loss: 3.6456 0.8781 sec/batch\n",
      "Epoch 1/3  Iteration 20/534 Training loss: 3.6248 0.5792 sec/batch\n",
      "Epoch 1/3  Iteration 21/534 Training loss: 3.6059 0.5586 sec/batch\n",
      "Epoch 1/3  Iteration 22/534 Training loss: 3.5886 0.5330 sec/batch\n",
      "Epoch 1/3  Iteration 23/534 Training loss: 3.5717 0.5461 sec/batch\n",
      "Epoch 1/3  Iteration 24/534 Training loss: 3.5566 0.5744 sec/batch\n",
      "Epoch 1/3  Iteration 25/534 Training loss: 3.5421 0.5216 sec/batch\n",
      "Epoch 1/3  Iteration 26/534 Training loss: 3.5289 0.5346 sec/batch\n",
      "Epoch 1/3  Iteration 27/534 Training loss: 3.5169 0.5832 sec/batch\n",
      "Epoch 1/3  Iteration 28/534 Training loss: 3.5047 0.5990 sec/batch\n",
      "Epoch 1/3  Iteration 29/534 Training loss: 3.4936 0.5982 sec/batch\n",
      "Epoch 1/3  Iteration 30/534 Training loss: 3.4834 0.6598 sec/batch\n",
      "Epoch 1/3  Iteration 31/534 Training loss: 3.4742 0.6559 sec/batch\n",
      "Epoch 1/3  Iteration 32/534 Training loss: 3.4642 0.6481 sec/batch\n",
      "Epoch 1/3  Iteration 33/534 Training loss: 3.4547 0.7017 sec/batch\n",
      "Epoch 1/3  Iteration 34/534 Training loss: 3.4463 0.7038 sec/batch\n",
      "Epoch 1/3  Iteration 35/534 Training loss: 3.4376 0.6894 sec/batch\n",
      "Epoch 1/3  Iteration 36/534 Training loss: 3.4302 0.5286 sec/batch\n",
      "Epoch 1/3  Iteration 37/534 Training loss: 3.4220 0.5097 sec/batch\n",
      "Epoch 1/3  Iteration 38/534 Training loss: 3.4145 0.5354 sec/batch\n",
      "Epoch 1/3  Iteration 39/534 Training loss: 3.4071 0.6329 sec/batch\n",
      "Epoch 1/3  Iteration 40/534 Training loss: 3.4001 0.5501 sec/batch\n",
      "Epoch 1/3  Iteration 41/534 Training loss: 3.3934 0.5626 sec/batch\n",
      "Epoch 1/3  Iteration 42/534 Training loss: 3.3870 0.5862 sec/batch\n",
      "Epoch 1/3  Iteration 43/534 Training loss: 3.3807 0.5729 sec/batch\n",
      "Epoch 1/3  Iteration 44/534 Training loss: 3.3749 0.5608 sec/batch\n",
      "Epoch 1/3  Iteration 45/534 Training loss: 3.3691 0.5898 sec/batch\n",
      "Epoch 1/3  Iteration 46/534 Training loss: 3.3639 0.5800 sec/batch\n",
      "Epoch 1/3  Iteration 47/534 Training loss: 3.3590 0.5716 sec/batch\n",
      "Epoch 1/3  Iteration 48/534 Training loss: 3.3542 0.5075 sec/batch\n",
      "Epoch 1/3  Iteration 49/534 Training loss: 3.3496 0.6210 sec/batch\n",
      "Epoch 1/3  Iteration 50/534 Training loss: 3.3452 0.5613 sec/batch\n",
      "Epoch 1/3  Iteration 51/534 Training loss: 3.3406 0.8352 sec/batch\n",
      "Epoch 1/3  Iteration 52/534 Training loss: 3.3362 0.9755 sec/batch\n",
      "Epoch 1/3  Iteration 53/534 Training loss: 3.3319 0.6919 sec/batch\n",
      "Epoch 1/3  Iteration 54/534 Training loss: 3.3276 0.6588 sec/batch\n",
      "Epoch 1/3  Iteration 55/534 Training loss: 3.3236 0.8217 sec/batch\n",
      "Epoch 1/3  Iteration 56/534 Training loss: 3.3194 0.9113 sec/batch\n",
      "Epoch 1/3  Iteration 57/534 Training loss: 3.3155 0.8235 sec/batch\n",
      "Epoch 1/3  Iteration 58/534 Training loss: 3.3118 0.5417 sec/batch\n",
      "Epoch 1/3  Iteration 59/534 Training loss: 3.3081 0.6740 sec/batch\n",
      "Epoch 1/3  Iteration 60/534 Training loss: 3.3045 0.6450 sec/batch\n",
      "Epoch 1/3  Iteration 61/534 Training loss: 3.3011 0.7839 sec/batch\n",
      "Epoch 1/3  Iteration 62/534 Training loss: 3.2979 0.7803 sec/batch\n",
      "Epoch 1/3  Iteration 63/534 Training loss: 3.2950 0.7011 sec/batch\n",
      "Epoch 1/3  Iteration 64/534 Training loss: 3.2914 0.6051 sec/batch\n",
      "Epoch 1/3  Iteration 65/534 Training loss: 3.2879 1.0962 sec/batch\n",
      "Epoch 1/3  Iteration 66/534 Training loss: 3.2851 1.1429 sec/batch\n",
      "Epoch 1/3  Iteration 67/534 Training loss: 3.2821 1.1209 sec/batch\n",
      "Epoch 1/3  Iteration 68/534 Training loss: 3.2785 0.9579 sec/batch\n",
      "Epoch 1/3  Iteration 69/534 Training loss: 3.2753 0.8137 sec/batch\n",
      "Epoch 1/3  Iteration 70/534 Training loss: 3.2725 0.9875 sec/batch\n",
      "Epoch 1/3  Iteration 71/534 Training loss: 3.2695 1.0878 sec/batch\n",
      "Epoch 1/3  Iteration 72/534 Training loss: 3.2669 0.7496 sec/batch\n",
      "Epoch 1/3  Iteration 73/534 Training loss: 3.2641 0.7699 sec/batch\n",
      "Epoch 1/3  Iteration 74/534 Training loss: 3.2613 0.9673 sec/batch\n",
      "Epoch 1/3  Iteration 75/534 Training loss: 3.2587 0.7831 sec/batch\n",
      "Epoch 1/3  Iteration 76/534 Training loss: 3.2563 0.8509 sec/batch\n",
      "Epoch 1/3  Iteration 77/534 Training loss: 3.2536 0.8774 sec/batch\n",
      "Epoch 1/3  Iteration 78/534 Training loss: 3.2510 0.8950 sec/batch\n",
      "Epoch 1/3  Iteration 79/534 Training loss: 3.2484 0.8448 sec/batch\n",
      "Epoch 1/3  Iteration 80/534 Training loss: 3.2456 0.6621 sec/batch\n",
      "Epoch 1/3  Iteration 81/534 Training loss: 3.2430 0.6266 sec/batch\n",
      "Epoch 1/3  Iteration 82/534 Training loss: 3.2406 0.6075 sec/batch\n",
      "Epoch 1/3  Iteration 83/534 Training loss: 3.2382 0.8094 sec/batch\n",
      "Epoch 1/3  Iteration 84/534 Training loss: 3.2357 0.6797 sec/batch\n",
      "Epoch 1/3  Iteration 85/534 Training loss: 3.2330 0.8680 sec/batch\n",
      "Epoch 1/3  Iteration 86/534 Training loss: 3.2304 0.8087 sec/batch\n",
      "Epoch 1/3  Iteration 87/534 Training loss: 3.2278 0.5954 sec/batch\n",
      "Epoch 1/3  Iteration 88/534 Training loss: 3.2253 0.6715 sec/batch\n",
      "Epoch 1/3  Iteration 89/534 Training loss: 3.2229 0.7895 sec/batch\n",
      "Epoch 1/3  Iteration 90/534 Training loss: 3.2206 0.6471 sec/batch\n",
      "Epoch 1/3  Iteration 91/534 Training loss: 3.2183 0.7834 sec/batch\n",
      "Epoch 1/3  Iteration 92/534 Training loss: 3.2158 0.9156 sec/batch\n",
      "Epoch 1/3  Iteration 93/534 Training loss: 3.2134 0.5982 sec/batch\n",
      "Epoch 1/3  Iteration 94/534 Training loss: 3.2110 0.6652 sec/batch\n",
      "Epoch 1/3  Iteration 95/534 Training loss: 3.2085 0.9184 sec/batch\n",
      "Epoch 1/3  Iteration 96/534 Training loss: 3.2059 0.8050 sec/batch\n",
      "Epoch 1/3  Iteration 97/534 Training loss: 3.2035 0.5949 sec/batch\n",
      "Epoch 1/3  Iteration 98/534 Training loss: 3.2010 0.6032 sec/batch\n",
      "Epoch 1/3  Iteration 99/534 Training loss: 3.1986 0.6694 sec/batch\n",
      "Epoch 1/3  Iteration 100/534 Training loss: 3.1961 0.4882 sec/batch\n",
      "Epoch 1/3  Iteration 101/534 Training loss: 3.1938 0.5900 sec/batch\n",
      "Epoch 1/3  Iteration 102/534 Training loss: 3.1913 0.6005 sec/batch\n",
      "Epoch 1/3  Iteration 103/534 Training loss: 3.1888 0.5766 sec/batch\n",
      "Epoch 1/3  Iteration 104/534 Training loss: 3.1863 0.6323 sec/batch\n",
      "Epoch 1/3  Iteration 105/534 Training loss: 3.1838 0.8731 sec/batch\n",
      "Epoch 1/3  Iteration 106/534 Training loss: 3.1813 0.7788 sec/batch\n",
      "Epoch 1/3  Iteration 107/534 Training loss: 3.1787 0.6610 sec/batch\n",
      "Epoch 1/3  Iteration 108/534 Training loss: 3.1761 0.6237 sec/batch\n",
      "Epoch 1/3  Iteration 109/534 Training loss: 3.1736 0.5585 sec/batch\n",
      "Epoch 1/3  Iteration 110/534 Training loss: 3.1707 0.5279 sec/batch\n",
      "Epoch 1/3  Iteration 111/534 Training loss: 3.1680 0.5355 sec/batch\n",
      "Epoch 1/3  Iteration 112/534 Training loss: 3.1655 0.5807 sec/batch\n",
      "Epoch 1/3  Iteration 113/534 Training loss: 3.1628 0.6265 sec/batch\n",
      "Epoch 1/3  Iteration 114/534 Training loss: 3.1600 0.6571 sec/batch\n",
      "Epoch 1/3  Iteration 115/534 Training loss: 3.1572 0.6204 sec/batch\n",
      "Epoch 1/3  Iteration 116/534 Training loss: 3.1544 0.5428 sec/batch\n",
      "Epoch 1/3  Iteration 117/534 Training loss: 3.1517 0.5674 sec/batch\n",
      "Epoch 1/3  Iteration 118/534 Training loss: 3.1490 0.6408 sec/batch\n",
      "Epoch 1/3  Iteration 119/534 Training loss: 3.1464 0.6799 sec/batch\n",
      "Epoch 1/3  Iteration 120/534 Training loss: 3.1437 0.7861 sec/batch\n",
      "Epoch 1/3  Iteration 121/534 Training loss: 3.1412 0.7132 sec/batch\n",
      "Epoch 1/3  Iteration 122/534 Training loss: 3.1385 0.6790 sec/batch\n",
      "Epoch 1/3  Iteration 123/534 Training loss: 3.1359 0.6039 sec/batch\n",
      "Epoch 1/3  Iteration 124/534 Training loss: 3.1332 0.6217 sec/batch\n",
      "Epoch 1/3  Iteration 125/534 Training loss: 3.1304 0.6112 sec/batch\n",
      "Epoch 1/3  Iteration 126/534 Training loss: 3.1275 0.5789 sec/batch\n",
      "Epoch 1/3  Iteration 127/534 Training loss: 3.1248 0.6006 sec/batch\n",
      "Epoch 1/3  Iteration 128/534 Training loss: 3.1222 0.6042 sec/batch\n",
      "Epoch 1/3  Iteration 129/534 Training loss: 3.1194 0.5888 sec/batch\n",
      "Epoch 1/3  Iteration 130/534 Training loss: 3.1166 0.5790 sec/batch\n",
      "Epoch 1/3  Iteration 131/534 Training loss: 3.1138 0.6084 sec/batch\n",
      "Epoch 1/3  Iteration 132/534 Training loss: 3.1110 0.5903 sec/batch\n",
      "Epoch 1/3  Iteration 133/534 Training loss: 3.1083 0.5699 sec/batch\n",
      "Epoch 1/3  Iteration 134/534 Training loss: 3.1056 0.6087 sec/batch\n",
      "Epoch 1/3  Iteration 135/534 Training loss: 3.1025 0.6208 sec/batch\n",
      "Epoch 1/3  Iteration 136/534 Training loss: 3.0996 0.5556 sec/batch\n",
      "Epoch 1/3  Iteration 137/534 Training loss: 3.0968 0.6852 sec/batch\n",
      "Epoch 1/3  Iteration 138/534 Training loss: 3.0939 0.6879 sec/batch\n",
      "Epoch 1/3  Iteration 139/534 Training loss: 3.0912 1.0543 sec/batch\n",
      "Epoch 1/3  Iteration 140/534 Training loss: 3.0884 0.7343 sec/batch\n",
      "Epoch 1/3  Iteration 141/534 Training loss: 3.0858 0.5827 sec/batch\n",
      "Epoch 1/3  Iteration 142/534 Training loss: 3.0829 0.5724 sec/batch\n",
      "Epoch 1/3  Iteration 143/534 Training loss: 3.0801 0.5487 sec/batch\n",
      "Epoch 1/3  Iteration 144/534 Training loss: 3.0773 0.7286 sec/batch\n",
      "Epoch 1/3  Iteration 145/534 Training loss: 3.0746 0.6910 sec/batch\n",
      "Epoch 1/3  Iteration 146/534 Training loss: 3.0719 0.7837 sec/batch\n",
      "Epoch 1/3  Iteration 147/534 Training loss: 3.0693 0.6712 sec/batch\n",
      "Epoch 1/3  Iteration 148/534 Training loss: 3.0667 0.8068 sec/batch\n",
      "Epoch 1/3  Iteration 149/534 Training loss: 3.0639 0.5971 sec/batch\n",
      "Epoch 1/3  Iteration 150/534 Training loss: 3.0611 0.9766 sec/batch\n",
      "Epoch 1/3  Iteration 151/534 Training loss: 3.0586 0.9877 sec/batch\n",
      "Epoch 1/3  Iteration 152/534 Training loss: 3.0562 0.8107 sec/batch\n",
      "Epoch 1/3  Iteration 153/534 Training loss: 3.0536 1.0889 sec/batch\n",
      "Epoch 1/3  Iteration 154/534 Training loss: 3.0510 1.1569 sec/batch\n",
      "Epoch 1/3  Iteration 155/534 Training loss: 3.0483 1.0612 sec/batch\n",
      "Epoch 1/3  Iteration 156/534 Training loss: 3.0456 0.9387 sec/batch\n",
      "Epoch 1/3  Iteration 157/534 Training loss: 3.0429 0.9959 sec/batch\n",
      "Epoch 1/3  Iteration 158/534 Training loss: 3.0402 0.5976 sec/batch\n",
      "Epoch 1/3  Iteration 159/534 Training loss: 3.0374 0.5929 sec/batch\n",
      "Epoch 1/3  Iteration 160/534 Training loss: 3.0348 0.9862 sec/batch\n",
      "Epoch 1/3  Iteration 161/534 Training loss: 3.0322 0.7623 sec/batch\n",
      "Epoch 1/3  Iteration 162/534 Training loss: 3.0295 0.5697 sec/batch\n",
      "Epoch 1/3  Iteration 163/534 Training loss: 3.0268 0.6360 sec/batch\n",
      "Epoch 1/3  Iteration 164/534 Training loss: 3.0241 2.1655 sec/batch\n",
      "Epoch 1/3  Iteration 165/534 Training loss: 3.0215 1.0993 sec/batch\n",
      "Epoch 1/3  Iteration 166/534 Training loss: 3.0189 0.7989 sec/batch\n",
      "Epoch 1/3  Iteration 167/534 Training loss: 3.0164 0.6998 sec/batch\n",
      "Epoch 1/3  Iteration 168/534 Training loss: 3.0139 0.6612 sec/batch\n",
      "Epoch 1/3  Iteration 169/534 Training loss: 3.0114 0.8532 sec/batch\n",
      "Epoch 1/3  Iteration 170/534 Training loss: 3.0089 0.6656 sec/batch\n",
      "Epoch 1/3  Iteration 171/534 Training loss: 3.0064 0.8502 sec/batch\n",
      "Epoch 1/3  Iteration 172/534 Training loss: 3.0041 0.7027 sec/batch\n",
      "Epoch 1/3  Iteration 173/534 Training loss: 3.0019 0.8465 sec/batch\n",
      "Epoch 1/3  Iteration 174/534 Training loss: 2.9997 0.9002 sec/batch\n",
      "Epoch 1/3  Iteration 175/534 Training loss: 2.9974 0.7950 sec/batch\n",
      "Epoch 1/3  Iteration 176/534 Training loss: 2.9950 0.9155 sec/batch\n",
      "Epoch 1/3  Iteration 177/534 Training loss: 2.9926 0.7603 sec/batch\n",
      "Epoch 1/3  Iteration 178/534 Training loss: 2.9900 0.8093 sec/batch\n",
      "Epoch 2/3  Iteration 179/534 Training loss: 2.6065 0.6874 sec/batch\n",
      "Epoch 2/3  Iteration 180/534 Training loss: 2.5654 0.5653 sec/batch\n",
      "Epoch 2/3  Iteration 181/534 Training loss: 2.5586 0.7087 sec/batch\n",
      "Epoch 2/3  Iteration 182/534 Training loss: 2.5543 0.5314 sec/batch\n",
      "Epoch 2/3  Iteration 183/534 Training loss: 2.5515 0.5152 sec/batch\n",
      "Epoch 2/3  Iteration 184/534 Training loss: 2.5498 0.5275 sec/batch\n",
      "Epoch 2/3  Iteration 185/534 Training loss: 2.5500 0.5234 sec/batch\n",
      "Epoch 2/3  Iteration 186/534 Training loss: 2.5508 0.5678 sec/batch\n",
      "Epoch 2/3  Iteration 187/534 Training loss: 2.5508 0.5839 sec/batch\n",
      "Epoch 2/3  Iteration 188/534 Training loss: 2.5489 0.6428 sec/batch\n",
      "Epoch 2/3  Iteration 189/534 Training loss: 2.5464 0.6115 sec/batch\n",
      "Epoch 2/3  Iteration 190/534 Training loss: 2.5460 0.5974 sec/batch\n",
      "Epoch 2/3  Iteration 191/534 Training loss: 2.5450 0.7801 sec/batch\n",
      "Epoch 2/3  Iteration 192/534 Training loss: 2.5461 0.7663 sec/batch\n",
      "Epoch 2/3  Iteration 193/534 Training loss: 2.5452 1.0795 sec/batch\n",
      "Epoch 2/3  Iteration 194/534 Training loss: 2.5447 1.0576 sec/batch\n",
      "Epoch 2/3  Iteration 195/534 Training loss: 2.5442 0.8090 sec/batch\n",
      "Epoch 2/3  Iteration 196/534 Training loss: 2.5450 0.7322 sec/batch\n",
      "Epoch 2/3  Iteration 197/534 Training loss: 2.5446 0.8077 sec/batch\n",
      "Epoch 2/3  Iteration 198/534 Training loss: 2.5424 0.9300 sec/batch\n",
      "Epoch 2/3  Iteration 199/534 Training loss: 2.5415 0.7833 sec/batch\n",
      "Epoch 2/3  Iteration 200/534 Training loss: 2.5418 0.8532 sec/batch\n",
      "Epoch 2/3  Iteration 201/534 Training loss: 2.5408 0.7664 sec/batch\n",
      "Epoch 2/3  Iteration 202/534 Training loss: 2.5395 0.8792 sec/batch\n",
      "Epoch 2/3  Iteration 203/534 Training loss: 2.5381 0.8774 sec/batch\n",
      "Epoch 2/3  Iteration 204/534 Training loss: 2.5373 0.8458 sec/batch\n",
      "Epoch 2/3  Iteration 205/534 Training loss: 2.5359 1.0903 sec/batch\n",
      "Epoch 2/3  Iteration 206/534 Training loss: 2.5345 0.7516 sec/batch\n",
      "Epoch 2/3  Iteration 207/534 Training loss: 2.5339 0.6400 sec/batch\n",
      "Epoch 2/3  Iteration 208/534 Training loss: 2.5330 0.8254 sec/batch\n",
      "Epoch 2/3  Iteration 209/534 Training loss: 2.5326 0.6791 sec/batch\n",
      "Epoch 2/3  Iteration 210/534 Training loss: 2.5314 0.9148 sec/batch\n",
      "Epoch 2/3  Iteration 211/534 Training loss: 2.5299 0.7906 sec/batch\n",
      "Epoch 2/3  Iteration 212/534 Training loss: 2.5288 0.5915 sec/batch\n",
      "Epoch 2/3  Iteration 213/534 Training loss: 2.5276 0.5045 sec/batch\n",
      "Epoch 2/3  Iteration 214/534 Training loss: 2.5270 0.6533 sec/batch\n",
      "Epoch 2/3  Iteration 215/534 Training loss: 2.5258 0.7083 sec/batch\n",
      "Epoch 2/3  Iteration 216/534 Training loss: 2.5242 0.5203 sec/batch\n",
      "Epoch 2/3  Iteration 217/534 Training loss: 2.5227 0.8032 sec/batch\n",
      "Epoch 2/3  Iteration 218/534 Training loss: 2.5213 0.5483 sec/batch\n",
      "Epoch 2/3  Iteration 219/534 Training loss: 2.5199 0.4955 sec/batch\n",
      "Epoch 2/3  Iteration 220/534 Training loss: 2.5187 0.5379 sec/batch\n",
      "Epoch 2/3  Iteration 221/534 Training loss: 2.5174 0.5538 sec/batch\n",
      "Epoch 2/3  Iteration 222/534 Training loss: 2.5161 0.8322 sec/batch\n",
      "Epoch 2/3  Iteration 223/534 Training loss: 2.5148 0.9624 sec/batch\n",
      "Epoch 2/3  Iteration 224/534 Training loss: 2.5132 0.9844 sec/batch\n",
      "Epoch 2/3  Iteration 225/534 Training loss: 2.5123 1.1287 sec/batch\n",
      "Epoch 2/3  Iteration 226/534 Training loss: 2.5113 0.7698 sec/batch\n",
      "Epoch 2/3  Iteration 227/534 Training loss: 2.5102 0.8146 sec/batch\n",
      "Epoch 2/3  Iteration 228/534 Training loss: 2.5098 0.8049 sec/batch\n",
      "Epoch 2/3  Iteration 229/534 Training loss: 2.5088 0.6761 sec/batch\n",
      "Epoch 2/3  Iteration 230/534 Training loss: 2.5080 0.6438 sec/batch\n",
      "Epoch 2/3  Iteration 231/534 Training loss: 2.5071 0.6602 sec/batch\n",
      "Epoch 2/3  Iteration 232/534 Training loss: 2.5060 0.5951 sec/batch\n",
      "Epoch 2/3  Iteration 233/534 Training loss: 2.5049 0.6051 sec/batch\n",
      "Epoch 2/3  Iteration 234/534 Training loss: 2.5040 0.5309 sec/batch\n",
      "Epoch 2/3  Iteration 235/534 Training loss: 2.5032 0.6106 sec/batch\n",
      "Epoch 2/3  Iteration 236/534 Training loss: 2.5020 0.6881 sec/batch\n",
      "Epoch 2/3  Iteration 237/534 Training loss: 2.5010 0.9354 sec/batch\n",
      "Epoch 2/3  Iteration 238/534 Training loss: 2.5004 0.6183 sec/batch\n",
      "Epoch 2/3  Iteration 239/534 Training loss: 2.4995 0.7246 sec/batch\n",
      "Epoch 2/3  Iteration 240/534 Training loss: 2.4988 0.5612 sec/batch\n",
      "Epoch 2/3  Iteration 241/534 Training loss: 2.4982 0.5167 sec/batch\n",
      "Epoch 2/3  Iteration 242/534 Training loss: 2.4973 0.5718 sec/batch\n",
      "Epoch 2/3  Iteration 243/534 Training loss: 2.4964 0.6071 sec/batch\n",
      "Epoch 2/3  Iteration 244/534 Training loss: 2.4958 0.6568 sec/batch\n",
      "Epoch 2/3  Iteration 245/534 Training loss: 2.4950 0.8344 sec/batch\n",
      "Epoch 2/3  Iteration 246/534 Training loss: 2.4938 0.8043 sec/batch\n",
      "Epoch 2/3  Iteration 247/534 Training loss: 2.4929 0.8415 sec/batch\n",
      "Epoch 2/3  Iteration 248/534 Training loss: 2.4923 0.6526 sec/batch\n",
      "Epoch 2/3  Iteration 249/534 Training loss: 2.4915 0.6441 sec/batch\n",
      "Epoch 2/3  Iteration 250/534 Training loss: 2.4908 0.6266 sec/batch\n",
      "Epoch 2/3  Iteration 251/534 Training loss: 2.4901 0.6538 sec/batch\n",
      "Epoch 2/3  Iteration 252/534 Training loss: 2.4891 0.7390 sec/batch\n",
      "Epoch 2/3  Iteration 253/534 Training loss: 2.4885 0.7294 sec/batch\n",
      "Epoch 2/3  Iteration 254/534 Training loss: 2.4882 0.7266 sec/batch\n",
      "Epoch 2/3  Iteration 255/534 Training loss: 2.4873 0.7822 sec/batch\n",
      "Epoch 2/3  Iteration 256/534 Training loss: 2.4867 0.5688 sec/batch\n",
      "Epoch 2/3  Iteration 257/534 Training loss: 2.4858 0.5768 sec/batch\n",
      "Epoch 2/3  Iteration 258/534 Training loss: 2.4850 0.6866 sec/batch\n",
      "Epoch 2/3  Iteration 259/534 Training loss: 2.4842 0.5992 sec/batch\n",
      "Epoch 2/3  Iteration 260/534 Training loss: 2.4836 0.5520 sec/batch\n",
      "Epoch 2/3  Iteration 261/534 Training loss: 2.4828 0.5367 sec/batch\n",
      "Epoch 2/3  Iteration 262/534 Training loss: 2.4818 0.5817 sec/batch\n",
      "Epoch 2/3  Iteration 263/534 Training loss: 2.4806 0.5470 sec/batch\n",
      "Epoch 2/3  Iteration 264/534 Training loss: 2.4797 0.5073 sec/batch\n",
      "Epoch 2/3  Iteration 265/534 Training loss: 2.4790 0.5237 sec/batch\n",
      "Epoch 2/3  Iteration 266/534 Training loss: 2.4781 0.5066 sec/batch\n",
      "Epoch 2/3  Iteration 267/534 Training loss: 2.4773 0.6425 sec/batch\n",
      "Epoch 2/3  Iteration 268/534 Training loss: 2.4767 0.7665 sec/batch\n",
      "Epoch 2/3  Iteration 269/534 Training loss: 2.4760 0.7841 sec/batch\n",
      "Epoch 2/3  Iteration 270/534 Training loss: 2.4754 0.7379 sec/batch\n",
      "Epoch 2/3  Iteration 271/534 Training loss: 2.4746 0.6615 sec/batch\n",
      "Epoch 2/3  Iteration 272/534 Training loss: 2.4738 0.4579 sec/batch\n",
      "Epoch 2/3  Iteration 273/534 Training loss: 2.4729 0.4695 sec/batch\n",
      "Epoch 2/3  Iteration 274/534 Training loss: 2.4720 0.6925 sec/batch\n",
      "Epoch 2/3  Iteration 275/534 Training loss: 2.4713 0.5140 sec/batch\n",
      "Epoch 2/3  Iteration 276/534 Training loss: 2.4705 0.5989 sec/batch\n",
      "Epoch 2/3  Iteration 277/534 Training loss: 2.4697 0.5801 sec/batch\n",
      "Epoch 2/3  Iteration 278/534 Training loss: 2.4689 0.5707 sec/batch\n",
      "Epoch 2/3  Iteration 279/534 Training loss: 2.4684 0.4920 sec/batch\n",
      "Epoch 2/3  Iteration 280/534 Training loss: 2.4678 0.5883 sec/batch\n",
      "Epoch 2/3  Iteration 281/534 Training loss: 2.4668 0.5405 sec/batch\n",
      "Epoch 2/3  Iteration 282/534 Training loss: 2.4661 0.5663 sec/batch\n",
      "Epoch 2/3  Iteration 283/534 Training loss: 2.4653 0.6486 sec/batch\n",
      "Epoch 2/3  Iteration 284/534 Training loss: 2.4647 0.7199 sec/batch\n",
      "Epoch 2/3  Iteration 285/534 Training loss: 2.4639 0.7571 sec/batch\n",
      "Epoch 2/3  Iteration 286/534 Training loss: 2.4634 0.7183 sec/batch\n",
      "Epoch 2/3  Iteration 287/534 Training loss: 2.4628 0.6667 sec/batch\n",
      "Epoch 2/3  Iteration 288/534 Training loss: 2.4620 0.6200 sec/batch\n",
      "Epoch 2/3  Iteration 289/534 Training loss: 2.4614 0.5834 sec/batch\n",
      "Epoch 2/3  Iteration 290/534 Training loss: 2.4609 0.6765 sec/batch\n",
      "Epoch 2/3  Iteration 291/534 Training loss: 2.4602 0.5133 sec/batch\n",
      "Epoch 2/3  Iteration 292/534 Training loss: 2.4594 0.5500 sec/batch\n",
      "Epoch 2/3  Iteration 293/534 Training loss: 2.4588 0.5525 sec/batch\n",
      "Epoch 2/3  Iteration 294/534 Training loss: 2.4579 0.5134 sec/batch\n",
      "Epoch 2/3  Iteration 295/534 Training loss: 2.4573 0.5854 sec/batch\n",
      "Epoch 2/3  Iteration 296/534 Training loss: 2.4567 0.6139 sec/batch\n",
      "Epoch 2/3  Iteration 297/534 Training loss: 2.4563 0.5051 sec/batch\n",
      "Epoch 2/3  Iteration 298/534 Training loss: 2.4557 0.7198 sec/batch\n",
      "Epoch 2/3  Iteration 299/534 Training loss: 2.4553 0.7148 sec/batch\n",
      "Epoch 2/3  Iteration 300/534 Training loss: 2.4547 0.6736 sec/batch\n",
      "Epoch 2/3  Iteration 301/534 Training loss: 2.4541 0.5597 sec/batch\n",
      "Epoch 2/3  Iteration 302/534 Training loss: 2.4536 0.8136 sec/batch\n",
      "Epoch 2/3  Iteration 303/534 Training loss: 2.4529 0.8396 sec/batch\n",
      "Epoch 2/3  Iteration 304/534 Training loss: 2.4522 1.1341 sec/batch\n",
      "Epoch 2/3  Iteration 305/534 Training loss: 2.4517 0.9082 sec/batch\n",
      "Epoch 2/3  Iteration 306/534 Training loss: 2.4512 0.9614 sec/batch\n",
      "Epoch 2/3  Iteration 307/534 Training loss: 2.4506 0.6315 sec/batch\n",
      "Epoch 2/3  Iteration 308/534 Training loss: 2.4501 0.6138 sec/batch\n",
      "Epoch 2/3  Iteration 309/534 Training loss: 2.4496 0.5669 sec/batch\n",
      "Epoch 2/3  Iteration 310/534 Training loss: 2.4489 0.5365 sec/batch\n",
      "Epoch 2/3  Iteration 311/534 Training loss: 2.4484 0.7221 sec/batch\n",
      "Epoch 2/3  Iteration 312/534 Training loss: 2.4480 0.7514 sec/batch\n",
      "Epoch 2/3  Iteration 313/534 Training loss: 2.4472 0.5690 sec/batch\n",
      "Epoch 2/3  Iteration 314/534 Training loss: 2.4468 0.5989 sec/batch\n",
      "Epoch 2/3  Iteration 315/534 Training loss: 2.4463 0.6477 sec/batch\n",
      "Epoch 2/3  Iteration 316/534 Training loss: 2.4457 0.7419 sec/batch\n",
      "Epoch 2/3  Iteration 317/534 Training loss: 2.4454 0.7001 sec/batch\n",
      "Epoch 2/3  Iteration 318/534 Training loss: 2.4448 0.6760 sec/batch\n",
      "Epoch 2/3  Iteration 319/534 Training loss: 2.4444 0.5367 sec/batch\n",
      "Epoch 2/3  Iteration 320/534 Training loss: 2.4438 0.5627 sec/batch\n",
      "Epoch 2/3  Iteration 321/534 Training loss: 2.4434 0.6076 sec/batch\n",
      "Epoch 2/3  Iteration 322/534 Training loss: 2.4427 0.5756 sec/batch\n",
      "Epoch 2/3  Iteration 323/534 Training loss: 2.4422 0.5339 sec/batch\n",
      "Epoch 2/3  Iteration 324/534 Training loss: 2.4418 0.6229 sec/batch\n",
      "Epoch 2/3  Iteration 325/534 Training loss: 2.4413 0.6427 sec/batch\n",
      "Epoch 2/3  Iteration 326/534 Training loss: 2.4409 0.4738 sec/batch\n",
      "Epoch 2/3  Iteration 327/534 Training loss: 2.4403 0.4719 sec/batch\n",
      "Epoch 2/3  Iteration 328/534 Training loss: 2.4397 0.6213 sec/batch\n",
      "Epoch 2/3  Iteration 329/534 Training loss: 2.4394 0.5141 sec/batch\n",
      "Epoch 2/3  Iteration 330/534 Training loss: 2.4392 0.4723 sec/batch\n",
      "Epoch 2/3  Iteration 331/534 Training loss: 2.4388 0.5750 sec/batch\n",
      "Epoch 2/3  Iteration 332/534 Training loss: 2.4384 0.6154 sec/batch\n",
      "Epoch 2/3  Iteration 333/534 Training loss: 2.4378 0.5328 sec/batch\n",
      "Epoch 2/3  Iteration 334/534 Training loss: 2.4373 0.8328 sec/batch\n",
      "Epoch 2/3  Iteration 335/534 Training loss: 2.4368 0.8971 sec/batch\n",
      "Epoch 2/3  Iteration 336/534 Training loss: 2.4362 0.7389 sec/batch\n",
      "Epoch 2/3  Iteration 337/534 Training loss: 2.4356 0.5929 sec/batch\n",
      "Epoch 2/3  Iteration 338/534 Training loss: 2.4352 0.5364 sec/batch\n",
      "Epoch 2/3  Iteration 339/534 Training loss: 2.4348 0.5490 sec/batch\n",
      "Epoch 2/3  Iteration 340/534 Training loss: 2.4341 0.5517 sec/batch\n",
      "Epoch 2/3  Iteration 341/534 Training loss: 2.4336 0.4722 sec/batch\n",
      "Epoch 2/3  Iteration 342/534 Training loss: 2.4331 0.4876 sec/batch\n",
      "Epoch 2/3  Iteration 343/534 Training loss: 2.4327 0.7683 sec/batch\n",
      "Epoch 2/3  Iteration 344/534 Training loss: 2.4322 0.4820 sec/batch\n",
      "Epoch 2/3  Iteration 345/534 Training loss: 2.4317 0.4781 sec/batch\n",
      "Epoch 2/3  Iteration 346/534 Training loss: 2.4313 0.4549 sec/batch\n",
      "Epoch 2/3  Iteration 347/534 Training loss: 2.4308 0.4998 sec/batch\n",
      "Epoch 2/3  Iteration 348/534 Training loss: 2.4302 0.7567 sec/batch\n",
      "Epoch 2/3  Iteration 349/534 Training loss: 2.4298 0.6816 sec/batch\n",
      "Epoch 2/3  Iteration 350/534 Training loss: 2.4295 0.6787 sec/batch\n",
      "Epoch 2/3  Iteration 351/534 Training loss: 2.4293 0.7468 sec/batch\n",
      "Epoch 2/3  Iteration 352/534 Training loss: 2.4291 0.8444 sec/batch\n",
      "Epoch 2/3  Iteration 353/534 Training loss: 2.4289 0.6791 sec/batch\n",
      "Epoch 2/3  Iteration 354/534 Training loss: 2.4284 0.7604 sec/batch\n",
      "Epoch 2/3  Iteration 355/534 Training loss: 2.4278 0.7149 sec/batch\n",
      "Epoch 2/3  Iteration 356/534 Training loss: 2.4272 0.8541 sec/batch\n",
      "Epoch 3/3  Iteration 357/534 Training loss: 2.3864 0.8437 sec/batch\n",
      "Epoch 3/3  Iteration 358/534 Training loss: 2.3411 0.4726 sec/batch\n",
      "Epoch 3/3  Iteration 359/534 Training loss: 2.3338 0.5031 sec/batch\n",
      "Epoch 3/3  Iteration 360/534 Training loss: 2.3328 0.6617 sec/batch\n",
      "Epoch 3/3  Iteration 361/534 Training loss: 2.3338 0.5672 sec/batch\n",
      "Epoch 3/3  Iteration 362/534 Training loss: 2.3318 0.8920 sec/batch\n",
      "Epoch 3/3  Iteration 363/534 Training loss: 2.3340 1.2907 sec/batch\n",
      "Epoch 3/3  Iteration 364/534 Training loss: 2.3342 2.0095 sec/batch\n",
      "Epoch 3/3  Iteration 365/534 Training loss: 2.3362 1.1325 sec/batch\n",
      "Epoch 3/3  Iteration 366/534 Training loss: 2.3365 1.0537 sec/batch\n",
      "Epoch 3/3  Iteration 367/534 Training loss: 2.3351 1.1540 sec/batch\n",
      "Epoch 3/3  Iteration 368/534 Training loss: 2.3341 0.8790 sec/batch\n",
      "Epoch 3/3  Iteration 369/534 Training loss: 2.3337 0.8303 sec/batch\n",
      "Epoch 3/3  Iteration 370/534 Training loss: 2.3359 0.6333 sec/batch\n",
      "Epoch 3/3  Iteration 371/534 Training loss: 2.3356 0.6165 sec/batch\n",
      "Epoch 3/3  Iteration 372/534 Training loss: 2.3348 0.5744 sec/batch\n",
      "Epoch 3/3  Iteration 373/534 Training loss: 2.3338 0.4891 sec/batch\n",
      "Epoch 3/3  Iteration 374/534 Training loss: 2.3354 0.6237 sec/batch\n",
      "Epoch 3/3  Iteration 375/534 Training loss: 2.3352 0.8658 sec/batch\n",
      "Epoch 3/3  Iteration 376/534 Training loss: 2.3336 0.9921 sec/batch\n",
      "Epoch 3/3  Iteration 377/534 Training loss: 2.3325 1.0235 sec/batch\n",
      "Epoch 3/3  Iteration 378/534 Training loss: 2.3342 0.8517 sec/batch\n",
      "Epoch 3/3  Iteration 379/534 Training loss: 2.3339 0.7909 sec/batch\n",
      "Epoch 3/3  Iteration 380/534 Training loss: 2.3331 0.5953 sec/batch\n",
      "Epoch 3/3  Iteration 381/534 Training loss: 2.3323 0.5051 sec/batch\n",
      "Epoch 3/3  Iteration 382/534 Training loss: 2.3317 0.4691 sec/batch\n",
      "Epoch 3/3  Iteration 383/534 Training loss: 2.3307 0.5963 sec/batch\n",
      "Epoch 3/3  Iteration 384/534 Training loss: 2.3302 0.4268 sec/batch\n",
      "Epoch 3/3  Iteration 385/534 Training loss: 2.3305 0.4335 sec/batch\n",
      "Epoch 3/3  Iteration 386/534 Training loss: 2.3302 0.4196 sec/batch\n",
      "Epoch 3/3  Iteration 387/534 Training loss: 2.3303 0.4115 sec/batch\n",
      "Epoch 3/3  Iteration 388/534 Training loss: 2.3294 0.4088 sec/batch\n",
      "Epoch 3/3  Iteration 389/534 Training loss: 2.3288 0.4030 sec/batch\n",
      "Epoch 3/3  Iteration 390/534 Training loss: 2.3288 0.3935 sec/batch\n",
      "Epoch 3/3  Iteration 391/534 Training loss: 2.3281 0.4191 sec/batch\n",
      "Epoch 3/3  Iteration 392/534 Training loss: 2.3279 0.4193 sec/batch\n",
      "Epoch 3/3  Iteration 393/534 Training loss: 2.3273 0.4249 sec/batch\n",
      "Epoch 3/3  Iteration 394/534 Training loss: 2.3261 0.6429 sec/batch\n",
      "Epoch 3/3  Iteration 395/534 Training loss: 2.3252 0.6125 sec/batch\n",
      "Epoch 3/3  Iteration 396/534 Training loss: 2.3242 0.5651 sec/batch\n",
      "Epoch 3/3  Iteration 397/534 Training loss: 2.3233 0.5464 sec/batch\n",
      "Epoch 3/3  Iteration 398/534 Training loss: 2.3227 0.4365 sec/batch\n",
      "Epoch 3/3  Iteration 399/534 Training loss: 2.3219 0.4881 sec/batch\n",
      "Epoch 3/3  Iteration 400/534 Training loss: 2.3212 0.5096 sec/batch\n",
      "Epoch 3/3  Iteration 401/534 Training loss: 2.3207 0.4099 sec/batch\n",
      "Epoch 3/3  Iteration 402/534 Training loss: 2.3194 0.4090 sec/batch\n",
      "Epoch 3/3  Iteration 403/534 Training loss: 2.3193 0.4487 sec/batch\n",
      "Epoch 3/3  Iteration 404/534 Training loss: 2.3188 0.4884 sec/batch\n",
      "Epoch 3/3  Iteration 405/534 Training loss: 2.3184 0.4907 sec/batch\n",
      "Epoch 3/3  Iteration 406/534 Training loss: 2.3185 0.4281 sec/batch\n",
      "Epoch 3/3  Iteration 407/534 Training loss: 2.3179 0.4182 sec/batch\n",
      "Epoch 3/3  Iteration 408/534 Training loss: 2.3176 0.4737 sec/batch\n",
      "Epoch 3/3  Iteration 409/534 Training loss: 2.3170 0.4532 sec/batch\n",
      "Epoch 3/3  Iteration 410/534 Training loss: 2.3163 0.5176 sec/batch\n",
      "Epoch 3/3  Iteration 411/534 Training loss: 2.3157 0.4535 sec/batch\n",
      "Epoch 3/3  Iteration 412/534 Training loss: 2.3155 0.5365 sec/batch\n",
      "Epoch 3/3  Iteration 413/534 Training loss: 2.3152 0.5023 sec/batch\n",
      "Epoch 3/3  Iteration 414/534 Training loss: 2.3146 0.5158 sec/batch\n",
      "Epoch 3/3  Iteration 415/534 Training loss: 2.3140 0.9253 sec/batch\n",
      "Epoch 3/3  Iteration 416/534 Training loss: 2.3141 0.7831 sec/batch\n",
      "Epoch 3/3  Iteration 417/534 Training loss: 2.3135 0.7779 sec/batch\n",
      "Epoch 3/3  Iteration 418/534 Training loss: 2.3135 0.8893 sec/batch\n",
      "Epoch 3/3  Iteration 419/534 Training loss: 2.3134 0.6375 sec/batch\n",
      "Epoch 3/3  Iteration 420/534 Training loss: 2.3129 0.6668 sec/batch\n",
      "Epoch 3/3  Iteration 421/534 Training loss: 2.3124 0.5414 sec/batch\n",
      "Epoch 3/3  Iteration 422/534 Training loss: 2.3122 0.5501 sec/batch\n",
      "Epoch 3/3  Iteration 423/534 Training loss: 2.3120 0.8529 sec/batch\n",
      "Epoch 3/3  Iteration 424/534 Training loss: 2.3113 0.7399 sec/batch\n",
      "Epoch 3/3  Iteration 425/534 Training loss: 2.3107 0.6078 sec/batch\n",
      "Epoch 3/3  Iteration 426/534 Training loss: 2.3106 0.8105 sec/batch\n",
      "Epoch 3/3  Iteration 427/534 Training loss: 2.3106 0.6062 sec/batch\n",
      "Epoch 3/3  Iteration 428/534 Training loss: 2.3104 0.5693 sec/batch\n",
      "Epoch 3/3  Iteration 429/534 Training loss: 2.3102 0.6572 sec/batch\n",
      "Epoch 3/3  Iteration 430/534 Training loss: 2.3096 0.8007 sec/batch\n",
      "Epoch 3/3  Iteration 431/534 Training loss: 2.3092 0.7631 sec/batch\n",
      "Epoch 3/3  Iteration 432/534 Training loss: 2.3092 0.8780 sec/batch\n",
      "Epoch 3/3  Iteration 433/534 Training loss: 2.3087 0.6608 sec/batch\n",
      "Epoch 3/3  Iteration 434/534 Training loss: 2.3085 0.8667 sec/batch\n",
      "Epoch 3/3  Iteration 435/534 Training loss: 2.3079 0.7452 sec/batch\n",
      "Epoch 3/3  Iteration 436/534 Training loss: 2.3075 0.9633 sec/batch\n",
      "Epoch 3/3  Iteration 437/534 Training loss: 2.3069 0.6778 sec/batch\n",
      "Epoch 3/3  Iteration 438/534 Training loss: 2.3067 0.9720 sec/batch\n",
      "Epoch 3/3  Iteration 439/534 Training loss: 2.3061 0.9699 sec/batch\n",
      "Epoch 3/3  Iteration 440/534 Training loss: 2.3055 1.1849 sec/batch\n",
      "Epoch 3/3  Iteration 441/534 Training loss: 2.3046 0.9584 sec/batch\n",
      "Epoch 3/3  Iteration 442/534 Training loss: 2.3041 1.1694 sec/batch\n",
      "Epoch 3/3  Iteration 443/534 Training loss: 2.3036 1.0027 sec/batch\n",
      "Epoch 3/3  Iteration 444/534 Training loss: 2.3030 1.2223 sec/batch\n",
      "Epoch 3/3  Iteration 445/534 Training loss: 2.3024 1.1950 sec/batch\n",
      "Epoch 3/3  Iteration 446/534 Training loss: 2.3022 0.9168 sec/batch\n",
      "Epoch 3/3  Iteration 447/534 Training loss: 2.3017 0.9089 sec/batch\n",
      "Epoch 3/3  Iteration 448/534 Training loss: 2.3013 0.7611 sec/batch\n",
      "Epoch 3/3  Iteration 449/534 Training loss: 2.3007 0.7659 sec/batch\n",
      "Epoch 3/3  Iteration 450/534 Training loss: 2.3002 0.7236 sec/batch\n",
      "Epoch 3/3  Iteration 451/534 Training loss: 2.2996 0.8642 sec/batch\n",
      "Epoch 3/3  Iteration 452/534 Training loss: 2.2992 0.7774 sec/batch\n",
      "Epoch 3/3  Iteration 453/534 Training loss: 2.2987 0.6602 sec/batch\n",
      "Epoch 3/3  Iteration 454/534 Training loss: 2.2982 1.1469 sec/batch\n",
      "Epoch 3/3  Iteration 455/534 Training loss: 2.2977 1.0055 sec/batch\n",
      "Epoch 3/3  Iteration 456/534 Training loss: 2.2972 0.8643 sec/batch\n",
      "Epoch 3/3  Iteration 457/534 Training loss: 2.2970 1.2808 sec/batch\n",
      "Epoch 3/3  Iteration 458/534 Training loss: 2.2967 0.7391 sec/batch\n",
      "Epoch 3/3  Iteration 459/534 Training loss: 2.2962 0.9202 sec/batch\n",
      "Epoch 3/3  Iteration 460/534 Training loss: 2.2958 0.7463 sec/batch\n",
      "Epoch 3/3  Iteration 461/534 Training loss: 2.2953 1.0331 sec/batch\n",
      "Epoch 3/3  Iteration 462/534 Training loss: 2.2950 0.8118 sec/batch\n",
      "Epoch 3/3  Iteration 463/534 Training loss: 2.2946 0.5876 sec/batch\n",
      "Epoch 3/3  Iteration 464/534 Training loss: 2.2944 0.5874 sec/batch\n",
      "Epoch 3/3  Iteration 465/534 Training loss: 2.2941 0.6285 sec/batch\n",
      "Epoch 3/3  Iteration 466/534 Training loss: 2.2935 0.7172 sec/batch\n",
      "Epoch 3/3  Iteration 467/534 Training loss: 2.2932 0.7569 sec/batch\n",
      "Epoch 3/3  Iteration 468/534 Training loss: 2.2929 0.9322 sec/batch\n",
      "Epoch 3/3  Iteration 469/534 Training loss: 2.2924 0.7871 sec/batch\n",
      "Epoch 3/3  Iteration 470/534 Training loss: 2.2919 0.6404 sec/batch\n",
      "Epoch 3/3  Iteration 471/534 Training loss: 2.2915 0.5558 sec/batch\n",
      "Epoch 3/3  Iteration 472/534 Training loss: 2.2908 0.5113 sec/batch\n",
      "Epoch 3/3  Iteration 473/534 Training loss: 2.2905 0.5468 sec/batch\n",
      "Epoch 3/3  Iteration 474/534 Training loss: 2.2902 0.5908 sec/batch\n",
      "Epoch 3/3  Iteration 475/534 Training loss: 2.2900 0.6265 sec/batch\n",
      "Epoch 3/3  Iteration 476/534 Training loss: 2.2897 0.7284 sec/batch\n",
      "Epoch 3/3  Iteration 477/534 Training loss: 2.2896 0.5922 sec/batch\n",
      "Epoch 3/3  Iteration 478/534 Training loss: 2.2891 0.5694 sec/batch\n",
      "Epoch 3/3  Iteration 479/534 Training loss: 2.2888 0.5771 sec/batch\n",
      "Epoch 3/3  Iteration 480/534 Training loss: 2.2886 0.6320 sec/batch\n",
      "Epoch 3/3  Iteration 481/534 Training loss: 2.2882 0.7993 sec/batch\n",
      "Epoch 3/3  Iteration 482/534 Training loss: 2.2877 0.7443 sec/batch\n",
      "Epoch 3/3  Iteration 483/534 Training loss: 2.2875 0.7485 sec/batch\n",
      "Epoch 3/3  Iteration 484/534 Training loss: 2.2873 0.6183 sec/batch\n",
      "Epoch 3/3  Iteration 485/534 Training loss: 2.2870 0.5609 sec/batch\n",
      "Epoch 3/3  Iteration 486/534 Training loss: 2.2867 0.6641 sec/batch\n",
      "Epoch 3/3  Iteration 487/534 Training loss: 2.2864 0.6054 sec/batch\n",
      "Epoch 3/3  Iteration 488/534 Training loss: 2.2859 0.5729 sec/batch\n",
      "Epoch 3/3  Iteration 489/534 Training loss: 2.2856 0.6014 sec/batch\n",
      "Epoch 3/3  Iteration 490/534 Training loss: 2.2854 0.5947 sec/batch\n",
      "Epoch 3/3  Iteration 491/534 Training loss: 2.2850 0.5577 sec/batch\n",
      "Epoch 3/3  Iteration 492/534 Training loss: 2.2848 0.5803 sec/batch\n",
      "Epoch 3/3  Iteration 493/534 Training loss: 2.2845 0.5833 sec/batch\n",
      "Epoch 3/3  Iteration 494/534 Training loss: 2.2842 0.5446 sec/batch\n",
      "Epoch 3/3  Iteration 495/534 Training loss: 2.2841 0.5920 sec/batch\n",
      "Epoch 3/3  Iteration 496/534 Training loss: 2.2837 0.6380 sec/batch\n",
      "Epoch 3/3  Iteration 497/534 Training loss: 2.2835 0.7504 sec/batch\n",
      "Epoch 3/3  Iteration 498/534 Training loss: 2.2831 0.6654 sec/batch\n",
      "Epoch 3/3  Iteration 499/534 Training loss: 2.2828 0.6221 sec/batch\n",
      "Epoch 3/3  Iteration 500/534 Training loss: 2.2824 0.6416 sec/batch\n",
      "Epoch 3/3  Iteration 501/534 Training loss: 2.2820 0.6896 sec/batch\n",
      "Epoch 3/3  Iteration 502/534 Training loss: 2.2818 0.6020 sec/batch\n",
      "Epoch 3/3  Iteration 503/534 Training loss: 2.2815 0.5816 sec/batch\n",
      "Epoch 3/3  Iteration 504/534 Training loss: 2.2814 0.5061 sec/batch\n",
      "Epoch 3/3  Iteration 505/534 Training loss: 2.2810 0.5576 sec/batch\n",
      "Epoch 3/3  Iteration 506/534 Training loss: 2.2806 0.5917 sec/batch\n",
      "Epoch 3/3  Iteration 507/534 Training loss: 2.2804 0.5801 sec/batch\n",
      "Epoch 3/3  Iteration 508/534 Training loss: 2.2804 0.6285 sec/batch\n",
      "Epoch 3/3  Iteration 509/534 Training loss: 2.2802 0.8286 sec/batch\n",
      "Epoch 3/3  Iteration 510/534 Training loss: 2.2800 0.6305 sec/batch\n",
      "Epoch 3/3  Iteration 511/534 Training loss: 2.2796 0.7811 sec/batch\n",
      "Epoch 3/3  Iteration 512/534 Training loss: 2.2793 1.1313 sec/batch\n",
      "Epoch 3/3  Iteration 513/534 Training loss: 2.2789 1.1131 sec/batch\n",
      "Epoch 3/3  Iteration 514/534 Training loss: 2.2785 1.0267 sec/batch\n",
      "Epoch 3/3  Iteration 515/534 Training loss: 2.2781 1.2685 sec/batch\n",
      "Epoch 3/3  Iteration 516/534 Training loss: 2.2779 1.2091 sec/batch\n",
      "Epoch 3/3  Iteration 517/534 Training loss: 2.2777 1.0026 sec/batch\n",
      "Epoch 3/3  Iteration 518/534 Training loss: 2.2773 0.8610 sec/batch\n",
      "Epoch 3/3  Iteration 519/534 Training loss: 2.2769 0.8326 sec/batch\n",
      "Epoch 3/3  Iteration 520/534 Training loss: 2.2766 0.7650 sec/batch\n",
      "Epoch 3/3  Iteration 521/534 Training loss: 2.2765 0.6217 sec/batch\n",
      "Epoch 3/3  Iteration 522/534 Training loss: 2.2761 1.0116 sec/batch\n",
      "Epoch 3/3  Iteration 523/534 Training loss: 2.2759 1.0770 sec/batch\n",
      "Epoch 3/3  Iteration 524/534 Training loss: 2.2757 0.7363 sec/batch\n",
      "Epoch 3/3  Iteration 525/534 Training loss: 2.2755 0.6002 sec/batch\n",
      "Epoch 3/3  Iteration 526/534 Training loss: 2.2751 0.6889 sec/batch\n",
      "Epoch 3/3  Iteration 527/534 Training loss: 2.2749 0.7519 sec/batch\n",
      "Epoch 3/3  Iteration 528/534 Training loss: 2.2747 0.5997 sec/batch\n",
      "Epoch 3/3  Iteration 529/534 Training loss: 2.2746 0.6377 sec/batch\n",
      "Epoch 3/3  Iteration 530/534 Training loss: 2.2744 0.5696 sec/batch\n",
      "Epoch 3/3  Iteration 531/534 Training loss: 2.2743 0.4717 sec/batch\n",
      "Epoch 3/3  Iteration 532/534 Training loss: 2.2745 0.6084 sec/batch\n",
      "Epoch 3/3  Iteration 533/534 Training loss: 2.2745 0.7345 sec/batch\n",
      "Epoch 3/3  Iteration 534/534 Training loss: 2.2745 0.6828 sec/batch\n",
      "Epoch 1/3  Iteration 1/534 Training loss: 4.4239 1.5374 sec/batch\n",
      "Epoch 1/3  Iteration 2/534 Training loss: 4.4193 0.8622 sec/batch\n",
      "Epoch 1/3  Iteration 3/534 Training loss: 4.4143 0.8533 sec/batch\n",
      "Epoch 1/3  Iteration 4/534 Training loss: 4.4089 1.0231 sec/batch\n",
      "Epoch 1/3  Iteration 5/534 Training loss: 4.4034 1.1884 sec/batch\n",
      "Epoch 1/3  Iteration 6/534 Training loss: 4.3969 1.4979 sec/batch\n",
      "Epoch 1/3  Iteration 7/534 Training loss: 4.3889 0.8751 sec/batch\n",
      "Epoch 1/3  Iteration 8/534 Training loss: 4.3777 0.9697 sec/batch\n",
      "Epoch 1/3  Iteration 9/534 Training loss: 4.3600 0.9087 sec/batch\n",
      "Epoch 1/3  Iteration 10/534 Training loss: 4.3297 0.8014 sec/batch\n",
      "Epoch 1/3  Iteration 11/534 Training loss: 4.2884 1.1768 sec/batch\n",
      "Epoch 1/3  Iteration 12/534 Training loss: 4.2441 0.9031 sec/batch\n",
      "Epoch 1/3  Iteration 13/534 Training loss: 4.2009 0.9966 sec/batch\n",
      "Epoch 1/3  Iteration 14/534 Training loss: 4.1594 0.9132 sec/batch\n",
      "Epoch 1/3  Iteration 15/534 Training loss: 4.1208 1.0605 sec/batch\n",
      "Epoch 1/3  Iteration 16/534 Training loss: 4.0852 1.1724 sec/batch\n",
      "Epoch 1/3  Iteration 17/534 Training loss: 4.0500 1.0797 sec/batch\n",
      "Epoch 1/3  Iteration 18/534 Training loss: 4.0185 1.0475 sec/batch\n",
      "Epoch 1/3  Iteration 19/534 Training loss: 3.9889 0.9500 sec/batch\n",
      "Epoch 1/3  Iteration 20/534 Training loss: 3.9587 0.9888 sec/batch\n",
      "Epoch 1/3  Iteration 21/534 Training loss: 3.9314 0.8831 sec/batch\n",
      "Epoch 1/3  Iteration 22/534 Training loss: 3.9049 0.7979 sec/batch\n",
      "Epoch 1/3  Iteration 23/534 Training loss: 3.8803 0.6387 sec/batch\n",
      "Epoch 1/3  Iteration 24/534 Training loss: 3.8577 0.9893 sec/batch\n",
      "Epoch 1/3  Iteration 25/534 Training loss: 3.8359 0.6640 sec/batch\n",
      "Epoch 1/3  Iteration 26/534 Training loss: 3.8162 1.2916 sec/batch\n",
      "Epoch 1/3  Iteration 27/534 Training loss: 3.7979 0.6336 sec/batch\n",
      "Epoch 1/3  Iteration 28/534 Training loss: 3.7793 0.9760 sec/batch\n",
      "Epoch 1/3  Iteration 29/534 Training loss: 3.7618 0.9867 sec/batch\n",
      "Epoch 1/3  Iteration 30/534 Training loss: 3.7461 0.9535 sec/batch\n",
      "Epoch 1/3  Iteration 31/534 Training loss: 3.7317 0.7855 sec/batch\n",
      "Epoch 1/3  Iteration 32/534 Training loss: 3.7170 0.9085 sec/batch\n",
      "Epoch 1/3  Iteration 33/534 Training loss: 3.7030 0.8951 sec/batch\n",
      "Epoch 1/3  Iteration 34/534 Training loss: 3.6901 0.6907 sec/batch\n",
      "Epoch 1/3  Iteration 35/534 Training loss: 3.6769 0.6043 sec/batch\n",
      "Epoch 1/3  Iteration 36/534 Training loss: 3.6654 0.9756 sec/batch\n",
      "Epoch 1/3  Iteration 37/534 Training loss: 3.6529 0.6900 sec/batch\n",
      "Epoch 1/3  Iteration 38/534 Training loss: 3.6415 1.0728 sec/batch\n",
      "Epoch 1/3  Iteration 39/534 Training loss: 3.6305 0.7974 sec/batch\n",
      "Epoch 1/3  Iteration 40/534 Training loss: 3.6202 0.8289 sec/batch\n",
      "Epoch 1/3  Iteration 41/534 Training loss: 3.6100 0.6000 sec/batch\n",
      "Epoch 1/3  Iteration 42/534 Training loss: 3.6005 1.3747 sec/batch\n",
      "Epoch 1/3  Iteration 43/534 Training loss: 3.5911 0.7109 sec/batch\n",
      "Epoch 1/3  Iteration 44/534 Training loss: 3.5821 0.8374 sec/batch\n",
      "Epoch 1/3  Iteration 45/534 Training loss: 3.5732 0.8961 sec/batch\n",
      "Epoch 1/3  Iteration 46/534 Training loss: 3.5651 1.0989 sec/batch\n",
      "Epoch 1/3  Iteration 47/534 Training loss: 3.5575 0.9608 sec/batch\n",
      "Epoch 1/3  Iteration 48/534 Training loss: 3.5501 1.2824 sec/batch\n",
      "Epoch 1/3  Iteration 49/534 Training loss: 3.5431 0.6099 sec/batch\n",
      "Epoch 1/3  Iteration 50/534 Training loss: 3.5361 0.8533 sec/batch\n",
      "Epoch 1/3  Iteration 51/534 Training loss: 3.5293 1.1520 sec/batch\n",
      "Epoch 1/3  Iteration 52/534 Training loss: 3.5225 1.1478 sec/batch\n",
      "Epoch 1/3  Iteration 53/534 Training loss: 3.5161 0.5864 sec/batch\n",
      "Epoch 1/3  Iteration 54/534 Training loss: 3.5096 1.3123 sec/batch\n",
      "Epoch 1/3  Iteration 55/534 Training loss: 3.5036 0.6969 sec/batch\n",
      "Epoch 1/3  Iteration 56/534 Training loss: 3.4973 0.7761 sec/batch\n",
      "Epoch 1/3  Iteration 57/534 Training loss: 3.4916 0.8199 sec/batch\n",
      "Epoch 1/3  Iteration 58/534 Training loss: 3.4859 1.0450 sec/batch\n",
      "Epoch 1/3  Iteration 59/534 Training loss: 3.4803 1.1577 sec/batch\n",
      "Epoch 1/3  Iteration 60/534 Training loss: 3.4749 1.0098 sec/batch\n",
      "Epoch 1/3  Iteration 61/534 Training loss: 3.4699 0.8742 sec/batch\n",
      "Epoch 1/3  Iteration 62/534 Training loss: 3.4652 0.8639 sec/batch\n",
      "Epoch 1/3  Iteration 63/534 Training loss: 3.4606 1.0995 sec/batch\n",
      "Epoch 1/3  Iteration 64/534 Training loss: 3.4556 1.2262 sec/batch\n",
      "Epoch 1/3  Iteration 65/534 Training loss: 3.4508 0.7940 sec/batch\n",
      "Epoch 1/3  Iteration 66/534 Training loss: 3.4464 1.1826 sec/batch\n",
      "Epoch 1/3  Iteration 67/534 Training loss: 3.4421 1.0372 sec/batch\n",
      "Epoch 1/3  Iteration 68/534 Training loss: 3.4373 1.7490 sec/batch\n",
      "Epoch 1/3  Iteration 69/534 Training loss: 3.4329 0.9738 sec/batch\n",
      "Epoch 1/3  Iteration 70/534 Training loss: 3.4290 0.9296 sec/batch\n",
      "Epoch 1/3  Iteration 71/534 Training loss: 3.4250 1.5910 sec/batch\n",
      "Epoch 1/3  Iteration 72/534 Training loss: 3.4213 1.2360 sec/batch\n",
      "Epoch 1/3  Iteration 73/534 Training loss: 3.4174 0.9282 sec/batch\n",
      "Epoch 1/3  Iteration 74/534 Training loss: 3.4136 1.3412 sec/batch\n",
      "Epoch 1/3  Iteration 75/534 Training loss: 3.4101 1.1069 sec/batch\n",
      "Epoch 1/3  Iteration 76/534 Training loss: 3.4067 0.9968 sec/batch\n",
      "Epoch 1/3  Iteration 77/534 Training loss: 3.4031 0.9863 sec/batch\n",
      "Epoch 1/3  Iteration 78/534 Training loss: 3.3999 1.0076 sec/batch\n",
      "Epoch 1/3  Iteration 79/534 Training loss: 3.3965 1.3026 sec/batch\n",
      "Epoch 1/3  Iteration 80/534 Training loss: 3.3930 1.6384 sec/batch\n",
      "Epoch 1/3  Iteration 81/534 Training loss: 3.3897 1.2952 sec/batch\n",
      "Epoch 1/3  Iteration 82/534 Training loss: 3.3867 0.8019 sec/batch\n",
      "Epoch 1/3  Iteration 83/534 Training loss: 3.3837 0.7347 sec/batch\n",
      "Epoch 1/3  Iteration 84/534 Training loss: 3.3806 0.6401 sec/batch\n",
      "Epoch 1/3  Iteration 85/534 Training loss: 3.3775 0.5932 sec/batch\n",
      "Epoch 1/3  Iteration 86/534 Training loss: 3.3744 0.6379 sec/batch\n",
      "Epoch 1/3  Iteration 87/534 Training loss: 3.3714 0.7680 sec/batch\n",
      "Epoch 1/3  Iteration 88/534 Training loss: 3.3685 0.6581 sec/batch\n",
      "Epoch 1/3  Iteration 89/534 Training loss: 3.3658 1.0290 sec/batch\n",
      "Epoch 1/3  Iteration 90/534 Training loss: 3.3632 0.9591 sec/batch\n",
      "Epoch 1/3  Iteration 91/534 Training loss: 3.3605 0.9240 sec/batch\n",
      "Epoch 1/3  Iteration 92/534 Training loss: 3.3578 0.9208 sec/batch\n",
      "Epoch 1/3  Iteration 93/534 Training loss: 3.3553 0.7241 sec/batch\n",
      "Epoch 1/3  Iteration 94/534 Training loss: 3.3528 0.5991 sec/batch\n",
      "Epoch 1/3  Iteration 95/534 Training loss: 3.3502 0.8722 sec/batch\n",
      "Epoch 1/3  Iteration 96/534 Training loss: 3.3477 0.7008 sec/batch\n",
      "Epoch 1/3  Iteration 97/534 Training loss: 3.3453 0.7096 sec/batch\n",
      "Epoch 1/3  Iteration 98/534 Training loss: 3.3427 0.7315 sec/batch\n",
      "Epoch 1/3  Iteration 99/534 Training loss: 3.3403 0.6568 sec/batch\n",
      "Epoch 1/3  Iteration 100/534 Training loss: 3.3380 0.6243 sec/batch\n",
      "Epoch 1/3  Iteration 101/534 Training loss: 3.3357 0.8140 sec/batch\n",
      "Epoch 1/3  Iteration 102/534 Training loss: 3.3335 0.8036 sec/batch\n",
      "Epoch 1/3  Iteration 103/534 Training loss: 3.3313 0.8361 sec/batch\n",
      "Epoch 1/3  Iteration 104/534 Training loss: 3.3289 0.8702 sec/batch\n",
      "Epoch 1/3  Iteration 105/534 Training loss: 3.3267 0.8096 sec/batch\n",
      "Epoch 1/3  Iteration 106/534 Training loss: 3.3245 0.6226 sec/batch\n",
      "Epoch 1/3  Iteration 107/534 Training loss: 3.3222 0.5000 sec/batch\n",
      "Epoch 1/3  Iteration 108/534 Training loss: 3.3199 0.5509 sec/batch\n",
      "Epoch 1/3  Iteration 109/534 Training loss: 3.3178 0.9879 sec/batch\n",
      "Epoch 1/3  Iteration 110/534 Training loss: 3.3154 0.7188 sec/batch\n",
      "Epoch 1/3  Iteration 111/534 Training loss: 3.3134 0.7128 sec/batch\n",
      "Epoch 1/3  Iteration 112/534 Training loss: 3.3113 0.6811 sec/batch\n",
      "Epoch 1/3  Iteration 113/534 Training loss: 3.3092 0.6408 sec/batch\n",
      "Epoch 1/3  Iteration 114/534 Training loss: 3.3071 0.5789 sec/batch\n",
      "Epoch 1/3  Iteration 115/534 Training loss: 3.3049 0.5876 sec/batch\n",
      "Epoch 1/3  Iteration 116/534 Training loss: 3.3027 0.6269 sec/batch\n",
      "Epoch 1/3  Iteration 117/534 Training loss: 3.3007 0.7255 sec/batch\n",
      "Epoch 1/3  Iteration 118/534 Training loss: 3.2988 1.0332 sec/batch\n",
      "Epoch 1/3  Iteration 119/534 Training loss: 3.2970 0.8115 sec/batch\n",
      "Epoch 1/3  Iteration 120/534 Training loss: 3.2950 0.7322 sec/batch\n",
      "Epoch 1/3  Iteration 121/534 Training loss: 3.2933 0.6151 sec/batch\n",
      "Epoch 1/3  Iteration 122/534 Training loss: 3.2914 0.5899 sec/batch\n",
      "Epoch 1/3  Iteration 123/534 Training loss: 3.2896 0.5344 sec/batch\n",
      "Epoch 1/3  Iteration 124/534 Training loss: 3.2878 0.5714 sec/batch\n",
      "Epoch 1/3  Iteration 125/534 Training loss: 3.2859 0.6444 sec/batch\n",
      "Epoch 1/3  Iteration 126/534 Training loss: 3.2839 0.7246 sec/batch\n",
      "Epoch 1/3  Iteration 127/534 Training loss: 3.2821 0.7302 sec/batch\n",
      "Epoch 1/3  Iteration 128/534 Training loss: 3.2804 0.7453 sec/batch\n",
      "Epoch 1/3  Iteration 129/534 Training loss: 3.2785 0.7363 sec/batch\n",
      "Epoch 1/3  Iteration 130/534 Training loss: 3.2767 0.8631 sec/batch\n",
      "Epoch 1/3  Iteration 131/534 Training loss: 3.2750 0.5664 sec/batch\n",
      "Epoch 1/3  Iteration 132/534 Training loss: 3.2732 0.6715 sec/batch\n",
      "Epoch 1/3  Iteration 133/534 Training loss: 3.2715 0.7138 sec/batch\n",
      "Epoch 1/3  Iteration 134/534 Training loss: 3.2697 0.9114 sec/batch\n",
      "Epoch 1/3  Iteration 135/534 Training loss: 3.2677 0.7947 sec/batch\n",
      "Epoch 1/3  Iteration 136/534 Training loss: 3.2658 0.6730 sec/batch\n",
      "Epoch 1/3  Iteration 137/534 Training loss: 3.2639 0.5477 sec/batch\n",
      "Epoch 1/3  Iteration 138/534 Training loss: 3.2621 0.5918 sec/batch\n",
      "Epoch 1/3  Iteration 139/534 Training loss: 3.2604 0.5119 sec/batch\n",
      "Epoch 1/3  Iteration 140/534 Training loss: 3.2586 0.5364 sec/batch\n",
      "Epoch 1/3  Iteration 141/534 Training loss: 3.2570 0.5593 sec/batch\n",
      "Epoch 1/3  Iteration 142/534 Training loss: 3.2551 0.5799 sec/batch\n",
      "Epoch 1/3  Iteration 143/534 Training loss: 3.2533 0.5585 sec/batch\n",
      "Epoch 1/3  Iteration 144/534 Training loss: 3.2515 0.6291 sec/batch\n",
      "Epoch 1/3  Iteration 145/534 Training loss: 3.2498 0.6559 sec/batch\n",
      "Epoch 1/3  Iteration 146/534 Training loss: 3.2481 0.5924 sec/batch\n",
      "Epoch 1/3  Iteration 147/534 Training loss: 3.2465 0.5918 sec/batch\n",
      "Epoch 1/3  Iteration 148/534 Training loss: 3.2449 0.7841 sec/batch\n",
      "Epoch 1/3  Iteration 149/534 Training loss: 3.2431 0.8855 sec/batch\n",
      "Epoch 1/3  Iteration 150/534 Training loss: 3.2414 1.1099 sec/batch\n",
      "Epoch 1/3  Iteration 151/534 Training loss: 3.2398 0.7999 sec/batch\n",
      "Epoch 1/3  Iteration 152/534 Training loss: 3.2383 0.8220 sec/batch\n",
      "Epoch 1/3  Iteration 153/534 Training loss: 3.2367 0.7476 sec/batch\n",
      "Epoch 1/3  Iteration 154/534 Training loss: 3.2350 0.6936 sec/batch\n",
      "Epoch 1/3  Iteration 155/534 Training loss: 3.2332 1.0728 sec/batch\n",
      "Epoch 1/3  Iteration 156/534 Training loss: 3.2315 1.0559 sec/batch\n",
      "Epoch 1/3  Iteration 157/534 Training loss: 3.2298 0.5254 sec/batch\n",
      "Epoch 1/3  Iteration 158/534 Training loss: 3.2280 0.5354 sec/batch\n",
      "Epoch 1/3  Iteration 159/534 Training loss: 3.2262 0.5925 sec/batch\n",
      "Epoch 1/3  Iteration 160/534 Training loss: 3.2244 0.5535 sec/batch\n",
      "Epoch 1/3  Iteration 161/534 Training loss: 3.2227 0.5528 sec/batch\n",
      "Epoch 1/3  Iteration 162/534 Training loss: 3.2209 0.6185 sec/batch\n",
      "Epoch 1/3  Iteration 163/534 Training loss: 3.2190 0.8013 sec/batch\n",
      "Epoch 1/3  Iteration 164/534 Training loss: 3.2173 0.7842 sec/batch\n",
      "Epoch 1/3  Iteration 165/534 Training loss: 3.2156 0.8486 sec/batch\n",
      "Epoch 1/3  Iteration 166/534 Training loss: 3.2138 0.7555 sec/batch\n",
      "Epoch 1/3  Iteration 167/534 Training loss: 3.2121 0.6884 sec/batch\n",
      "Epoch 1/3  Iteration 168/534 Training loss: 3.2104 0.7083 sec/batch\n",
      "Epoch 1/3  Iteration 169/534 Training loss: 3.2087 0.7013 sec/batch\n",
      "Epoch 1/3  Iteration 170/534 Training loss: 3.2070 0.7834 sec/batch\n",
      "Epoch 1/3  Iteration 171/534 Training loss: 3.2053 0.8824 sec/batch\n",
      "Epoch 1/3  Iteration 172/534 Training loss: 3.2038 0.7840 sec/batch\n",
      "Epoch 1/3  Iteration 173/534 Training loss: 3.2023 1.0643 sec/batch\n",
      "Epoch 1/3  Iteration 174/534 Training loss: 3.2009 1.0337 sec/batch\n",
      "Epoch 1/3  Iteration 175/534 Training loss: 3.1993 1.1487 sec/batch\n",
      "Epoch 1/3  Iteration 176/534 Training loss: 3.1976 1.0874 sec/batch\n",
      "Epoch 1/3  Iteration 177/534 Training loss: 3.1959 0.8940 sec/batch\n",
      "Epoch 1/3  Iteration 178/534 Training loss: 3.1941 0.7480 sec/batch\n",
      "Epoch 2/3  Iteration 179/534 Training loss: 2.9456 0.7579 sec/batch\n",
      "Epoch 2/3  Iteration 180/534 Training loss: 2.9008 0.5736 sec/batch\n",
      "Epoch 2/3  Iteration 181/534 Training loss: 2.8868 0.5919 sec/batch\n",
      "Epoch 2/3  Iteration 182/534 Training loss: 2.8806 0.5824 sec/batch\n",
      "Epoch 2/3  Iteration 183/534 Training loss: 2.8791 0.6058 sec/batch\n",
      "Epoch 2/3  Iteration 184/534 Training loss: 2.8768 0.5598 sec/batch\n",
      "Epoch 2/3  Iteration 185/534 Training loss: 2.8762 0.6004 sec/batch\n",
      "Epoch 2/3  Iteration 186/534 Training loss: 2.8759 0.7239 sec/batch\n",
      "Epoch 2/3  Iteration 187/534 Training loss: 2.8734 0.5949 sec/batch\n",
      "Epoch 2/3  Iteration 188/534 Training loss: 2.8711 0.6665 sec/batch\n",
      "Epoch 2/3  Iteration 189/534 Training loss: 2.8675 0.7252 sec/batch\n",
      "Epoch 2/3  Iteration 190/534 Training loss: 2.8661 0.7711 sec/batch\n",
      "Epoch 2/3  Iteration 191/534 Training loss: 2.8641 0.8071 sec/batch\n",
      "Epoch 2/3  Iteration 192/534 Training loss: 2.8637 0.9179 sec/batch\n",
      "Epoch 2/3  Iteration 193/534 Training loss: 2.8626 0.6633 sec/batch\n",
      "Epoch 2/3  Iteration 194/534 Training loss: 2.8613 0.5531 sec/batch\n",
      "Epoch 2/3  Iteration 195/534 Training loss: 2.8593 0.6022 sec/batch\n",
      "Epoch 2/3  Iteration 196/534 Training loss: 2.8594 0.6194 sec/batch\n",
      "Epoch 2/3  Iteration 197/534 Training loss: 2.8582 0.6366 sec/batch\n",
      "Epoch 2/3  Iteration 198/534 Training loss: 2.8550 0.6113 sec/batch\n",
      "Epoch 2/3  Iteration 199/534 Training loss: 2.8529 0.5893 sec/batch\n",
      "Epoch 2/3  Iteration 200/534 Training loss: 2.8516 0.5879 sec/batch\n",
      "Epoch 2/3  Iteration 201/534 Training loss: 2.8498 0.5629 sec/batch\n",
      "Epoch 2/3  Iteration 202/534 Training loss: 2.8477 0.7571 sec/batch\n",
      "Epoch 2/3  Iteration 203/534 Training loss: 2.8456 0.5634 sec/batch\n",
      "Epoch 2/3  Iteration 204/534 Training loss: 2.8442 0.6394 sec/batch\n",
      "Epoch 2/3  Iteration 205/534 Training loss: 2.8429 0.8573 sec/batch\n",
      "Epoch 2/3  Iteration 206/534 Training loss: 2.8409 0.7417 sec/batch\n",
      "Epoch 2/3  Iteration 207/534 Training loss: 2.8394 0.7454 sec/batch\n",
      "Epoch 2/3  Iteration 208/534 Training loss: 2.8379 0.6353 sec/batch\n",
      "Epoch 2/3  Iteration 209/534 Training loss: 2.8371 0.5589 sec/batch\n",
      "Epoch 2/3  Iteration 210/534 Training loss: 2.8353 0.8387 sec/batch\n",
      "Epoch 2/3  Iteration 211/534 Training loss: 2.8332 0.7721 sec/batch\n",
      "Epoch 2/3  Iteration 212/534 Training loss: 2.8316 0.6753 sec/batch\n",
      "Epoch 2/3  Iteration 213/534 Training loss: 2.8295 0.7263 sec/batch\n",
      "Epoch 2/3  Iteration 214/534 Training loss: 2.8282 0.6533 sec/batch\n",
      "Epoch 2/3  Iteration 215/534 Training loss: 2.8262 0.5010 sec/batch\n",
      "Epoch 2/3  Iteration 216/534 Training loss: 2.8239 0.5041 sec/batch\n",
      "Epoch 2/3  Iteration 217/534 Training loss: 2.8218 0.5714 sec/batch\n",
      "Epoch 2/3  Iteration 218/534 Training loss: 2.8199 0.5476 sec/batch\n",
      "Epoch 2/3  Iteration 219/534 Training loss: 2.8178 0.5920 sec/batch\n",
      "Epoch 2/3  Iteration 220/534 Training loss: 2.8159 0.5848 sec/batch\n",
      "Epoch 2/3  Iteration 221/534 Training loss: 2.8138 0.7081 sec/batch\n",
      "Epoch 2/3  Iteration 222/534 Training loss: 2.8118 0.9558 sec/batch\n",
      "Epoch 2/3  Iteration 223/534 Training loss: 2.8098 0.7374 sec/batch\n",
      "Epoch 2/3  Iteration 224/534 Training loss: 2.8078 0.8818 sec/batch\n",
      "Epoch 2/3  Iteration 225/534 Training loss: 2.8063 0.7096 sec/batch\n",
      "Epoch 2/3  Iteration 226/534 Training loss: 2.8047 1.1806 sec/batch\n",
      "Epoch 2/3  Iteration 227/534 Training loss: 2.8031 0.8401 sec/batch\n",
      "Epoch 2/3  Iteration 228/534 Training loss: 2.8019 0.7484 sec/batch\n",
      "Epoch 2/3  Iteration 229/534 Training loss: 2.8001 0.7415 sec/batch\n",
      "Epoch 2/3  Iteration 230/534 Training loss: 2.7985 0.7228 sec/batch\n",
      "Epoch 2/3  Iteration 231/534 Training loss: 2.7967 0.7199 sec/batch\n",
      "Epoch 2/3  Iteration 232/534 Training loss: 2.7948 0.7171 sec/batch\n",
      "Epoch 2/3  Iteration 233/534 Training loss: 2.7930 0.6211 sec/batch\n",
      "Epoch 2/3  Iteration 234/534 Training loss: 2.7913 0.7189 sec/batch\n",
      "Epoch 2/3  Iteration 235/534 Training loss: 2.7898 0.7316 sec/batch\n",
      "Epoch 2/3  Iteration 236/534 Training loss: 2.7879 0.8501 sec/batch\n",
      "Epoch 2/3  Iteration 237/534 Training loss: 2.7862 0.8695 sec/batch\n",
      "Epoch 2/3  Iteration 238/534 Training loss: 2.7847 0.7052 sec/batch\n",
      "Epoch 2/3  Iteration 239/534 Training loss: 2.7831 0.7427 sec/batch\n",
      "Epoch 2/3  Iteration 240/534 Training loss: 2.7817 0.6151 sec/batch\n",
      "Epoch 2/3  Iteration 241/534 Training loss: 2.7805 0.6255 sec/batch\n",
      "Epoch 2/3  Iteration 242/534 Training loss: 2.7789 0.6176 sec/batch\n",
      "Epoch 2/3  Iteration 243/534 Training loss: 2.7771 0.5753 sec/batch\n",
      "Epoch 2/3  Iteration 244/534 Training loss: 2.7759 0.6161 sec/batch\n",
      "Epoch 2/3  Iteration 245/534 Training loss: 2.7744 0.5931 sec/batch\n",
      "Epoch 2/3  Iteration 246/534 Training loss: 2.7723 0.8282 sec/batch\n",
      "Epoch 2/3  Iteration 247/534 Training loss: 2.7705 0.7485 sec/batch\n",
      "Epoch 2/3  Iteration 248/534 Training loss: 2.7691 0.7736 sec/batch\n",
      "Epoch 2/3  Iteration 249/534 Training loss: 2.7676 0.7512 sec/batch\n",
      "Epoch 2/3  Iteration 250/534 Training loss: 2.7664 0.8511 sec/batch\n",
      "Epoch 2/3  Iteration 251/534 Training loss: 2.7648 0.6982 sec/batch\n",
      "Epoch 2/3  Iteration 252/534 Training loss: 2.7635 0.5777 sec/batch\n",
      "Epoch 2/3  Iteration 253/534 Training loss: 2.7622 0.5875 sec/batch\n",
      "Epoch 2/3  Iteration 254/534 Training loss: 2.7610 0.5996 sec/batch\n",
      "Epoch 2/3  Iteration 255/534 Training loss: 2.7596 0.6309 sec/batch\n",
      "Epoch 2/3  Iteration 256/534 Training loss: 2.7583 0.7702 sec/batch\n",
      "Epoch 2/3  Iteration 257/534 Training loss: 2.7567 0.6279 sec/batch\n",
      "Epoch 2/3  Iteration 258/534 Training loss: 2.7553 0.6067 sec/batch\n",
      "Epoch 2/3  Iteration 259/534 Training loss: 2.7537 0.5880 sec/batch\n",
      "Epoch 2/3  Iteration 260/534 Training loss: 2.7526 0.5358 sec/batch\n",
      "Epoch 2/3  Iteration 261/534 Training loss: 2.7512 0.5975 sec/batch\n",
      "Epoch 2/3  Iteration 262/534 Training loss: 2.7497 0.6168 sec/batch\n",
      "Epoch 2/3  Iteration 263/534 Training loss: 2.7480 0.7553 sec/batch\n",
      "Epoch 2/3  Iteration 264/534 Training loss: 2.7466 0.7990 sec/batch\n",
      "Epoch 2/3  Iteration 265/534 Training loss: 2.7452 0.7292 sec/batch\n",
      "Epoch 2/3  Iteration 266/534 Training loss: 2.7438 0.7340 sec/batch\n",
      "Epoch 2/3  Iteration 267/534 Training loss: 2.7425 0.7975 sec/batch\n",
      "Epoch 2/3  Iteration 268/534 Training loss: 2.7413 0.6893 sec/batch\n",
      "Epoch 2/3  Iteration 269/534 Training loss: 2.7400 0.5545 sec/batch\n",
      "Epoch 2/3  Iteration 270/534 Training loss: 2.7387 0.5457 sec/batch\n",
      "Epoch 2/3  Iteration 271/534 Training loss: 2.7374 0.5879 sec/batch\n",
      "Epoch 2/3  Iteration 272/534 Training loss: 2.7359 0.5528 sec/batch\n",
      "Epoch 2/3  Iteration 273/534 Training loss: 2.7344 0.5595 sec/batch\n",
      "Epoch 2/3  Iteration 274/534 Training loss: 2.7330 0.5418 sec/batch\n",
      "Epoch 2/3  Iteration 275/534 Training loss: 2.7317 0.5305 sec/batch\n",
      "Epoch 2/3  Iteration 276/534 Training loss: 2.7304 0.6059 sec/batch\n",
      "Epoch 2/3  Iteration 277/534 Training loss: 2.7292 0.5617 sec/batch\n",
      "Epoch 2/3  Iteration 278/534 Training loss: 2.7279 0.5706 sec/batch\n",
      "Epoch 2/3  Iteration 279/534 Training loss: 2.7267 0.5669 sec/batch\n",
      "Epoch 2/3  Iteration 280/534 Training loss: 2.7255 0.5845 sec/batch\n",
      "Epoch 2/3  Iteration 281/534 Training loss: 2.7241 0.6297 sec/batch\n",
      "Epoch 2/3  Iteration 282/534 Training loss: 2.7228 0.6851 sec/batch\n",
      "Epoch 2/3  Iteration 283/534 Training loss: 2.7214 0.7322 sec/batch\n",
      "Epoch 2/3  Iteration 284/534 Training loss: 2.7202 0.7662 sec/batch\n",
      "Epoch 2/3  Iteration 285/534 Training loss: 2.7189 0.5322 sec/batch\n",
      "Epoch 2/3  Iteration 286/534 Training loss: 2.7178 0.5698 sec/batch\n",
      "Epoch 2/3  Iteration 287/534 Training loss: 2.7168 0.5891 sec/batch\n",
      "Epoch 2/3  Iteration 288/534 Training loss: 2.7153 0.5929 sec/batch\n",
      "Epoch 2/3  Iteration 289/534 Training loss: 2.7142 0.5267 sec/batch\n",
      "Epoch 2/3  Iteration 290/534 Training loss: 2.7132 0.5528 sec/batch\n",
      "Epoch 2/3  Iteration 291/534 Training loss: 2.7120 0.5615 sec/batch\n",
      "Epoch 2/3  Iteration 292/534 Training loss: 2.7107 0.5865 sec/batch\n",
      "Epoch 2/3  Iteration 293/534 Training loss: 2.7094 0.5747 sec/batch\n",
      "Epoch 2/3  Iteration 294/534 Training loss: 2.7080 0.5791 sec/batch\n",
      "Epoch 2/3  Iteration 295/534 Training loss: 2.7068 0.5294 sec/batch\n",
      "Epoch 2/3  Iteration 296/534 Training loss: 2.7057 0.5499 sec/batch\n",
      "Epoch 2/3  Iteration 297/534 Training loss: 2.7047 0.6030 sec/batch\n",
      "Epoch 2/3  Iteration 298/534 Training loss: 2.7036 0.6069 sec/batch\n",
      "Epoch 2/3  Iteration 299/534 Training loss: 2.7027 0.6758 sec/batch\n",
      "Epoch 2/3  Iteration 300/534 Training loss: 2.7016 0.6767 sec/batch\n",
      "Epoch 2/3  Iteration 301/534 Training loss: 2.7005 0.7488 sec/batch\n",
      "Epoch 2/3  Iteration 302/534 Training loss: 2.6995 0.7732 sec/batch\n",
      "Epoch 2/3  Iteration 303/534 Training loss: 2.6983 0.6227 sec/batch\n",
      "Epoch 2/3  Iteration 304/534 Training loss: 2.6971 0.5697 sec/batch\n",
      "Epoch 2/3  Iteration 305/534 Training loss: 2.6961 0.5569 sec/batch\n",
      "Epoch 2/3  Iteration 306/534 Training loss: 2.6952 0.5569 sec/batch\n",
      "Epoch 2/3  Iteration 307/534 Training loss: 2.6940 0.5710 sec/batch\n",
      "Epoch 2/3  Iteration 308/534 Training loss: 2.6930 0.5506 sec/batch\n",
      "Epoch 2/3  Iteration 309/534 Training loss: 2.6920 0.5791 sec/batch\n",
      "Epoch 2/3  Iteration 310/534 Training loss: 2.6908 0.5353 sec/batch\n",
      "Epoch 2/3  Iteration 311/534 Training loss: 2.6898 0.5605 sec/batch\n",
      "Epoch 2/3  Iteration 312/534 Training loss: 2.6889 0.5545 sec/batch\n",
      "Epoch 2/3  Iteration 313/534 Training loss: 2.6876 0.5526 sec/batch\n",
      "Epoch 2/3  Iteration 314/534 Training loss: 2.6865 0.5499 sec/batch\n",
      "Epoch 2/3  Iteration 315/534 Training loss: 2.6855 0.5833 sec/batch\n",
      "Epoch 2/3  Iteration 316/534 Training loss: 2.6844 0.7622 sec/batch\n",
      "Epoch 2/3  Iteration 317/534 Training loss: 2.6836 0.8751 sec/batch\n",
      "Epoch 2/3  Iteration 318/534 Training loss: 2.6826 1.4769 sec/batch\n",
      "Epoch 2/3  Iteration 319/534 Training loss: 2.6817 1.0572 sec/batch\n",
      "Epoch 2/3  Iteration 320/534 Training loss: 2.6806 1.0673 sec/batch\n",
      "Epoch 2/3  Iteration 321/534 Training loss: 2.6796 0.8272 sec/batch\n",
      "Epoch 2/3  Iteration 322/534 Training loss: 2.6785 0.9753 sec/batch\n",
      "Epoch 2/3  Iteration 323/534 Training loss: 2.6776 1.0746 sec/batch\n",
      "Epoch 2/3  Iteration 324/534 Training loss: 2.6767 0.9040 sec/batch\n",
      "Epoch 2/3  Iteration 325/534 Training loss: 2.6757 0.8917 sec/batch\n",
      "Epoch 2/3  Iteration 326/534 Training loss: 2.6749 0.9558 sec/batch\n",
      "Epoch 2/3  Iteration 327/534 Training loss: 2.6739 1.0774 sec/batch\n",
      "Epoch 2/3  Iteration 328/534 Training loss: 2.6729 0.7346 sec/batch\n",
      "Epoch 2/3  Iteration 329/534 Training loss: 2.6722 0.8833 sec/batch\n",
      "Epoch 2/3  Iteration 330/534 Training loss: 2.6715 1.1260 sec/batch\n",
      "Epoch 2/3  Iteration 331/534 Training loss: 2.6707 0.7155 sec/batch\n",
      "Epoch 2/3  Iteration 332/534 Training loss: 2.6699 0.6152 sec/batch\n",
      "Epoch 2/3  Iteration 333/534 Training loss: 2.6689 0.5863 sec/batch\n",
      "Epoch 2/3  Iteration 334/534 Training loss: 2.6680 0.5581 sec/batch\n",
      "Epoch 2/3  Iteration 335/534 Training loss: 2.6670 0.5601 sec/batch\n",
      "Epoch 2/3  Iteration 336/534 Training loss: 2.6660 0.4884 sec/batch\n",
      "Epoch 2/3  Iteration 337/534 Training loss: 2.6650 0.5543 sec/batch\n",
      "Epoch 2/3  Iteration 338/534 Training loss: 2.6642 0.5853 sec/batch\n",
      "Epoch 2/3  Iteration 339/534 Training loss: 2.6633 0.5658 sec/batch\n",
      "Epoch 2/3  Iteration 340/534 Training loss: 2.6622 0.5664 sec/batch\n",
      "Epoch 2/3  Iteration 341/534 Training loss: 2.6612 0.5551 sec/batch\n",
      "Epoch 2/3  Iteration 342/534 Training loss: 2.6602 0.7080 sec/batch\n",
      "Epoch 2/3  Iteration 343/534 Training loss: 2.6594 0.8224 sec/batch\n",
      "Epoch 2/3  Iteration 344/534 Training loss: 2.6585 0.8147 sec/batch\n",
      "Epoch 2/3  Iteration 345/534 Training loss: 2.6577 0.8774 sec/batch\n",
      "Epoch 2/3  Iteration 346/534 Training loss: 2.6569 0.5751 sec/batch\n",
      "Epoch 2/3  Iteration 347/534 Training loss: 2.6561 0.6889 sec/batch\n",
      "Epoch 2/3  Iteration 348/534 Training loss: 2.6551 0.5539 sec/batch\n",
      "Epoch 2/3  Iteration 349/534 Training loss: 2.6543 0.4892 sec/batch\n",
      "Epoch 2/3  Iteration 350/534 Training loss: 2.6536 0.5589 sec/batch\n",
      "Epoch 2/3  Iteration 351/534 Training loss: 2.6530 0.8459 sec/batch\n",
      "Epoch 2/3  Iteration 352/534 Training loss: 2.6524 0.5456 sec/batch\n",
      "Epoch 2/3  Iteration 353/534 Training loss: 2.6518 0.5293 sec/batch\n",
      "Epoch 2/3  Iteration 354/534 Training loss: 2.6510 0.6054 sec/batch\n",
      "Epoch 2/3  Iteration 355/534 Training loss: 2.6501 0.6730 sec/batch\n",
      "Epoch 2/3  Iteration 356/534 Training loss: 2.6491 0.6680 sec/batch\n",
      "Epoch 3/3  Iteration 357/534 Training loss: 2.5654 0.7189 sec/batch\n",
      "Epoch 3/3  Iteration 358/534 Training loss: 2.5191 0.8305 sec/batch\n",
      "Epoch 3/3  Iteration 359/534 Training loss: 2.5047 0.8541 sec/batch\n",
      "Epoch 3/3  Iteration 360/534 Training loss: 2.4988 0.6836 sec/batch\n",
      "Epoch 3/3  Iteration 361/534 Training loss: 2.4961 0.6022 sec/batch\n",
      "Epoch 3/3  Iteration 362/534 Training loss: 2.4934 0.7226 sec/batch\n",
      "Epoch 3/3  Iteration 363/534 Training loss: 2.4928 0.5978 sec/batch\n",
      "Epoch 3/3  Iteration 364/534 Training loss: 2.4939 0.6380 sec/batch\n",
      "Epoch 3/3  Iteration 365/534 Training loss: 2.4950 0.5949 sec/batch\n",
      "Epoch 3/3  Iteration 366/534 Training loss: 2.4932 0.6371 sec/batch\n",
      "Epoch 3/3  Iteration 367/534 Training loss: 2.4914 0.5189 sec/batch\n",
      "Epoch 3/3  Iteration 368/534 Training loss: 2.4911 0.5702 sec/batch\n",
      "Epoch 3/3  Iteration 369/534 Training loss: 2.4902 0.6282 sec/batch\n",
      "Epoch 3/3  Iteration 370/534 Training loss: 2.4922 0.6110 sec/batch\n",
      "Epoch 3/3  Iteration 371/534 Training loss: 2.4918 0.6100 sec/batch\n",
      "Epoch 3/3  Iteration 372/534 Training loss: 2.4918 0.7340 sec/batch\n",
      "Epoch 3/3  Iteration 373/534 Training loss: 2.4912 0.6988 sec/batch\n",
      "Epoch 3/3  Iteration 374/534 Training loss: 2.4928 0.6854 sec/batch\n",
      "Epoch 3/3  Iteration 375/534 Training loss: 2.4931 0.7062 sec/batch\n",
      "Epoch 3/3  Iteration 376/534 Training loss: 2.4916 0.6984 sec/batch\n",
      "Epoch 3/3  Iteration 377/534 Training loss: 2.4908 0.6040 sec/batch\n",
      "Epoch 3/3  Iteration 378/534 Training loss: 2.4916 0.5551 sec/batch\n",
      "Epoch 3/3  Iteration 379/534 Training loss: 2.4912 0.5469 sec/batch\n",
      "Epoch 3/3  Iteration 380/534 Training loss: 2.4904 0.6999 sec/batch\n",
      "Epoch 3/3  Iteration 381/534 Training loss: 2.4895 0.8598 sec/batch\n",
      "Epoch 3/3  Iteration 382/534 Training loss: 2.4892 0.5852 sec/batch\n",
      "Epoch 3/3  Iteration 383/534 Training loss: 2.4885 0.6047 sec/batch\n",
      "Epoch 3/3  Iteration 384/534 Training loss: 2.4881 0.6156 sec/batch\n",
      "Epoch 3/3  Iteration 385/534 Training loss: 2.4881 0.4971 sec/batch\n",
      "Epoch 3/3  Iteration 386/534 Training loss: 2.4877 0.4766 sec/batch\n",
      "Epoch 3/3  Iteration 387/534 Training loss: 2.4883 0.5923 sec/batch\n",
      "Epoch 3/3  Iteration 388/534 Training loss: 2.4873 0.5885 sec/batch\n",
      "Epoch 3/3  Iteration 389/534 Training loss: 2.4864 0.6460 sec/batch\n",
      "Epoch 3/3  Iteration 390/534 Training loss: 2.4861 0.7923 sec/batch\n",
      "Epoch 3/3  Iteration 391/534 Training loss: 2.4855 0.6537 sec/batch\n",
      "Epoch 3/3  Iteration 392/534 Training loss: 2.4851 0.7004 sec/batch\n",
      "Epoch 3/3  Iteration 393/534 Training loss: 2.4845 0.5712 sec/batch\n",
      "Epoch 3/3  Iteration 394/534 Training loss: 2.4833 0.7861 sec/batch\n",
      "Epoch 3/3  Iteration 395/534 Training loss: 2.4823 0.9852 sec/batch\n",
      "Epoch 3/3  Iteration 396/534 Training loss: 2.4815 0.7608 sec/batch\n",
      "Epoch 3/3  Iteration 397/534 Training loss: 2.4804 0.6713 sec/batch\n",
      "Epoch 3/3  Iteration 398/534 Training loss: 2.4795 0.6507 sec/batch\n",
      "Epoch 3/3  Iteration 399/534 Training loss: 2.4785 0.5849 sec/batch\n",
      "Epoch 3/3  Iteration 400/534 Training loss: 2.4776 0.5980 sec/batch\n",
      "Epoch 3/3  Iteration 401/534 Training loss: 2.4767 0.5365 sec/batch\n",
      "Epoch 3/3  Iteration 402/534 Training loss: 2.4753 0.5915 sec/batch\n",
      "Epoch 3/3  Iteration 403/534 Training loss: 2.4752 0.5953 sec/batch\n",
      "Epoch 3/3  Iteration 404/534 Training loss: 2.4747 0.5959 sec/batch\n",
      "Epoch 3/3  Iteration 405/534 Training loss: 2.4742 0.7110 sec/batch\n",
      "Epoch 3/3  Iteration 406/534 Training loss: 2.4743 0.8116 sec/batch\n",
      "Epoch 3/3  Iteration 407/534 Training loss: 2.4736 0.7492 sec/batch\n",
      "Epoch 3/3  Iteration 408/534 Training loss: 2.4732 0.9110 sec/batch\n",
      "Epoch 3/3  Iteration 409/534 Training loss: 2.4725 0.6247 sec/batch\n",
      "Epoch 3/3  Iteration 410/534 Training loss: 2.4718 0.6177 sec/batch\n",
      "Epoch 3/3  Iteration 411/534 Training loss: 2.4711 0.8937 sec/batch\n",
      "Epoch 3/3  Iteration 412/534 Training loss: 2.4707 0.9979 sec/batch\n",
      "Epoch 3/3  Iteration 413/534 Training loss: 2.4703 1.0904 sec/batch\n",
      "Epoch 3/3  Iteration 414/534 Training loss: 2.4696 0.9225 sec/batch\n",
      "Epoch 3/3  Iteration 415/534 Training loss: 2.4690 0.6957 sec/batch\n",
      "Epoch 3/3  Iteration 416/534 Training loss: 2.4687 0.7458 sec/batch\n",
      "Epoch 3/3  Iteration 417/534 Training loss: 2.4681 0.7654 sec/batch\n",
      "Epoch 3/3  Iteration 418/534 Training loss: 2.4679 0.8024 sec/batch\n",
      "Epoch 3/3  Iteration 419/534 Training loss: 2.4679 0.7793 sec/batch\n",
      "Epoch 3/3  Iteration 420/534 Training loss: 2.4674 0.8276 sec/batch\n",
      "Epoch 3/3  Iteration 421/534 Training loss: 2.4668 0.6290 sec/batch\n",
      "Epoch 3/3  Iteration 422/534 Training loss: 2.4666 0.7565 sec/batch\n",
      "Epoch 3/3  Iteration 423/534 Training loss: 2.4662 0.7297 sec/batch\n",
      "Epoch 3/3  Iteration 424/534 Training loss: 2.4653 0.6278 sec/batch\n",
      "Epoch 3/3  Iteration 425/534 Training loss: 2.4645 0.6005 sec/batch\n",
      "Epoch 3/3  Iteration 426/534 Training loss: 2.4641 0.6308 sec/batch\n",
      "Epoch 3/3  Iteration 427/534 Training loss: 2.4637 0.7418 sec/batch\n",
      "Epoch 3/3  Iteration 428/534 Training loss: 2.4635 0.6685 sec/batch\n",
      "Epoch 3/3  Iteration 429/534 Training loss: 2.4631 0.6782 sec/batch\n",
      "Epoch 3/3  Iteration 430/534 Training loss: 2.4625 0.6677 sec/batch\n",
      "Epoch 3/3  Iteration 431/534 Training loss: 2.4620 1.5905 sec/batch\n",
      "Epoch 3/3  Iteration 432/534 Training loss: 2.4621 0.7821 sec/batch\n",
      "Epoch 3/3  Iteration 433/534 Training loss: 2.4617 0.6624 sec/batch\n",
      "Epoch 3/3  Iteration 434/534 Training loss: 2.4616 0.6510 sec/batch\n",
      "Epoch 3/3  Iteration 435/534 Training loss: 2.4611 1.0508 sec/batch\n",
      "Epoch 3/3  Iteration 436/534 Training loss: 2.4606 0.7555 sec/batch\n",
      "Epoch 3/3  Iteration 437/534 Training loss: 2.4601 0.5214 sec/batch\n",
      "Epoch 3/3  Iteration 438/534 Training loss: 2.4599 0.5836 sec/batch\n",
      "Epoch 3/3  Iteration 439/534 Training loss: 2.4594 0.6654 sec/batch\n",
      "Epoch 3/3  Iteration 440/534 Training loss: 2.4588 0.6849 sec/batch\n",
      "Epoch 3/3  Iteration 441/534 Training loss: 2.4579 0.6564 sec/batch\n",
      "Epoch 3/3  Iteration 442/534 Training loss: 2.4574 0.5659 sec/batch\n",
      "Epoch 3/3  Iteration 443/534 Training loss: 2.4569 0.6649 sec/batch\n",
      "Epoch 3/3  Iteration 444/534 Training loss: 2.4565 0.6769 sec/batch\n",
      "Epoch 3/3  Iteration 445/534 Training loss: 2.4560 0.8488 sec/batch\n",
      "Epoch 3/3  Iteration 446/534 Training loss: 2.4555 0.7872 sec/batch\n",
      "Epoch 3/3  Iteration 447/534 Training loss: 2.4551 0.7663 sec/batch\n",
      "Epoch 3/3  Iteration 448/534 Training loss: 2.4547 0.7163 sec/batch\n",
      "Epoch 3/3  Iteration 449/534 Training loss: 2.4543 0.6328 sec/batch\n",
      "Epoch 3/3  Iteration 450/534 Training loss: 2.4537 0.5859 sec/batch\n",
      "Epoch 3/3  Iteration 451/534 Training loss: 2.4532 0.6104 sec/batch\n",
      "Epoch 3/3  Iteration 452/534 Training loss: 2.4526 0.8060 sec/batch\n",
      "Epoch 3/3  Iteration 453/534 Training loss: 2.4522 0.9831 sec/batch\n",
      "Epoch 3/3  Iteration 454/534 Training loss: 2.4517 0.8085 sec/batch\n",
      "Epoch 3/3  Iteration 455/534 Training loss: 2.4513 0.8786 sec/batch\n",
      "Epoch 3/3  Iteration 456/534 Training loss: 2.4509 0.6533 sec/batch\n",
      "Epoch 3/3  Iteration 457/534 Training loss: 2.4506 0.5902 sec/batch\n",
      "Epoch 3/3  Iteration 458/534 Training loss: 2.4502 0.6455 sec/batch\n",
      "Epoch 3/3  Iteration 459/534 Training loss: 2.4497 0.7615 sec/batch\n",
      "Epoch 3/3  Iteration 460/534 Training loss: 2.4492 0.7922 sec/batch\n",
      "Epoch 3/3  Iteration 461/534 Training loss: 2.4488 0.7531 sec/batch\n",
      "Epoch 3/3  Iteration 462/534 Training loss: 2.4485 0.6704 sec/batch\n",
      "Epoch 3/3  Iteration 463/534 Training loss: 2.4480 0.5934 sec/batch\n",
      "Epoch 3/3  Iteration 464/534 Training loss: 2.4478 0.6437 sec/batch\n",
      "Epoch 3/3  Iteration 465/534 Training loss: 2.4475 0.6489 sec/batch\n",
      "Epoch 3/3  Iteration 466/534 Training loss: 2.4470 0.6306 sec/batch\n",
      "Epoch 3/3  Iteration 467/534 Training loss: 2.4466 0.6351 sec/batch\n",
      "Epoch 3/3  Iteration 468/534 Training loss: 2.4465 0.5873 sec/batch\n",
      "Epoch 3/3  Iteration 469/534 Training loss: 2.4461 0.7255 sec/batch\n",
      "Epoch 3/3  Iteration 470/534 Training loss: 2.4456 0.6534 sec/batch\n",
      "Epoch 3/3  Iteration 471/534 Training loss: 2.4452 0.6093 sec/batch\n",
      "Epoch 3/3  Iteration 472/534 Training loss: 2.4445 0.6053 sec/batch\n",
      "Epoch 3/3  Iteration 473/534 Training loss: 2.4441 0.5986 sec/batch\n",
      "Epoch 3/3  Iteration 474/534 Training loss: 2.4439 0.6327 sec/batch\n",
      "Epoch 3/3  Iteration 475/534 Training loss: 2.4437 0.8367 sec/batch\n",
      "Epoch 3/3  Iteration 476/534 Training loss: 2.4434 0.7377 sec/batch\n",
      "Epoch 3/3  Iteration 477/534 Training loss: 2.4432 0.7559 sec/batch\n",
      "Epoch 3/3  Iteration 478/534 Training loss: 2.4428 0.7020 sec/batch\n",
      "Epoch 3/3  Iteration 479/534 Training loss: 2.4424 0.6379 sec/batch\n",
      "Epoch 3/3  Iteration 480/534 Training loss: 2.4422 0.6387 sec/batch\n",
      "Epoch 3/3  Iteration 481/534 Training loss: 2.4418 0.5966 sec/batch\n",
      "Epoch 3/3  Iteration 482/534 Training loss: 2.4414 0.6178 sec/batch\n",
      "Epoch 3/3  Iteration 483/534 Training loss: 2.4411 0.6601 sec/batch\n",
      "Epoch 3/3  Iteration 484/534 Training loss: 2.4409 0.6216 sec/batch\n",
      "Epoch 3/3  Iteration 485/534 Training loss: 2.4406 0.6698 sec/batch\n",
      "Epoch 3/3  Iteration 486/534 Training loss: 2.4403 0.6966 sec/batch\n",
      "Epoch 3/3  Iteration 487/534 Training loss: 2.4400 0.6580 sec/batch\n",
      "Epoch 3/3  Iteration 488/534 Training loss: 2.4395 0.6559 sec/batch\n",
      "Epoch 3/3  Iteration 489/534 Training loss: 2.4393 0.6457 sec/batch\n",
      "Epoch 3/3  Iteration 490/534 Training loss: 2.4391 0.6661 sec/batch\n",
      "Epoch 3/3  Iteration 491/534 Training loss: 2.4386 0.7541 sec/batch\n",
      "Epoch 3/3  Iteration 492/534 Training loss: 2.4382 0.7600 sec/batch\n",
      "Epoch 3/3  Iteration 493/534 Training loss: 2.4379 0.8009 sec/batch\n",
      "Epoch 3/3  Iteration 494/534 Training loss: 2.4377 0.6426 sec/batch\n",
      "Epoch 3/3  Iteration 495/534 Training loss: 2.4375 0.6245 sec/batch\n",
      "Epoch 3/3  Iteration 496/534 Training loss: 2.4371 0.5887 sec/batch\n",
      "Epoch 3/3  Iteration 497/534 Training loss: 2.4370 0.6469 sec/batch\n",
      "Epoch 3/3  Iteration 498/534 Training loss: 2.4366 0.5738 sec/batch\n",
      "Epoch 3/3  Iteration 499/534 Training loss: 2.4363 0.7009 sec/batch\n",
      "Epoch 3/3  Iteration 500/534 Training loss: 2.4359 0.7564 sec/batch\n",
      "Epoch 3/3  Iteration 501/534 Training loss: 2.4356 0.6216 sec/batch\n",
      "Epoch 3/3  Iteration 502/534 Training loss: 2.4355 0.5990 sec/batch\n",
      "Epoch 3/3  Iteration 503/534 Training loss: 2.4352 0.5807 sec/batch\n",
      "Epoch 3/3  Iteration 504/534 Training loss: 2.4350 0.6545 sec/batch\n",
      "Epoch 3/3  Iteration 505/534 Training loss: 2.4346 0.6275 sec/batch\n",
      "Epoch 3/3  Iteration 506/534 Training loss: 2.4341 0.5930 sec/batch\n",
      "Epoch 3/3  Iteration 507/534 Training loss: 2.4340 0.5913 sec/batch\n",
      "Epoch 3/3  Iteration 508/534 Training loss: 2.4340 0.5715 sec/batch\n",
      "Epoch 3/3  Iteration 509/534 Training loss: 2.4338 0.6462 sec/batch\n",
      "Epoch 3/3  Iteration 510/534 Training loss: 2.4336 0.7417 sec/batch\n",
      "Epoch 3/3  Iteration 511/534 Training loss: 2.4333 0.6707 sec/batch\n",
      "Epoch 3/3  Iteration 512/534 Training loss: 2.4330 0.7805 sec/batch\n",
      "Epoch 3/3  Iteration 513/534 Training loss: 2.4326 0.7273 sec/batch\n",
      "Epoch 3/3  Iteration 514/534 Training loss: 2.4323 0.6086 sec/batch\n",
      "Epoch 3/3  Iteration 515/534 Training loss: 2.4319 0.5402 sec/batch\n",
      "Epoch 3/3  Iteration 516/534 Training loss: 2.4318 0.5752 sec/batch\n",
      "Epoch 3/3  Iteration 517/534 Training loss: 2.4315 0.6977 sec/batch\n",
      "Epoch 3/3  Iteration 518/534 Training loss: 2.4311 0.6271 sec/batch\n",
      "Epoch 3/3  Iteration 519/534 Training loss: 2.4307 0.5974 sec/batch\n",
      "Epoch 3/3  Iteration 520/534 Training loss: 2.4304 0.5833 sec/batch\n",
      "Epoch 3/3  Iteration 521/534 Training loss: 2.4302 0.6004 sec/batch\n",
      "Epoch 3/3  Iteration 522/534 Training loss: 2.4300 0.5814 sec/batch\n",
      "Epoch 3/3  Iteration 523/534 Training loss: 2.4297 0.5915 sec/batch\n",
      "Epoch 3/3  Iteration 524/534 Training loss: 2.4295 0.5982 sec/batch\n",
      "Epoch 3/3  Iteration 525/534 Training loss: 2.4293 0.7674 sec/batch\n",
      "Epoch 3/3  Iteration 526/534 Training loss: 2.4289 0.7117 sec/batch\n",
      "Epoch 3/3  Iteration 527/534 Training loss: 2.4287 0.6756 sec/batch\n",
      "Epoch 3/3  Iteration 528/534 Training loss: 2.4285 0.7402 sec/batch\n",
      "Epoch 3/3  Iteration 529/534 Training loss: 2.4285 0.7060 sec/batch\n",
      "Epoch 3/3  Iteration 530/534 Training loss: 2.4284 0.6864 sec/batch\n",
      "Epoch 3/3  Iteration 531/534 Training loss: 2.4283 0.6898 sec/batch\n",
      "Epoch 3/3  Iteration 532/534 Training loss: 2.4281 0.6130 sec/batch\n",
      "Epoch 3/3  Iteration 533/534 Training loss: 2.4278 0.5704 sec/batch\n",
      "Epoch 3/3  Iteration 534/534 Training loss: 2.4274 0.5911 sec/batch\n",
      "Epoch 1/3  Iteration 1/534 Training loss: 4.4182 3.3857 sec/batch\n",
      "Epoch 1/3  Iteration 2/534 Training loss: 4.4049 0.9513 sec/batch\n",
      "Epoch 1/3  Iteration 3/534 Training loss: 4.3833 0.9532 sec/batch\n",
      "Epoch 1/3  Iteration 4/534 Training loss: 4.3350 0.9350 sec/batch\n",
      "Epoch 1/3  Iteration 5/534 Training loss: 4.2403 1.0234 sec/batch\n",
      "Epoch 1/3  Iteration 6/534 Training loss: 4.1441 0.9656 sec/batch\n",
      "Epoch 1/3  Iteration 7/534 Training loss: 4.0609 0.9619 sec/batch\n",
      "Epoch 1/3  Iteration 8/534 Training loss: 3.9906 0.9679 sec/batch\n",
      "Epoch 1/3  Iteration 9/534 Training loss: 3.9287 0.9695 sec/batch\n",
      "Epoch 1/3  Iteration 10/534 Training loss: 3.8757 1.1311 sec/batch\n",
      "Epoch 1/3  Iteration 11/534 Training loss: 3.8286 1.2931 sec/batch\n",
      "Epoch 1/3  Iteration 12/534 Training loss: 3.7881 1.0865 sec/batch\n",
      "Epoch 1/3  Iteration 13/534 Training loss: 3.7529 0.8810 sec/batch\n",
      "Epoch 1/3  Iteration 14/534 Training loss: 3.7220 0.9072 sec/batch\n",
      "Epoch 1/3  Iteration 15/534 Training loss: 3.6933 0.9061 sec/batch\n",
      "Epoch 1/3  Iteration 16/534 Training loss: 3.6681 0.9228 sec/batch\n",
      "Epoch 1/3  Iteration 17/534 Training loss: 3.6441 0.9552 sec/batch\n",
      "Epoch 1/3  Iteration 18/534 Training loss: 3.6248 0.9452 sec/batch\n",
      "Epoch 1/3  Iteration 19/534 Training loss: 3.6061 0.9089 sec/batch\n",
      "Epoch 1/3  Iteration 20/534 Training loss: 3.5865 0.9386 sec/batch\n",
      "Epoch 1/3  Iteration 21/534 Training loss: 3.5695 0.9999 sec/batch\n",
      "Epoch 1/3  Iteration 22/534 Training loss: 3.5538 1.2041 sec/batch\n",
      "Epoch 1/3  Iteration 23/534 Training loss: 3.5395 1.0428 sec/batch\n",
      "Epoch 1/3  Iteration 24/534 Training loss: 3.5261 0.9177 sec/batch\n",
      "Epoch 1/3  Iteration 25/534 Training loss: 3.5133 1.0766 sec/batch\n",
      "Epoch 1/3  Iteration 26/534 Training loss: 3.5020 1.0705 sec/batch\n",
      "Epoch 1/3  Iteration 27/534 Training loss: 3.4916 0.9711 sec/batch\n",
      "Epoch 1/3  Iteration 28/534 Training loss: 3.4806 0.9602 sec/batch\n",
      "Epoch 1/3  Iteration 29/534 Training loss: 3.4710 0.9699 sec/batch\n",
      "Epoch 1/3  Iteration 30/534 Training loss: 3.4619 0.9053 sec/batch\n",
      "Epoch 1/3  Iteration 31/534 Training loss: 3.4539 1.0419 sec/batch\n",
      "Epoch 1/3  Iteration 32/534 Training loss: 3.4455 1.1270 sec/batch\n",
      "Epoch 1/3  Iteration 33/534 Training loss: 3.4372 1.1549 sec/batch\n",
      "Epoch 1/3  Iteration 34/534 Training loss: 3.4299 0.9881 sec/batch\n",
      "Epoch 1/3  Iteration 35/534 Training loss: 3.4226 0.8894 sec/batch\n",
      "Epoch 1/3  Iteration 36/534 Training loss: 3.4161 0.8939 sec/batch\n",
      "Epoch 1/3  Iteration 37/534 Training loss: 3.4091 0.8792 sec/batch\n",
      "Epoch 1/3  Iteration 38/534 Training loss: 3.4025 0.9994 sec/batch\n",
      "Epoch 1/3  Iteration 39/534 Training loss: 3.3960 0.9384 sec/batch\n",
      "Epoch 1/3  Iteration 40/534 Training loss: 3.3902 0.8634 sec/batch\n",
      "Epoch 1/3  Iteration 41/534 Training loss: 3.3845 0.9184 sec/batch\n",
      "Epoch 1/3  Iteration 42/534 Training loss: 3.3791 0.9195 sec/batch\n",
      "Epoch 1/3  Iteration 43/534 Training loss: 3.3738 1.2527 sec/batch\n",
      "Epoch 1/3  Iteration 44/534 Training loss: 3.3688 1.1168 sec/batch\n",
      "Epoch 1/3  Iteration 45/534 Training loss: 3.3637 0.9340 sec/batch\n",
      "Epoch 1/3  Iteration 46/534 Training loss: 3.3593 0.8902 sec/batch\n",
      "Epoch 1/3  Iteration 47/534 Training loss: 3.3551 0.8649 sec/batch\n",
      "Epoch 1/3  Iteration 48/534 Training loss: 3.3512 0.9128 sec/batch\n",
      "Epoch 1/3  Iteration 49/534 Training loss: 3.3474 0.9101 sec/batch\n",
      "Epoch 1/3  Iteration 50/534 Training loss: 3.3436 0.8629 sec/batch\n",
      "Epoch 1/3  Iteration 51/534 Training loss: 3.3399 0.9075 sec/batch\n",
      "Epoch 1/3  Iteration 52/534 Training loss: 3.3362 0.9658 sec/batch\n",
      "Epoch 1/3  Iteration 53/534 Training loss: 3.3329 0.8980 sec/batch\n",
      "Epoch 1/3  Iteration 54/534 Training loss: 3.3292 1.0687 sec/batch\n",
      "Epoch 1/3  Iteration 55/534 Training loss: 3.3259 1.1922 sec/batch\n",
      "Epoch 1/3  Iteration 56/534 Training loss: 3.3224 0.9622 sec/batch\n",
      "Epoch 1/3  Iteration 57/534 Training loss: 3.3192 0.8761 sec/batch\n",
      "Epoch 1/3  Iteration 58/534 Training loss: 3.3161 0.9938 sec/batch\n",
      "Epoch 1/3  Iteration 59/534 Training loss: 3.3130 0.9146 sec/batch\n",
      "Epoch 1/3  Iteration 60/534 Training loss: 3.3102 0.8840 sec/batch\n",
      "Epoch 1/3  Iteration 61/534 Training loss: 3.3074 0.8909 sec/batch\n",
      "Epoch 1/3  Iteration 62/534 Training loss: 3.3050 0.9174 sec/batch\n",
      "Epoch 1/3  Iteration 63/534 Training loss: 3.3028 0.8560 sec/batch\n",
      "Epoch 1/3  Iteration 64/534 Training loss: 3.2999 1.1042 sec/batch\n",
      "Epoch 1/3  Iteration 65/534 Training loss: 3.2972 1.1299 sec/batch\n",
      "Epoch 1/3  Iteration 66/534 Training loss: 3.2950 1.2490 sec/batch\n",
      "Epoch 1/3  Iteration 67/534 Training loss: 3.2927 0.8787 sec/batch\n",
      "Epoch 1/3  Iteration 68/534 Training loss: 3.2898 0.8874 sec/batch\n",
      "Epoch 1/3  Iteration 69/534 Training loss: 3.2874 0.9140 sec/batch\n",
      "Epoch 1/3  Iteration 70/534 Training loss: 3.2854 0.9188 sec/batch\n",
      "Epoch 1/3  Iteration 71/534 Training loss: 3.2833 0.8834 sec/batch\n",
      "Epoch 1/3  Iteration 72/534 Training loss: 3.2815 0.8548 sec/batch\n",
      "Epoch 1/3  Iteration 73/534 Training loss: 3.2794 0.8958 sec/batch\n",
      "Epoch 1/3  Iteration 74/534 Training loss: 3.2774 0.9059 sec/batch\n",
      "Epoch 1/3  Iteration 75/534 Training loss: 3.2756 0.8468 sec/batch\n",
      "Epoch 1/3  Iteration 76/534 Training loss: 3.2738 0.8726 sec/batch\n",
      "Epoch 1/3  Iteration 77/534 Training loss: 3.2721 0.9278 sec/batch\n",
      "Epoch 1/3  Iteration 78/534 Training loss: 3.2703 0.9870 sec/batch\n",
      "Epoch 1/3  Iteration 79/534 Training loss: 3.2685 1.0665 sec/batch\n",
      "Epoch 1/3  Iteration 80/534 Training loss: 3.2666 1.0767 sec/batch\n",
      "Epoch 1/3  Iteration 81/534 Training loss: 3.2648 0.8802 sec/batch\n",
      "Epoch 1/3  Iteration 82/534 Training loss: 3.2632 0.8638 sec/batch\n",
      "Epoch 1/3  Iteration 83/534 Training loss: 3.2616 0.8773 sec/batch\n",
      "Epoch 1/3  Iteration 84/534 Training loss: 3.2599 0.9036 sec/batch\n",
      "Epoch 1/3  Iteration 85/534 Training loss: 3.2581 0.8675 sec/batch\n",
      "Epoch 1/3  Iteration 86/534 Training loss: 3.2564 0.9809 sec/batch\n",
      "Epoch 1/3  Iteration 87/534 Training loss: 3.2547 0.9325 sec/batch\n",
      "Epoch 1/3  Iteration 88/534 Training loss: 3.2531 0.8486 sec/batch\n",
      "Epoch 1/3  Iteration 89/534 Training loss: 3.2516 0.9449 sec/batch\n",
      "Epoch 1/3  Iteration 90/534 Training loss: 3.2502 1.3064 sec/batch\n",
      "Epoch 1/3  Iteration 91/534 Training loss: 3.2488 1.0758 sec/batch\n",
      "Epoch 1/3  Iteration 92/534 Training loss: 3.2473 0.9013 sec/batch\n",
      "Epoch 1/3  Iteration 93/534 Training loss: 3.2457 0.8859 sec/batch\n",
      "Epoch 1/3  Iteration 94/534 Training loss: 3.2444 0.8589 sec/batch\n",
      "Epoch 1/3  Iteration 95/534 Training loss: 3.2428 0.8653 sec/batch\n",
      "Epoch 1/3  Iteration 96/534 Training loss: 3.2413 0.8612 sec/batch\n",
      "Epoch 1/3  Iteration 97/534 Training loss: 3.2400 0.9184 sec/batch\n",
      "Epoch 1/3  Iteration 98/534 Training loss: 3.2385 0.7984 sec/batch\n",
      "Epoch 1/3  Iteration 99/534 Training loss: 3.2372 0.8469 sec/batch\n",
      "Epoch 1/3  Iteration 100/534 Training loss: 3.2357 0.9284 sec/batch\n",
      "Epoch 1/3  Iteration 101/534 Training loss: 3.2344 1.0314 sec/batch\n",
      "Epoch 1/3  Iteration 102/534 Training loss: 3.2330 1.1029 sec/batch\n",
      "Epoch 1/3  Iteration 103/534 Training loss: 3.2317 1.0029 sec/batch\n",
      "Epoch 1/3  Iteration 104/534 Training loss: 3.2302 0.7935 sec/batch\n",
      "Epoch 1/3  Iteration 105/534 Training loss: 3.2288 0.7780 sec/batch\n",
      "Epoch 1/3  Iteration 106/534 Training loss: 3.2274 0.7971 sec/batch\n",
      "Epoch 1/3  Iteration 107/534 Training loss: 3.2258 1.0691 sec/batch\n",
      "Epoch 1/3  Iteration 108/534 Training loss: 3.2242 0.7669 sec/batch\n",
      "Epoch 1/3  Iteration 109/534 Training loss: 3.2228 0.7688 sec/batch\n",
      "Epoch 1/3  Iteration 110/534 Training loss: 3.2210 0.7805 sec/batch\n",
      "Epoch 1/3  Iteration 111/534 Training loss: 3.2196 0.8302 sec/batch\n",
      "Epoch 1/3  Iteration 112/534 Training loss: 3.2181 0.7763 sec/batch\n",
      "Epoch 1/3  Iteration 113/534 Training loss: 3.2165 0.7828 sec/batch\n",
      "Epoch 1/3  Iteration 114/534 Training loss: 3.2148 0.8941 sec/batch\n",
      "Epoch 1/3  Iteration 115/534 Training loss: 3.2131 1.0507 sec/batch\n",
      "Epoch 1/3  Iteration 116/534 Training loss: 3.2115 0.9044 sec/batch\n",
      "Epoch 1/3  Iteration 117/534 Training loss: 3.2099 0.8046 sec/batch\n",
      "Epoch 1/3  Iteration 118/534 Training loss: 3.2084 0.7856 sec/batch\n",
      "Epoch 1/3  Iteration 119/534 Training loss: 3.2069 0.7895 sec/batch\n",
      "Epoch 1/3  Iteration 120/534 Training loss: 3.2052 0.9356 sec/batch\n",
      "Epoch 1/3  Iteration 121/534 Training loss: 3.2038 0.9807 sec/batch\n",
      "Epoch 1/3  Iteration 122/534 Training loss: 3.2022 1.0012 sec/batch\n",
      "Epoch 1/3  Iteration 123/534 Training loss: 3.2006 0.8893 sec/batch\n",
      "Epoch 1/3  Iteration 124/534 Training loss: 3.1991 0.9477 sec/batch\n",
      "Epoch 1/3  Iteration 125/534 Training loss: 3.1973 1.1193 sec/batch\n",
      "Epoch 1/3  Iteration 126/534 Training loss: 3.1954 1.1316 sec/batch\n",
      "Epoch 1/3  Iteration 127/534 Training loss: 3.1937 0.9913 sec/batch\n",
      "Epoch 1/3  Iteration 128/534 Training loss: 3.1919 0.9154 sec/batch\n",
      "Epoch 1/3  Iteration 129/534 Training loss: 3.1901 0.8658 sec/batch\n",
      "Epoch 1/3  Iteration 130/534 Training loss: 3.1883 0.8643 sec/batch\n",
      "Epoch 1/3  Iteration 131/534 Training loss: 3.1865 0.9114 sec/batch\n",
      "Epoch 1/3  Iteration 132/534 Training loss: 3.1846 0.8623 sec/batch\n",
      "Epoch 1/3  Iteration 133/534 Training loss: 3.1827 0.8945 sec/batch\n",
      "Epoch 1/3  Iteration 134/534 Training loss: 3.1807 0.8823 sec/batch\n",
      "Epoch 1/3  Iteration 135/534 Training loss: 3.1784 0.8975 sec/batch\n",
      "Epoch 1/3  Iteration 136/534 Training loss: 3.1763 0.9878 sec/batch\n",
      "Epoch 1/3  Iteration 137/534 Training loss: 3.1742 1.0987 sec/batch\n",
      "Epoch 1/3  Iteration 138/534 Training loss: 3.1721 1.0716 sec/batch\n",
      "Epoch 1/3  Iteration 139/534 Training loss: 3.1701 0.8916 sec/batch\n",
      "Epoch 1/3  Iteration 140/534 Training loss: 3.1680 0.8749 sec/batch\n",
      "Epoch 1/3  Iteration 141/534 Training loss: 3.1660 0.8844 sec/batch\n",
      "Epoch 1/3  Iteration 142/534 Training loss: 3.1636 0.9085 sec/batch\n",
      "Epoch 1/3  Iteration 143/534 Training loss: 3.1614 0.8428 sec/batch\n",
      "Epoch 1/3  Iteration 144/534 Training loss: 3.1591 0.9091 sec/batch\n",
      "Epoch 1/3  Iteration 145/534 Training loss: 3.1569 0.9036 sec/batch\n",
      "Epoch 1/3  Iteration 146/534 Training loss: 3.1548 0.8805 sec/batch\n",
      "Epoch 1/3  Iteration 147/534 Training loss: 3.1526 0.8774 sec/batch\n",
      "Epoch 1/3  Iteration 148/534 Training loss: 3.1506 1.1077 sec/batch\n",
      "Epoch 1/3  Iteration 149/534 Training loss: 3.1483 1.1008 sec/batch\n",
      "Epoch 1/3  Iteration 150/534 Training loss: 3.1460 0.9266 sec/batch\n",
      "Epoch 1/3  Iteration 151/534 Training loss: 3.1441 0.8915 sec/batch\n",
      "Epoch 1/3  Iteration 152/534 Training loss: 3.1421 0.8810 sec/batch\n",
      "Epoch 1/3  Iteration 153/534 Training loss: 3.1398 1.0014 sec/batch\n",
      "Epoch 1/3  Iteration 154/534 Training loss: 3.1375 0.9792 sec/batch\n",
      "Epoch 1/3  Iteration 155/534 Training loss: 3.1352 0.8877 sec/batch\n",
      "Epoch 1/3  Iteration 156/534 Training loss: 3.1329 0.9886 sec/batch\n",
      "Epoch 1/3  Iteration 157/534 Training loss: 3.1304 0.9221 sec/batch\n",
      "Epoch 1/3  Iteration 158/534 Training loss: 3.1281 1.2194 sec/batch\n",
      "Epoch 1/3  Iteration 159/534 Training loss: 3.1256 1.8180 sec/batch\n",
      "Epoch 1/3  Iteration 160/534 Training loss: 3.1233 2.1620 sec/batch\n",
      "Epoch 1/3  Iteration 161/534 Training loss: 3.1209 1.3951 sec/batch\n",
      "Epoch 1/3  Iteration 162/534 Training loss: 3.1184 1.0663 sec/batch\n",
      "Epoch 1/3  Iteration 163/534 Training loss: 3.1159 1.2414 sec/batch\n",
      "Epoch 1/3  Iteration 164/534 Training loss: 3.1136 1.0104 sec/batch\n",
      "Epoch 1/3  Iteration 165/534 Training loss: 3.1112 1.0204 sec/batch\n",
      "Epoch 1/3  Iteration 166/534 Training loss: 3.1088 1.1027 sec/batch\n",
      "Epoch 1/3  Iteration 167/534 Training loss: 3.1065 1.3160 sec/batch\n",
      "Epoch 1/3  Iteration 168/534 Training loss: 3.1042 1.3856 sec/batch\n",
      "Epoch 1/3  Iteration 169/534 Training loss: 3.1019 1.0829 sec/batch\n",
      "Epoch 1/3  Iteration 170/534 Training loss: 3.0995 1.0551 sec/batch\n",
      "Epoch 1/3  Iteration 171/534 Training loss: 3.0972 0.9740 sec/batch\n",
      "Epoch 1/3  Iteration 172/534 Training loss: 3.0951 1.0426 sec/batch\n",
      "Epoch 1/3  Iteration 173/534 Training loss: 3.0932 1.0715 sec/batch\n",
      "Epoch 1/3  Iteration 174/534 Training loss: 3.0912 1.0627 sec/batch\n",
      "Epoch 1/3  Iteration 175/534 Training loss: 3.0890 1.2268 sec/batch\n",
      "Epoch 1/3  Iteration 176/534 Training loss: 3.0868 1.3851 sec/batch\n",
      "Epoch 1/3  Iteration 177/534 Training loss: 3.0845 1.0727 sec/batch\n",
      "Epoch 1/3  Iteration 178/534 Training loss: 3.0821 0.9982 sec/batch\n",
      "Epoch 2/3  Iteration 179/534 Training loss: 2.7132 1.0149 sec/batch\n",
      "Epoch 2/3  Iteration 180/534 Training loss: 2.6728 1.0022 sec/batch\n",
      "Epoch 2/3  Iteration 181/534 Training loss: 2.6683 1.1592 sec/batch\n",
      "Epoch 2/3  Iteration 182/534 Training loss: 2.6653 1.0924 sec/batch\n",
      "Epoch 2/3  Iteration 183/534 Training loss: 2.6631 0.9905 sec/batch\n",
      "Epoch 2/3  Iteration 184/534 Training loss: 2.6609 1.0235 sec/batch\n",
      "Epoch 2/3  Iteration 185/534 Training loss: 2.6608 1.1356 sec/batch\n",
      "Epoch 2/3  Iteration 186/534 Training loss: 2.6611 1.2357 sec/batch\n",
      "Epoch 2/3  Iteration 187/534 Training loss: 2.6607 1.1315 sec/batch\n",
      "Epoch 2/3  Iteration 188/534 Training loss: 2.6591 1.0011 sec/batch\n",
      "Epoch 2/3  Iteration 189/534 Training loss: 2.6563 0.8791 sec/batch\n",
      "Epoch 2/3  Iteration 190/534 Training loss: 2.6558 0.8361 sec/batch\n",
      "Epoch 2/3  Iteration 191/534 Training loss: 2.6542 0.8832 sec/batch\n",
      "Epoch 2/3  Iteration 192/534 Training loss: 2.6544 0.8615 sec/batch\n",
      "Epoch 2/3  Iteration 193/534 Training loss: 2.6529 0.8373 sec/batch\n",
      "Epoch 2/3  Iteration 194/534 Training loss: 2.6519 0.8397 sec/batch\n",
      "Epoch 2/3  Iteration 195/534 Training loss: 2.6501 0.8157 sec/batch\n",
      "Epoch 2/3  Iteration 196/534 Training loss: 2.6511 0.8794 sec/batch\n",
      "Epoch 2/3  Iteration 197/534 Training loss: 2.6500 1.0191 sec/batch\n",
      "Epoch 2/3  Iteration 198/534 Training loss: 2.6477 1.0093 sec/batch\n",
      "Epoch 2/3  Iteration 199/534 Training loss: 2.6463 0.8559 sec/batch\n",
      "Epoch 2/3  Iteration 200/534 Training loss: 2.6466 0.8257 sec/batch\n",
      "Epoch 2/3  Iteration 201/534 Training loss: 2.6451 0.7861 sec/batch\n",
      "Epoch 2/3  Iteration 202/534 Training loss: 2.6435 0.7918 sec/batch\n",
      "Epoch 2/3  Iteration 203/534 Training loss: 2.6415 0.8793 sec/batch\n",
      "Epoch 2/3  Iteration 204/534 Training loss: 2.6405 0.8915 sec/batch\n",
      "Epoch 2/3  Iteration 205/534 Training loss: 2.6390 0.8758 sec/batch\n",
      "Epoch 2/3  Iteration 206/534 Training loss: 2.6377 0.8220 sec/batch\n",
      "Epoch 2/3  Iteration 207/534 Training loss: 2.6365 0.8640 sec/batch\n",
      "Epoch 2/3  Iteration 208/534 Training loss: 2.6356 0.8236 sec/batch\n",
      "Epoch 2/3  Iteration 209/534 Training loss: 2.6348 1.0758 sec/batch\n",
      "Epoch 2/3  Iteration 210/534 Training loss: 2.6331 1.1082 sec/batch\n",
      "Epoch 2/3  Iteration 211/534 Training loss: 2.6314 0.8945 sec/batch\n",
      "Epoch 2/3  Iteration 212/534 Training loss: 2.6305 0.8416 sec/batch\n",
      "Epoch 2/3  Iteration 213/534 Training loss: 2.6288 0.8369 sec/batch\n",
      "Epoch 2/3  Iteration 214/534 Training loss: 2.6278 0.8107 sec/batch\n",
      "Epoch 2/3  Iteration 215/534 Training loss: 2.6261 0.9755 sec/batch\n",
      "Epoch 2/3  Iteration 216/534 Training loss: 2.6242 0.8057 sec/batch\n",
      "Epoch 2/3  Iteration 217/534 Training loss: 2.6224 0.8187 sec/batch\n",
      "Epoch 2/3  Iteration 218/534 Training loss: 2.6207 0.8344 sec/batch\n",
      "Epoch 2/3  Iteration 219/534 Training loss: 2.6188 0.8277 sec/batch\n",
      "Epoch 2/3  Iteration 220/534 Training loss: 2.6170 0.9349 sec/batch\n",
      "Epoch 2/3  Iteration 221/534 Training loss: 2.6152 1.1057 sec/batch\n",
      "Epoch 2/3  Iteration 222/534 Training loss: 2.6135 1.2710 sec/batch\n",
      "Epoch 2/3  Iteration 223/534 Training loss: 2.6118 1.1250 sec/batch\n",
      "Epoch 2/3  Iteration 224/534 Training loss: 2.6096 1.3985 sec/batch\n",
      "Epoch 2/3  Iteration 225/534 Training loss: 2.6086 0.9689 sec/batch\n",
      "Epoch 2/3  Iteration 226/534 Training loss: 2.6072 0.6737 sec/batch\n",
      "Epoch 2/3  Iteration 227/534 Training loss: 2.6058 0.8822 sec/batch\n",
      "Epoch 2/3  Iteration 228/534 Training loss: 2.6048 1.1106 sec/batch\n",
      "Epoch 2/3  Iteration 229/534 Training loss: 2.6032 0.8390 sec/batch\n",
      "Epoch 2/3  Iteration 230/534 Training loss: 2.6018 0.8794 sec/batch\n",
      "Epoch 2/3  Iteration 231/534 Training loss: 2.6005 1.1263 sec/batch\n",
      "Epoch 2/3  Iteration 232/534 Training loss: 2.5990 1.0649 sec/batch\n",
      "Epoch 2/3  Iteration 233/534 Training loss: 2.5976 0.8428 sec/batch\n",
      "Epoch 2/3  Iteration 234/534 Training loss: 2.5963 0.8391 sec/batch\n",
      "Epoch 2/3  Iteration 235/534 Training loss: 2.5949 0.8167 sec/batch\n",
      "Epoch 2/3  Iteration 236/534 Training loss: 2.5935 0.7998 sec/batch\n",
      "Epoch 2/3  Iteration 237/534 Training loss: 2.5921 0.7841 sec/batch\n",
      "Epoch 2/3  Iteration 238/534 Training loss: 2.5911 0.7873 sec/batch\n",
      "Epoch 2/3  Iteration 239/534 Training loss: 2.5899 0.8649 sec/batch\n",
      "Epoch 2/3  Iteration 240/534 Training loss: 2.5890 1.0191 sec/batch\n",
      "Epoch 2/3  Iteration 241/534 Training loss: 2.5880 0.7630 sec/batch\n",
      "Epoch 2/3  Iteration 242/534 Training loss: 2.5867 0.8244 sec/batch\n",
      "Epoch 2/3  Iteration 243/534 Training loss: 2.5852 1.1499 sec/batch\n",
      "Epoch 2/3  Iteration 244/534 Training loss: 2.5844 1.0323 sec/batch\n",
      "Epoch 2/3  Iteration 245/534 Training loss: 2.5833 1.2199 sec/batch\n",
      "Epoch 2/3  Iteration 246/534 Training loss: 2.5815 0.8922 sec/batch\n",
      "Epoch 2/3  Iteration 247/534 Training loss: 2.5801 0.9799 sec/batch\n",
      "Epoch 2/3  Iteration 248/534 Training loss: 2.5792 0.9012 sec/batch\n",
      "Epoch 2/3  Iteration 249/534 Training loss: 2.5780 0.9351 sec/batch\n",
      "Epoch 2/3  Iteration 250/534 Training loss: 2.5770 0.8922 sec/batch\n",
      "Epoch 2/3  Iteration 251/534 Training loss: 2.5760 0.8877 sec/batch\n",
      "Epoch 2/3  Iteration 252/534 Training loss: 2.5747 0.9354 sec/batch\n",
      "Epoch 2/3  Iteration 253/534 Training loss: 2.5736 0.8692 sec/batch\n",
      "Epoch 2/3  Iteration 254/534 Training loss: 2.5729 1.1644 sec/batch\n",
      "Epoch 2/3  Iteration 255/534 Training loss: 2.5718 1.0370 sec/batch\n",
      "Epoch 2/3  Iteration 256/534 Training loss: 2.5709 0.8240 sec/batch\n",
      "Epoch 2/3  Iteration 257/534 Training loss: 2.5696 0.7891 sec/batch\n",
      "Epoch 2/3  Iteration 258/534 Training loss: 2.5685 0.7817 sec/batch\n",
      "Epoch 2/3  Iteration 259/534 Training loss: 2.5674 0.8037 sec/batch\n",
      "Epoch 2/3  Iteration 260/534 Training loss: 2.5664 0.7882 sec/batch\n",
      "Epoch 2/3  Iteration 261/534 Training loss: 2.5653 0.7778 sec/batch\n",
      "Epoch 2/3  Iteration 262/534 Training loss: 2.5641 0.7706 sec/batch\n",
      "Epoch 2/3  Iteration 263/534 Training loss: 2.5627 0.8266 sec/batch\n",
      "Epoch 2/3  Iteration 264/534 Training loss: 2.5614 0.7865 sec/batch\n",
      "Epoch 2/3  Iteration 265/534 Training loss: 2.5603 0.7652 sec/batch\n",
      "Epoch 2/3  Iteration 266/534 Training loss: 2.5592 0.9310 sec/batch\n",
      "Epoch 2/3  Iteration 267/534 Training loss: 2.5581 1.0718 sec/batch\n",
      "Epoch 2/3  Iteration 268/534 Training loss: 2.5572 0.9530 sec/batch\n",
      "Epoch 2/3  Iteration 269/534 Training loss: 2.5561 0.7838 sec/batch\n",
      "Epoch 2/3  Iteration 270/534 Training loss: 2.5552 0.9361 sec/batch\n",
      "Epoch 2/3  Iteration 271/534 Training loss: 2.5541 1.0443 sec/batch\n",
      "Epoch 2/3  Iteration 272/534 Training loss: 2.5529 0.9487 sec/batch\n",
      "Epoch 2/3  Iteration 273/534 Training loss: 2.5516 0.9201 sec/batch\n",
      "Epoch 2/3  Iteration 274/534 Training loss: 2.5505 1.0484 sec/batch\n",
      "Epoch 2/3  Iteration 275/534 Training loss: 2.5496 0.9590 sec/batch\n",
      "Epoch 2/3  Iteration 276/534 Training loss: 2.5486 1.0809 sec/batch\n",
      "Epoch 2/3  Iteration 277/534 Training loss: 2.5476 1.2011 sec/batch\n",
      "Epoch 2/3  Iteration 278/534 Training loss: 2.5465 1.4169 sec/batch\n",
      "Epoch 2/3  Iteration 279/534 Training loss: 2.5457 1.3141 sec/batch\n",
      "Epoch 2/3  Iteration 280/534 Training loss: 2.5447 1.2224 sec/batch\n",
      "Epoch 2/3  Iteration 281/534 Training loss: 2.5436 1.0576 sec/batch\n",
      "Epoch 2/3  Iteration 282/534 Training loss: 2.5426 1.2045 sec/batch\n",
      "Epoch 2/3  Iteration 283/534 Training loss: 2.5415 1.0622 sec/batch\n",
      "Epoch 2/3  Iteration 284/534 Training loss: 2.5406 1.0261 sec/batch\n",
      "Epoch 2/3  Iteration 285/534 Training loss: 2.5396 1.0060 sec/batch\n",
      "Epoch 2/3  Iteration 286/534 Training loss: 2.5386 1.1539 sec/batch\n",
      "Epoch 2/3  Iteration 287/534 Training loss: 2.5377 1.2700 sec/batch\n",
      "Epoch 2/3  Iteration 288/534 Training loss: 2.5365 1.1384 sec/batch\n",
      "Epoch 2/3  Iteration 289/534 Training loss: 2.5357 1.1574 sec/batch\n",
      "Epoch 2/3  Iteration 290/534 Training loss: 2.5349 1.0990 sec/batch\n",
      "Epoch 2/3  Iteration 291/534 Training loss: 2.5339 1.0339 sec/batch\n",
      "Epoch 2/3  Iteration 292/534 Training loss: 2.5329 1.0561 sec/batch\n",
      "Epoch 2/3  Iteration 293/534 Training loss: 2.5320 0.9912 sec/batch\n",
      "Epoch 2/3  Iteration 294/534 Training loss: 2.5309 0.9847 sec/batch\n",
      "Epoch 2/3  Iteration 295/534 Training loss: 2.5300 1.1840 sec/batch\n",
      "Epoch 2/3  Iteration 296/534 Training loss: 2.5291 1.2700 sec/batch\n",
      "Epoch 2/3  Iteration 297/534 Training loss: 2.5284 1.2597 sec/batch\n",
      "Epoch 2/3  Iteration 298/534 Training loss: 2.5276 1.0462 sec/batch\n",
      "Epoch 2/3  Iteration 299/534 Training loss: 2.5269 0.9928 sec/batch\n",
      "Epoch 2/3  Iteration 300/534 Training loss: 2.5261 0.9821 sec/batch\n",
      "Epoch 2/3  Iteration 301/534 Training loss: 2.5253 1.0498 sec/batch\n",
      "Epoch 2/3  Iteration 302/534 Training loss: 2.5246 0.9699 sec/batch\n",
      "Epoch 2/3  Iteration 303/534 Training loss: 2.5237 0.9832 sec/batch\n",
      "Epoch 2/3  Iteration 304/534 Training loss: 2.5228 1.0063 sec/batch\n",
      "Epoch 2/3  Iteration 305/534 Training loss: 2.5219 0.9819 sec/batch\n",
      "Epoch 2/3  Iteration 306/534 Training loss: 2.5213 0.9628 sec/batch\n",
      "Epoch 2/3  Iteration 307/534 Training loss: 2.5204 1.3627 sec/batch\n",
      "Epoch 2/3  Iteration 308/534 Training loss: 2.5197 1.2795 sec/batch\n",
      "Epoch 2/3  Iteration 309/534 Training loss: 2.5189 1.3062 sec/batch\n",
      "Epoch 2/3  Iteration 310/534 Training loss: 2.5180 1.0218 sec/batch\n",
      "Epoch 2/3  Iteration 311/534 Training loss: 2.5172 1.1520 sec/batch\n",
      "Epoch 2/3  Iteration 312/534 Training loss: 2.5166 1.0336 sec/batch\n",
      "Epoch 2/3  Iteration 313/534 Training loss: 2.5156 1.2327 sec/batch\n",
      "Epoch 2/3  Iteration 314/534 Training loss: 2.5149 1.0088 sec/batch\n",
      "Epoch 2/3  Iteration 315/534 Training loss: 2.5140 0.9159 sec/batch\n",
      "Epoch 2/3  Iteration 316/534 Training loss: 2.5133 0.9822 sec/batch\n",
      "Epoch 2/3  Iteration 317/534 Training loss: 2.5126 1.1462 sec/batch\n",
      "Epoch 2/3  Iteration 318/534 Training loss: 2.5119 1.2257 sec/batch\n",
      "Epoch 2/3  Iteration 319/534 Training loss: 2.5113 1.1942 sec/batch\n",
      "Epoch 2/3  Iteration 320/534 Training loss: 2.5105 1.0957 sec/batch\n",
      "Epoch 2/3  Iteration 321/534 Training loss: 2.5097 1.0700 sec/batch\n",
      "Epoch 2/3  Iteration 322/534 Training loss: 2.5090 0.8712 sec/batch\n",
      "Epoch 2/3  Iteration 323/534 Training loss: 2.5082 0.8627 sec/batch\n",
      "Epoch 2/3  Iteration 324/534 Training loss: 2.5077 0.9463 sec/batch\n",
      "Epoch 2/3  Iteration 325/534 Training loss: 2.5070 1.1326 sec/batch\n",
      "Epoch 2/3  Iteration 326/534 Training loss: 2.5064 1.1823 sec/batch\n",
      "Epoch 2/3  Iteration 327/534 Training loss: 2.5057 1.0367 sec/batch\n",
      "Epoch 2/3  Iteration 328/534 Training loss: 2.5048 0.9899 sec/batch\n",
      "Epoch 2/3  Iteration 329/534 Training loss: 2.5043 0.8840 sec/batch\n",
      "Epoch 2/3  Iteration 330/534 Training loss: 2.5038 1.2084 sec/batch\n",
      "Epoch 2/3  Iteration 331/534 Training loss: 2.5032 1.1794 sec/batch\n",
      "Epoch 2/3  Iteration 332/534 Training loss: 2.5026 1.0387 sec/batch\n",
      "Epoch 2/3  Iteration 333/534 Training loss: 2.5018 1.0520 sec/batch\n",
      "Epoch 2/3  Iteration 334/534 Training loss: 2.5012 0.9362 sec/batch\n",
      "Epoch 2/3  Iteration 335/534 Training loss: 2.5005 1.0306 sec/batch\n",
      "Epoch 2/3  Iteration 336/534 Training loss: 2.4997 1.0090 sec/batch\n",
      "Epoch 2/3  Iteration 337/534 Training loss: 2.4989 0.9301 sec/batch\n",
      "Epoch 2/3  Iteration 338/534 Training loss: 2.4983 0.9357 sec/batch\n",
      "Epoch 2/3  Iteration 339/534 Training loss: 2.4977 0.9878 sec/batch\n",
      "Epoch 2/3  Iteration 340/534 Training loss: 2.4968 1.1050 sec/batch\n",
      "Epoch 2/3  Iteration 341/534 Training loss: 2.4960 1.2110 sec/batch\n",
      "Epoch 2/3  Iteration 342/534 Training loss: 2.4953 1.1497 sec/batch\n",
      "Epoch 2/3  Iteration 343/534 Training loss: 2.4947 1.0319 sec/batch\n",
      "Epoch 2/3  Iteration 344/534 Training loss: 2.4939 1.1073 sec/batch\n",
      "Epoch 2/3  Iteration 345/534 Training loss: 2.4933 1.2840 sec/batch\n",
      "Epoch 2/3  Iteration 346/534 Training loss: 2.4927 1.1514 sec/batch\n",
      "Epoch 2/3  Iteration 347/534 Training loss: 2.4920 1.1018 sec/batch\n",
      "Epoch 2/3  Iteration 348/534 Training loss: 2.4912 0.8752 sec/batch\n",
      "Epoch 2/3  Iteration 349/534 Training loss: 2.4906 1.0554 sec/batch\n",
      "Epoch 2/3  Iteration 350/534 Training loss: 2.4901 1.1072 sec/batch\n",
      "Epoch 2/3  Iteration 351/534 Training loss: 2.4897 1.1744 sec/batch\n",
      "Epoch 2/3  Iteration 352/534 Training loss: 2.4893 1.3545 sec/batch\n",
      "Epoch 2/3  Iteration 353/534 Training loss: 2.4889 0.9449 sec/batch\n",
      "Epoch 2/3  Iteration 354/534 Training loss: 2.4883 0.9815 sec/batch\n",
      "Epoch 2/3  Iteration 355/534 Training loss: 2.4876 0.9737 sec/batch\n",
      "Epoch 2/3  Iteration 356/534 Training loss: 2.4868 0.9608 sec/batch\n",
      "Epoch 3/3  Iteration 357/534 Training loss: 2.4178 1.0975 sec/batch\n",
      "Epoch 3/3  Iteration 358/534 Training loss: 2.3745 1.1454 sec/batch\n",
      "Epoch 3/3  Iteration 359/534 Training loss: 2.3634 1.5478 sec/batch\n",
      "Epoch 3/3  Iteration 360/534 Training loss: 2.3604 1.2031 sec/batch\n",
      "Epoch 3/3  Iteration 361/534 Training loss: 2.3595 1.1925 sec/batch\n",
      "Epoch 3/3  Iteration 362/534 Training loss: 2.3590 1.0873 sec/batch\n",
      "Epoch 3/3  Iteration 363/534 Training loss: 2.3592 1.1207 sec/batch\n",
      "Epoch 3/3  Iteration 364/534 Training loss: 2.3609 1.0293 sec/batch\n",
      "Epoch 3/3  Iteration 365/534 Training loss: 2.3622 0.9900 sec/batch\n",
      "Epoch 3/3  Iteration 366/534 Training loss: 2.3624 1.0446 sec/batch\n",
      "Epoch 3/3  Iteration 367/534 Training loss: 2.3610 1.0066 sec/batch\n",
      "Epoch 3/3  Iteration 368/534 Training loss: 2.3606 1.4416 sec/batch\n",
      "Epoch 3/3  Iteration 369/534 Training loss: 2.3607 1.3659 sec/batch\n",
      "Epoch 3/3  Iteration 370/534 Training loss: 2.3624 1.0776 sec/batch\n",
      "Epoch 3/3  Iteration 371/534 Training loss: 2.3623 1.0231 sec/batch\n",
      "Epoch 3/3  Iteration 372/534 Training loss: 2.3616 0.9765 sec/batch\n",
      "Epoch 3/3  Iteration 373/534 Training loss: 2.3606 1.0041 sec/batch\n",
      "Epoch 3/3  Iteration 374/534 Training loss: 2.3620 0.9990 sec/batch\n",
      "Epoch 3/3  Iteration 375/534 Training loss: 2.3621 1.0026 sec/batch\n",
      "Epoch 3/3  Iteration 376/534 Training loss: 2.3605 0.9855 sec/batch\n",
      "Epoch 3/3  Iteration 377/534 Training loss: 2.3592 1.1682 sec/batch\n",
      "Epoch 3/3  Iteration 378/534 Training loss: 2.3601 1.1885 sec/batch\n",
      "Epoch 3/3  Iteration 379/534 Training loss: 2.3597 1.3359 sec/batch\n",
      "Epoch 3/3  Iteration 380/534 Training loss: 2.3585 1.0633 sec/batch\n",
      "Epoch 3/3  Iteration 381/534 Training loss: 2.3575 0.9911 sec/batch\n",
      "Epoch 3/3  Iteration 382/534 Training loss: 2.3574 0.9772 sec/batch\n",
      "Epoch 3/3  Iteration 383/534 Training loss: 2.3565 0.9801 sec/batch\n",
      "Epoch 3/3  Iteration 384/534 Training loss: 2.3561 1.0061 sec/batch\n",
      "Epoch 3/3  Iteration 385/534 Training loss: 2.3563 1.0199 sec/batch\n",
      "Epoch 3/3  Iteration 386/534 Training loss: 2.3559 0.9690 sec/batch\n",
      "Epoch 3/3  Iteration 387/534 Training loss: 2.3558 1.0535 sec/batch\n",
      "Epoch 3/3  Iteration 388/534 Training loss: 2.3550 1.3769 sec/batch\n",
      "Epoch 3/3  Iteration 389/534 Training loss: 2.3542 1.2181 sec/batch\n",
      "Epoch 3/3  Iteration 390/534 Training loss: 2.3542 0.9908 sec/batch\n",
      "Epoch 3/3  Iteration 391/534 Training loss: 2.3533 1.1095 sec/batch\n",
      "Epoch 3/3  Iteration 392/534 Training loss: 2.3533 0.9666 sec/batch\n",
      "Epoch 3/3  Iteration 393/534 Training loss: 2.3526 0.9809 sec/batch\n",
      "Epoch 3/3  Iteration 394/534 Training loss: 2.3514 1.0169 sec/batch\n",
      "Epoch 3/3  Iteration 395/534 Training loss: 2.3501 1.0268 sec/batch\n",
      "Epoch 3/3  Iteration 396/534 Training loss: 2.3490 1.0544 sec/batch\n",
      "Epoch 3/3  Iteration 397/534 Training loss: 2.3482 1.0820 sec/batch\n",
      "Epoch 3/3  Iteration 398/534 Training loss: 2.3470 1.3156 sec/batch\n",
      "Epoch 3/3  Iteration 399/534 Training loss: 2.3460 1.2823 sec/batch\n",
      "Epoch 3/3  Iteration 400/534 Training loss: 2.3453 1.0782 sec/batch\n",
      "Epoch 3/3  Iteration 401/534 Training loss: 2.3446 0.9906 sec/batch\n",
      "Epoch 3/3  Iteration 402/534 Training loss: 2.3433 1.0426 sec/batch\n",
      "Epoch 3/3  Iteration 403/534 Training loss: 2.3432 1.1219 sec/batch\n",
      "Epoch 3/3  Iteration 404/534 Training loss: 2.3426 1.1232 sec/batch\n",
      "Epoch 3/3  Iteration 405/534 Training loss: 2.3421 1.0571 sec/batch\n",
      "Epoch 3/3  Iteration 406/534 Training loss: 2.3421 1.2323 sec/batch\n",
      "Epoch 3/3  Iteration 407/534 Training loss: 2.3415 1.3250 sec/batch\n",
      "Epoch 3/3  Iteration 408/534 Training loss: 2.3413 1.2746 sec/batch\n",
      "Epoch 3/3  Iteration 409/534 Training loss: 2.3407 0.9732 sec/batch\n",
      "Epoch 3/3  Iteration 410/534 Training loss: 2.3400 1.0068 sec/batch\n",
      "Epoch 3/3  Iteration 411/534 Training loss: 2.3394 1.0440 sec/batch\n",
      "Epoch 3/3  Iteration 412/534 Training loss: 2.3390 0.9833 sec/batch\n",
      "Epoch 3/3  Iteration 413/534 Training loss: 2.3388 0.8535 sec/batch\n",
      "Epoch 3/3  Iteration 414/534 Training loss: 2.3383 0.9784 sec/batch\n",
      "Epoch 3/3  Iteration 415/534 Training loss: 2.3377 0.8522 sec/batch\n",
      "Epoch 3/3  Iteration 416/534 Training loss: 2.3375 0.9658 sec/batch\n",
      "Epoch 3/3  Iteration 417/534 Training loss: 2.3371 1.1687 sec/batch\n",
      "Epoch 3/3  Iteration 418/534 Training loss: 2.3370 1.0348 sec/batch\n",
      "Epoch 3/3  Iteration 419/534 Training loss: 2.3368 0.9195 sec/batch\n",
      "Epoch 3/3  Iteration 420/534 Training loss: 2.3363 0.9095 sec/batch\n",
      "Epoch 3/3  Iteration 421/534 Training loss: 2.3356 0.8557 sec/batch\n",
      "Epoch 3/3  Iteration 422/534 Training loss: 2.3354 0.8669 sec/batch\n",
      "Epoch 3/3  Iteration 423/534 Training loss: 2.3352 0.8588 sec/batch\n",
      "Epoch 3/3  Iteration 424/534 Training loss: 2.3343 0.8620 sec/batch\n",
      "Epoch 3/3  Iteration 425/534 Training loss: 2.3337 0.8417 sec/batch\n",
      "Epoch 3/3  Iteration 426/534 Training loss: 2.3336 0.9113 sec/batch\n",
      "Epoch 3/3  Iteration 427/534 Training loss: 2.3334 0.9935 sec/batch\n",
      "Epoch 3/3  Iteration 428/534 Training loss: 2.3333 1.1074 sec/batch\n",
      "Epoch 3/3  Iteration 429/534 Training loss: 2.3330 1.0337 sec/batch\n",
      "Epoch 3/3  Iteration 430/534 Training loss: 2.3325 1.1202 sec/batch\n",
      "Epoch 3/3  Iteration 431/534 Training loss: 2.3321 0.9767 sec/batch\n",
      "Epoch 3/3  Iteration 432/534 Training loss: 2.3322 0.9285 sec/batch\n",
      "Epoch 3/3  Iteration 433/534 Training loss: 2.3317 0.9896 sec/batch\n",
      "Epoch 3/3  Iteration 434/534 Training loss: 2.3315 0.9698 sec/batch\n",
      "Epoch 3/3  Iteration 435/534 Training loss: 2.3310 0.9264 sec/batch\n",
      "Epoch 3/3  Iteration 436/534 Training loss: 2.3304 0.9547 sec/batch\n",
      "Epoch 3/3  Iteration 437/534 Training loss: 2.3299 1.0221 sec/batch\n",
      "Epoch 3/3  Iteration 438/534 Training loss: 2.3296 1.0738 sec/batch\n",
      "Epoch 3/3  Iteration 439/534 Training loss: 2.3290 1.3292 sec/batch\n",
      "Epoch 3/3  Iteration 440/534 Training loss: 2.3286 0.9936 sec/batch\n",
      "Epoch 3/3  Iteration 441/534 Training loss: 2.3277 1.0192 sec/batch\n",
      "Epoch 3/3  Iteration 442/534 Training loss: 2.3271 0.9955 sec/batch\n",
      "Epoch 3/3  Iteration 443/534 Training loss: 2.3267 0.9986 sec/batch\n",
      "Epoch 3/3  Iteration 444/534 Training loss: 2.3261 0.9793 sec/batch\n",
      "Epoch 3/3  Iteration 445/534 Training loss: 2.3257 1.0081 sec/batch\n",
      "Epoch 3/3  Iteration 446/534 Training loss: 2.3254 0.9646 sec/batch\n",
      "Epoch 3/3  Iteration 447/534 Training loss: 2.3250 1.1279 sec/batch\n",
      "Epoch 3/3  Iteration 448/534 Training loss: 2.3246 1.3838 sec/batch\n",
      "Epoch 3/3  Iteration 449/534 Training loss: 2.3241 1.2435 sec/batch\n",
      "Epoch 3/3  Iteration 450/534 Training loss: 2.3235 0.9946 sec/batch\n",
      "Epoch 3/3  Iteration 451/534 Training loss: 2.3230 1.0250 sec/batch\n",
      "Epoch 3/3  Iteration 452/534 Training loss: 2.3225 1.0190 sec/batch\n",
      "Epoch 3/3  Iteration 453/534 Training loss: 2.3221 0.9817 sec/batch\n",
      "Epoch 3/3  Iteration 454/534 Training loss: 2.3217 0.9874 sec/batch\n",
      "Epoch 3/3  Iteration 455/534 Training loss: 2.3211 0.9956 sec/batch\n",
      "Epoch 3/3  Iteration 456/534 Training loss: 2.3207 1.0074 sec/batch\n",
      "Epoch 3/3  Iteration 457/534 Training loss: 2.3204 1.3741 sec/batch\n",
      "Epoch 3/3  Iteration 458/534 Training loss: 2.3201 1.2386 sec/batch\n",
      "Epoch 3/3  Iteration 459/534 Training loss: 2.3195 1.2166 sec/batch\n",
      "Epoch 3/3  Iteration 460/534 Training loss: 2.3190 1.0767 sec/batch\n",
      "Epoch 3/3  Iteration 461/534 Training loss: 2.3186 0.9603 sec/batch\n",
      "Epoch 3/3  Iteration 462/534 Training loss: 2.3182 1.0488 sec/batch\n",
      "Epoch 3/3  Iteration 463/534 Training loss: 2.3177 1.0334 sec/batch\n",
      "Epoch 3/3  Iteration 464/534 Training loss: 2.3174 1.1159 sec/batch\n",
      "Epoch 3/3  Iteration 465/534 Training loss: 2.3172 1.1133 sec/batch\n",
      "Epoch 3/3  Iteration 466/534 Training loss: 2.3166 1.3164 sec/batch\n",
      "Epoch 3/3  Iteration 467/534 Training loss: 2.3163 1.2570 sec/batch\n",
      "Epoch 3/3  Iteration 468/534 Training loss: 2.3162 0.9908 sec/batch\n",
      "Epoch 3/3  Iteration 469/534 Training loss: 2.3156 1.0801 sec/batch\n",
      "Epoch 3/3  Iteration 470/534 Training loss: 2.3152 1.0580 sec/batch\n",
      "Epoch 3/3  Iteration 471/534 Training loss: 2.3147 1.0191 sec/batch\n",
      "Epoch 3/3  Iteration 472/534 Training loss: 2.3141 1.0185 sec/batch\n",
      "Epoch 3/3  Iteration 473/534 Training loss: 2.3138 1.0254 sec/batch\n",
      "Epoch 3/3  Iteration 474/534 Training loss: 2.3135 1.1209 sec/batch\n",
      "Epoch 3/3  Iteration 475/534 Training loss: 2.3134 1.3036 sec/batch\n",
      "Epoch 3/3  Iteration 476/534 Training loss: 2.3131 1.3350 sec/batch\n",
      "Epoch 3/3  Iteration 477/534 Training loss: 2.3128 0.9712 sec/batch\n",
      "Epoch 3/3  Iteration 478/534 Training loss: 2.3125 1.2705 sec/batch\n",
      "Epoch 3/3  Iteration 479/534 Training loss: 2.3122 1.1854 sec/batch\n",
      "Epoch 3/3  Iteration 480/534 Training loss: 2.3120 1.5882 sec/batch\n",
      "Epoch 3/3  Iteration 481/534 Training loss: 2.3117 1.7614 sec/batch\n",
      "Epoch 3/3  Iteration 482/534 Training loss: 2.3112 1.8792 sec/batch\n",
      "Epoch 3/3  Iteration 483/534 Training loss: 2.3109 2.0485 sec/batch\n",
      "Epoch 3/3  Iteration 484/534 Training loss: 2.3107 1.1701 sec/batch\n",
      "Epoch 3/3  Iteration 485/534 Training loss: 2.3104 1.0240 sec/batch\n",
      "Epoch 3/3  Iteration 486/534 Training loss: 2.3101 1.0000 sec/batch\n",
      "Epoch 3/3  Iteration 487/534 Training loss: 2.3098 0.9090 sec/batch\n",
      "Epoch 3/3  Iteration 488/534 Training loss: 2.3093 0.9781 sec/batch\n",
      "Epoch 3/3  Iteration 489/534 Training loss: 2.3090 1.3029 sec/batch\n",
      "Epoch 3/3  Iteration 490/534 Training loss: 2.3088 0.9430 sec/batch\n",
      "Epoch 3/3  Iteration 491/534 Training loss: 2.3084 1.1775 sec/batch\n",
      "Epoch 3/3  Iteration 492/534 Training loss: 2.3081 1.0599 sec/batch\n",
      "Epoch 3/3  Iteration 493/534 Training loss: 2.3078 1.2301 sec/batch\n",
      "Epoch 3/3  Iteration 494/534 Training loss: 2.3075 1.0078 sec/batch\n",
      "Epoch 3/3  Iteration 495/534 Training loss: 2.3074 1.1324 sec/batch\n",
      "Epoch 3/3  Iteration 496/534 Training loss: 2.3070 1.1606 sec/batch\n",
      "Epoch 3/3  Iteration 497/534 Training loss: 2.3069 1.0821 sec/batch\n",
      "Epoch 3/3  Iteration 498/534 Training loss: 2.3065 1.0993 sec/batch\n",
      "Epoch 3/3  Iteration 499/534 Training loss: 2.3063 1.0835 sec/batch\n",
      "Epoch 3/3  Iteration 500/534 Training loss: 2.3059 1.1248 sec/batch\n",
      "Epoch 3/3  Iteration 501/534 Training loss: 2.3056 1.6744 sec/batch\n",
      "Epoch 3/3  Iteration 502/534 Training loss: 2.3055 1.0972 sec/batch\n",
      "Epoch 3/3  Iteration 503/534 Training loss: 2.3053 1.0281 sec/batch\n",
      "Epoch 3/3  Iteration 504/534 Training loss: 2.3051 0.8122 sec/batch\n",
      "Epoch 3/3  Iteration 505/534 Training loss: 2.3048 1.8307 sec/batch\n",
      "Epoch 3/3  Iteration 506/534 Training loss: 2.3044 0.8549 sec/batch\n",
      "Epoch 3/3  Iteration 507/534 Training loss: 2.3042 0.7966 sec/batch\n",
      "Epoch 3/3  Iteration 508/534 Training loss: 2.3043 1.0068 sec/batch\n",
      "Epoch 3/3  Iteration 509/534 Training loss: 2.3039 1.4460 sec/batch\n",
      "Epoch 3/3  Iteration 510/534 Training loss: 2.3037 1.2754 sec/batch\n",
      "Epoch 3/3  Iteration 511/534 Training loss: 2.3034 0.8292 sec/batch\n",
      "Epoch 3/3  Iteration 512/534 Training loss: 2.3031 0.8038 sec/batch\n",
      "Epoch 3/3  Iteration 513/534 Training loss: 2.3028 0.8014 sec/batch\n",
      "Epoch 3/3  Iteration 514/534 Training loss: 2.3024 0.7996 sec/batch\n",
      "Epoch 3/3  Iteration 515/534 Training loss: 2.3020 0.7218 sec/batch\n",
      "Epoch 3/3  Iteration 516/534 Training loss: 2.3018 0.7234 sec/batch\n",
      "Epoch 3/3  Iteration 517/534 Training loss: 2.3016 0.7656 sec/batch\n",
      "Epoch 3/3  Iteration 518/534 Training loss: 2.3011 0.7815 sec/batch\n",
      "Epoch 3/3  Iteration 519/534 Training loss: 2.3008 0.7521 sec/batch\n",
      "Epoch 3/3  Iteration 520/534 Training loss: 2.3005 0.8992 sec/batch\n",
      "Epoch 3/3  Iteration 521/534 Training loss: 2.3002 1.0253 sec/batch\n",
      "Epoch 3/3  Iteration 522/534 Training loss: 2.2999 0.8385 sec/batch\n",
      "Epoch 3/3  Iteration 523/534 Training loss: 2.2997 0.7251 sec/batch\n",
      "Epoch 3/3  Iteration 524/534 Training loss: 2.2995 0.7473 sec/batch\n",
      "Epoch 3/3  Iteration 525/534 Training loss: 2.2992 0.8524 sec/batch\n",
      "Epoch 3/3  Iteration 526/534 Training loss: 2.2988 0.7772 sec/batch\n",
      "Epoch 3/3  Iteration 527/534 Training loss: 2.2986 0.7027 sec/batch\n",
      "Epoch 3/3  Iteration 528/534 Training loss: 2.2984 0.7032 sec/batch\n",
      "Epoch 3/3  Iteration 529/534 Training loss: 2.2983 0.8353 sec/batch\n",
      "Epoch 3/3  Iteration 530/534 Training loss: 2.2982 0.7268 sec/batch\n",
      "Epoch 3/3  Iteration 531/534 Training loss: 2.2981 0.7875 sec/batch\n",
      "Epoch 3/3  Iteration 532/534 Training loss: 2.2978 0.7777 sec/batch\n",
      "Epoch 3/3  Iteration 533/534 Training loss: 2.2974 1.1567 sec/batch\n",
      "Epoch 3/3  Iteration 534/534 Training loss: 2.2970 1.3386 sec/batch\n",
      "Epoch 1/3  Iteration 1/534 Training loss: 4.4179 2.5071 sec/batch\n",
      "Epoch 1/3  Iteration 2/534 Training loss: 4.4111 1.1265 sec/batch\n",
      "Epoch 1/3  Iteration 3/534 Training loss: 4.4033 1.2072 sec/batch\n",
      "Epoch 1/3  Iteration 4/534 Training loss: 4.3929 1.0321 sec/batch\n",
      "Epoch 1/3  Iteration 5/534 Training loss: 4.3780 1.1024 sec/batch\n",
      "Epoch 1/3  Iteration 6/534 Training loss: 4.3549 0.9469 sec/batch\n",
      "Epoch 1/3  Iteration 7/534 Training loss: 4.3153 1.3125 sec/batch\n",
      "Epoch 1/3  Iteration 8/534 Training loss: 4.2595 1.3367 sec/batch\n",
      "Epoch 1/3  Iteration 9/534 Training loss: 4.1976 1.0774 sec/batch\n",
      "Epoch 1/3  Iteration 10/534 Training loss: 4.1402 0.9288 sec/batch\n",
      "Epoch 1/3  Iteration 11/534 Training loss: 4.0862 0.9428 sec/batch\n",
      "Epoch 1/3  Iteration 12/534 Training loss: 4.0399 0.9105 sec/batch\n",
      "Epoch 1/3  Iteration 13/534 Training loss: 3.9963 0.9354 sec/batch\n",
      "Epoch 1/3  Iteration 14/534 Training loss: 3.9589 0.9243 sec/batch\n",
      "Epoch 1/3  Iteration 15/534 Training loss: 3.9233 1.0101 sec/batch\n",
      "Epoch 1/3  Iteration 16/534 Training loss: 3.8903 1.1580 sec/batch\n",
      "Epoch 1/3  Iteration 17/534 Training loss: 3.8603 1.1555 sec/batch\n",
      "Epoch 1/3  Iteration 18/534 Training loss: 3.8342 1.1603 sec/batch\n",
      "Epoch 1/3  Iteration 19/534 Training loss: 3.8099 1.0151 sec/batch\n",
      "Epoch 1/3  Iteration 20/534 Training loss: 3.7851 0.8743 sec/batch\n",
      "Epoch 1/3  Iteration 21/534 Training loss: 3.7642 0.9205 sec/batch\n",
      "Epoch 1/3  Iteration 22/534 Training loss: 3.7437 0.9346 sec/batch\n",
      "Epoch 1/3  Iteration 23/534 Training loss: 3.7247 1.0275 sec/batch\n",
      "Epoch 1/3  Iteration 24/534 Training loss: 3.7067 1.0742 sec/batch\n",
      "Epoch 1/3  Iteration 25/534 Training loss: 3.6899 0.9534 sec/batch\n",
      "Epoch 1/3  Iteration 26/534 Training loss: 3.6747 0.8614 sec/batch\n",
      "Epoch 1/3  Iteration 27/534 Training loss: 3.6603 1.1923 sec/batch\n",
      "Epoch 1/3  Iteration 28/534 Training loss: 3.6458 1.1876 sec/batch\n",
      "Epoch 1/3  Iteration 29/534 Training loss: 3.6328 1.1698 sec/batch\n",
      "Epoch 1/3  Iteration 30/534 Training loss: 3.6205 0.9900 sec/batch\n",
      "Epoch 1/3  Iteration 31/534 Training loss: 3.6098 0.9428 sec/batch\n",
      "Epoch 1/3  Iteration 32/534 Training loss: 3.5985 0.9993 sec/batch\n",
      "Epoch 1/3  Iteration 33/534 Training loss: 3.5876 0.9464 sec/batch\n",
      "Epoch 1/3  Iteration 34/534 Training loss: 3.5778 0.9898 sec/batch\n",
      "Epoch 1/3  Iteration 35/534 Training loss: 3.5678 1.0698 sec/batch\n",
      "Epoch 1/3  Iteration 36/534 Training loss: 3.5587 1.0511 sec/batch\n",
      "Epoch 1/3  Iteration 37/534 Training loss: 3.5494 0.9525 sec/batch\n",
      "Epoch 1/3  Iteration 38/534 Training loss: 3.5407 0.9165 sec/batch\n",
      "Epoch 1/3  Iteration 39/534 Training loss: 3.5321 0.9461 sec/batch\n",
      "Epoch 1/3  Iteration 40/534 Training loss: 3.5243 1.4067 sec/batch\n",
      "Epoch 1/3  Iteration 41/534 Training loss: 3.5165 1.1054 sec/batch\n",
      "Epoch 1/3  Iteration 42/534 Training loss: 3.5092 0.9551 sec/batch\n",
      "Epoch 1/3  Iteration 43/534 Training loss: 3.5020 0.8838 sec/batch\n",
      "Epoch 1/3  Iteration 44/534 Training loss: 3.4952 0.9313 sec/batch\n",
      "Epoch 1/3  Iteration 45/534 Training loss: 3.4885 0.9377 sec/batch\n",
      "Epoch 1/3  Iteration 46/534 Training loss: 3.4825 0.8714 sec/batch\n",
      "Epoch 1/3  Iteration 47/534 Training loss: 3.4767 0.8455 sec/batch\n",
      "Epoch 1/3  Iteration 48/534 Training loss: 3.4711 1.0089 sec/batch\n",
      "Epoch 1/3  Iteration 49/534 Training loss: 3.4657 0.9952 sec/batch\n",
      "Epoch 1/3  Iteration 50/534 Training loss: 3.4605 1.1756 sec/batch\n",
      "Epoch 1/3  Iteration 51/534 Training loss: 3.4554 1.2321 sec/batch\n",
      "Epoch 1/3  Iteration 52/534 Training loss: 3.4503 0.9734 sec/batch\n",
      "Epoch 1/3  Iteration 53/534 Training loss: 3.4457 0.9078 sec/batch\n",
      "Epoch 1/3  Iteration 54/534 Training loss: 3.4409 0.8821 sec/batch\n",
      "Epoch 1/3  Iteration 55/534 Training loss: 3.4364 0.9274 sec/batch\n",
      "Epoch 1/3  Iteration 56/534 Training loss: 3.4318 0.9354 sec/batch\n",
      "Epoch 1/3  Iteration 57/534 Training loss: 3.4275 0.9311 sec/batch\n",
      "Epoch 1/3  Iteration 58/534 Training loss: 3.4233 0.9551 sec/batch\n",
      "Epoch 1/3  Iteration 59/534 Training loss: 3.4191 0.9247 sec/batch\n",
      "Epoch 1/3  Iteration 60/534 Training loss: 3.4152 1.0366 sec/batch\n",
      "Epoch 1/3  Iteration 61/534 Training loss: 3.4113 1.1292 sec/batch\n",
      "Epoch 1/3  Iteration 62/534 Training loss: 3.4080 1.1633 sec/batch\n",
      "Epoch 1/3  Iteration 63/534 Training loss: 3.4048 0.9948 sec/batch\n",
      "Epoch 1/3  Iteration 64/534 Training loss: 3.4009 0.9262 sec/batch\n",
      "Epoch 1/3  Iteration 65/534 Training loss: 3.3974 1.0228 sec/batch\n",
      "Epoch 1/3  Iteration 66/534 Training loss: 3.3943 0.8346 sec/batch\n",
      "Epoch 1/3  Iteration 67/534 Training loss: 3.3912 0.8130 sec/batch\n",
      "Epoch 1/3  Iteration 68/534 Training loss: 3.3876 0.8292 sec/batch\n",
      "Epoch 1/3  Iteration 69/534 Training loss: 3.3843 1.0602 sec/batch\n",
      "Epoch 1/3  Iteration 70/534 Training loss: 3.3814 0.9654 sec/batch\n",
      "Epoch 1/3  Iteration 71/534 Training loss: 3.3784 1.2134 sec/batch\n",
      "Epoch 1/3  Iteration 72/534 Training loss: 3.3758 1.4064 sec/batch\n",
      "Epoch 1/3  Iteration 73/534 Training loss: 3.3729 1.2072 sec/batch\n",
      "Epoch 1/3  Iteration 74/534 Training loss: 3.3703 0.9632 sec/batch\n",
      "Epoch 1/3  Iteration 75/534 Training loss: 3.3677 0.8945 sec/batch\n",
      "Epoch 1/3  Iteration 76/534 Training loss: 3.3653 0.9553 sec/batch\n",
      "Epoch 1/3  Iteration 77/534 Training loss: 3.3629 0.9165 sec/batch\n",
      "Epoch 1/3  Iteration 78/534 Training loss: 3.3604 0.8944 sec/batch\n",
      "Epoch 1/3  Iteration 79/534 Training loss: 3.3579 0.8943 sec/batch\n",
      "Epoch 1/3  Iteration 80/534 Training loss: 3.3554 0.8967 sec/batch\n",
      "Epoch 1/3  Iteration 81/534 Training loss: 3.3530 0.9757 sec/batch\n",
      "Epoch 1/3  Iteration 82/534 Training loss: 3.3508 1.0621 sec/batch\n",
      "Epoch 1/3  Iteration 83/534 Training loss: 3.3488 1.0053 sec/batch\n",
      "Epoch 1/3  Iteration 84/534 Training loss: 3.3466 0.9472 sec/batch\n",
      "Epoch 1/3  Iteration 85/534 Training loss: 3.3443 0.8607 sec/batch\n",
      "Epoch 1/3  Iteration 86/534 Training loss: 3.3421 0.8859 sec/batch\n",
      "Epoch 1/3  Iteration 87/534 Training loss: 3.3399 0.8930 sec/batch\n",
      "Epoch 1/3  Iteration 88/534 Training loss: 3.3379 0.8833 sec/batch\n",
      "Epoch 1/3  Iteration 89/534 Training loss: 3.3360 0.8700 sec/batch\n",
      "Epoch 1/3  Iteration 90/534 Training loss: 3.3341 0.9060 sec/batch\n",
      "Epoch 1/3  Iteration 91/534 Training loss: 3.3323 0.8548 sec/batch\n",
      "Epoch 1/3  Iteration 92/534 Training loss: 3.3304 0.9526 sec/batch\n",
      "Epoch 1/3  Iteration 93/534 Training loss: 3.3286 1.0443 sec/batch\n",
      "Epoch 1/3  Iteration 94/534 Training loss: 3.3269 1.0527 sec/batch\n",
      "Epoch 1/3  Iteration 95/534 Training loss: 3.3251 0.9415 sec/batch\n",
      "Epoch 1/3  Iteration 96/534 Training loss: 3.3233 0.9625 sec/batch\n",
      "Epoch 1/3  Iteration 97/534 Training loss: 3.3216 0.8641 sec/batch\n",
      "Epoch 1/3  Iteration 98/534 Training loss: 3.3199 0.8344 sec/batch\n",
      "Epoch 1/3  Iteration 99/534 Training loss: 3.3182 0.8758 sec/batch\n",
      "Epoch 1/3  Iteration 100/534 Training loss: 3.3166 0.9974 sec/batch\n",
      "Epoch 1/3  Iteration 101/534 Training loss: 3.3150 0.9537 sec/batch\n",
      "Epoch 1/3  Iteration 102/534 Training loss: 3.3135 0.8582 sec/batch\n",
      "Epoch 1/3  Iteration 103/534 Training loss: 3.3120 0.9513 sec/batch\n",
      "Epoch 1/3  Iteration 104/534 Training loss: 3.3105 0.9732 sec/batch\n",
      "Epoch 1/3  Iteration 105/534 Training loss: 3.3091 1.1447 sec/batch\n",
      "Epoch 1/3  Iteration 106/534 Training loss: 3.3076 0.9451 sec/batch\n",
      "Epoch 1/3  Iteration 107/534 Training loss: 3.3060 0.8973 sec/batch\n",
      "Epoch 1/3  Iteration 108/534 Training loss: 3.3044 0.8871 sec/batch\n",
      "Epoch 1/3  Iteration 109/534 Training loss: 3.3030 0.8529 sec/batch\n",
      "Epoch 1/3  Iteration 110/534 Training loss: 3.3013 0.9031 sec/batch\n",
      "Epoch 1/3  Iteration 111/534 Training loss: 3.2999 0.8878 sec/batch\n",
      "Epoch 1/3  Iteration 112/534 Training loss: 3.2986 0.8961 sec/batch\n",
      "Epoch 1/3  Iteration 113/534 Training loss: 3.2971 0.9142 sec/batch\n",
      "Epoch 1/3  Iteration 114/534 Training loss: 3.2956 0.8843 sec/batch\n",
      "Epoch 1/3  Iteration 115/534 Training loss: 3.2941 0.8846 sec/batch\n",
      "Epoch 1/3  Iteration 116/534 Training loss: 3.2927 0.9166 sec/batch\n",
      "Epoch 1/3  Iteration 117/534 Training loss: 3.2913 0.8681 sec/batch\n",
      "Epoch 1/3  Iteration 118/534 Training loss: 3.2901 1.0683 sec/batch\n",
      "Epoch 1/3  Iteration 119/534 Training loss: 3.2890 1.0369 sec/batch\n",
      "Epoch 1/3  Iteration 120/534 Training loss: 3.2876 0.8928 sec/batch\n",
      "Epoch 1/3  Iteration 121/534 Training loss: 3.2865 0.8869 sec/batch\n",
      "Epoch 1/3  Iteration 122/534 Training loss: 3.2853 0.9013 sec/batch\n",
      "Epoch 1/3  Iteration 123/534 Training loss: 3.2842 0.8549 sec/batch\n",
      "Epoch 1/3  Iteration 124/534 Training loss: 3.2831 0.9587 sec/batch\n",
      "Epoch 1/3  Iteration 125/534 Training loss: 3.2818 0.8451 sec/batch\n",
      "Epoch 1/3  Iteration 126/534 Training loss: 3.2805 0.9129 sec/batch\n",
      "Epoch 1/3  Iteration 127/534 Training loss: 3.2793 0.8506 sec/batch\n",
      "Epoch 1/3  Iteration 128/534 Training loss: 3.2782 0.8907 sec/batch\n",
      "Epoch 1/3  Iteration 129/534 Training loss: 3.2770 1.0607 sec/batch\n",
      "Epoch 1/3  Iteration 130/534 Training loss: 3.2758 1.0639 sec/batch\n",
      "Epoch 1/3  Iteration 131/534 Training loss: 3.2747 1.0072 sec/batch\n",
      "Epoch 1/3  Iteration 132/534 Training loss: 3.2735 0.8845 sec/batch\n",
      "Epoch 1/3  Iteration 133/534 Training loss: 3.2724 0.9099 sec/batch\n",
      "Epoch 1/3  Iteration 134/534 Training loss: 3.2713 0.8848 sec/batch\n",
      "Epoch 1/3  Iteration 135/534 Training loss: 3.2699 0.8389 sec/batch\n",
      "Epoch 1/3  Iteration 136/534 Training loss: 3.2686 0.9275 sec/batch\n",
      "Epoch 1/3  Iteration 137/534 Training loss: 3.2675 0.9416 sec/batch\n",
      "Epoch 1/3  Iteration 138/534 Training loss: 3.2662 0.8579 sec/batch\n",
      "Epoch 1/3  Iteration 139/534 Training loss: 3.2651 0.8815 sec/batch\n",
      "Epoch 1/3  Iteration 140/534 Training loss: 3.2640 0.8972 sec/batch\n",
      "Epoch 1/3  Iteration 141/534 Training loss: 3.2629 1.1035 sec/batch\n",
      "Epoch 1/3  Iteration 142/534 Training loss: 3.2616 1.0223 sec/batch\n",
      "Epoch 1/3  Iteration 143/534 Training loss: 3.2604 0.8770 sec/batch\n",
      "Epoch 1/3  Iteration 144/534 Training loss: 3.2593 0.8725 sec/batch\n",
      "Epoch 1/3  Iteration 145/534 Training loss: 3.2582 0.9256 sec/batch\n",
      "Epoch 1/3  Iteration 146/534 Training loss: 3.2570 0.9839 sec/batch\n",
      "Epoch 1/3  Iteration 147/534 Training loss: 3.2560 0.8926 sec/batch\n",
      "Epoch 1/3  Iteration 148/534 Training loss: 3.2551 0.8708 sec/batch\n",
      "Epoch 1/3  Iteration 149/534 Training loss: 3.2539 0.9538 sec/batch\n",
      "Epoch 1/3  Iteration 150/534 Training loss: 3.2528 0.8313 sec/batch\n",
      "Epoch 1/3  Iteration 151/534 Training loss: 3.2518 0.9279 sec/batch\n",
      "Epoch 1/3  Iteration 152/534 Training loss: 3.2508 1.0803 sec/batch\n",
      "Epoch 1/3  Iteration 153/534 Training loss: 3.2497 1.0543 sec/batch\n",
      "Epoch 1/3  Iteration 154/534 Training loss: 3.2486 0.9362 sec/batch\n",
      "Epoch 1/3  Iteration 155/534 Training loss: 3.2474 0.9134 sec/batch\n",
      "Epoch 1/3  Iteration 156/534 Training loss: 3.2463 0.8651 sec/batch\n",
      "Epoch 1/3  Iteration 157/534 Training loss: 3.2451 0.8542 sec/batch\n",
      "Epoch 1/3  Iteration 158/534 Training loss: 3.2439 0.8697 sec/batch\n",
      "Epoch 1/3  Iteration 159/534 Training loss: 3.2426 0.9092 sec/batch\n",
      "Epoch 1/3  Iteration 160/534 Training loss: 3.2413 0.9238 sec/batch\n",
      "Epoch 1/3  Iteration 161/534 Training loss: 3.2402 0.8672 sec/batch\n",
      "Epoch 1/3  Iteration 162/534 Training loss: 3.2389 0.9145 sec/batch\n",
      "Epoch 1/3  Iteration 163/534 Training loss: 3.2375 1.0410 sec/batch\n",
      "Epoch 1/3  Iteration 164/534 Training loss: 3.2364 1.0235 sec/batch\n",
      "Epoch 1/3  Iteration 165/534 Training loss: 3.2351 1.1932 sec/batch\n",
      "Epoch 1/3  Iteration 166/534 Training loss: 3.2338 0.8737 sec/batch\n",
      "Epoch 1/3  Iteration 167/534 Training loss: 3.2326 0.9042 sec/batch\n",
      "Epoch 1/3  Iteration 168/534 Training loss: 3.2314 0.8671 sec/batch\n",
      "Epoch 1/3  Iteration 169/534 Training loss: 3.2301 0.8613 sec/batch\n",
      "Epoch 1/3  Iteration 170/534 Training loss: 3.2288 0.8849 sec/batch\n",
      "Epoch 1/3  Iteration 171/534 Training loss: 3.2275 0.8974 sec/batch\n",
      "Epoch 1/3  Iteration 172/534 Training loss: 3.2265 0.9055 sec/batch\n",
      "Epoch 1/3  Iteration 173/534 Training loss: 3.2255 0.8979 sec/batch\n",
      "Epoch 1/3  Iteration 174/534 Training loss: 3.2244 0.9140 sec/batch\n",
      "Epoch 1/3  Iteration 175/534 Training loss: 3.2232 1.1153 sec/batch\n",
      "Epoch 1/3  Iteration 176/534 Training loss: 3.2220 1.0947 sec/batch\n",
      "Epoch 1/3  Iteration 177/534 Training loss: 3.2207 0.9366 sec/batch\n",
      "Epoch 1/3  Iteration 178/534 Training loss: 3.2192 0.8635 sec/batch\n",
      "Epoch 2/3  Iteration 179/534 Training loss: 3.0187 0.8213 sec/batch\n",
      "Epoch 2/3  Iteration 180/534 Training loss: 2.9815 0.8956 sec/batch\n",
      "Epoch 2/3  Iteration 181/534 Training loss: 2.9676 0.8700 sec/batch\n",
      "Epoch 2/3  Iteration 182/534 Training loss: 2.9658 0.8419 sec/batch\n",
      "Epoch 2/3  Iteration 183/534 Training loss: 2.9638 0.8361 sec/batch\n",
      "Epoch 2/3  Iteration 184/534 Training loss: 2.9645 0.7534 sec/batch\n",
      "Epoch 2/3  Iteration 185/534 Training loss: 2.9634 0.7181 sec/batch\n",
      "Epoch 2/3  Iteration 186/534 Training loss: 2.9632 0.7757 sec/batch\n",
      "Epoch 2/3  Iteration 187/534 Training loss: 2.9605 0.9226 sec/batch\n",
      "Epoch 2/3  Iteration 188/534 Training loss: 2.9580 1.0633 sec/batch\n",
      "Epoch 2/3  Iteration 189/534 Training loss: 2.9554 1.0282 sec/batch\n",
      "Epoch 2/3  Iteration 190/534 Training loss: 2.9531 0.7598 sec/batch\n",
      "Epoch 2/3  Iteration 191/534 Training loss: 2.9516 0.9521 sec/batch\n",
      "Epoch 2/3  Iteration 192/534 Training loss: 2.9515 0.8703 sec/batch\n",
      "Epoch 2/3  Iteration 193/534 Training loss: 2.9499 0.7102 sec/batch\n",
      "Epoch 2/3  Iteration 194/534 Training loss: 2.9486 0.6980 sec/batch\n",
      "Epoch 2/3  Iteration 195/534 Training loss: 2.9463 0.8226 sec/batch\n",
      "Epoch 2/3  Iteration 196/534 Training loss: 2.9465 0.7730 sec/batch\n",
      "Epoch 2/3  Iteration 197/534 Training loss: 2.9453 0.8261 sec/batch\n",
      "Epoch 2/3  Iteration 198/534 Training loss: 2.9422 0.9656 sec/batch\n",
      "Epoch 2/3  Iteration 199/534 Training loss: 2.9403 0.9079 sec/batch\n",
      "Epoch 2/3  Iteration 200/534 Training loss: 2.9395 0.9034 sec/batch\n",
      "Epoch 2/3  Iteration 201/534 Training loss: 2.9375 0.8739 sec/batch\n",
      "Epoch 2/3  Iteration 202/534 Training loss: 2.9355 1.1066 sec/batch\n",
      "Epoch 2/3  Iteration 203/534 Training loss: 2.9334 1.1304 sec/batch\n",
      "Epoch 2/3  Iteration 204/534 Training loss: 2.9318 0.8992 sec/batch\n",
      "Epoch 2/3  Iteration 205/534 Training loss: 2.9303 0.9707 sec/batch\n",
      "Epoch 2/3  Iteration 206/534 Training loss: 2.9279 0.8824 sec/batch\n",
      "Epoch 2/3  Iteration 207/534 Training loss: 2.9263 0.8720 sec/batch\n",
      "Epoch 2/3  Iteration 208/534 Training loss: 2.9245 0.9410 sec/batch\n",
      "Epoch 2/3  Iteration 209/534 Training loss: 2.9237 0.9691 sec/batch\n",
      "Epoch 2/3  Iteration 210/534 Training loss: 2.9215 0.9759 sec/batch\n",
      "Epoch 2/3  Iteration 211/534 Training loss: 2.9193 0.9518 sec/batch\n",
      "Epoch 2/3  Iteration 212/534 Training loss: 2.9176 0.9801 sec/batch\n",
      "Epoch 2/3  Iteration 213/534 Training loss: 2.9153 1.1493 sec/batch\n",
      "Epoch 2/3  Iteration 214/534 Training loss: 2.9138 1.0770 sec/batch\n",
      "Epoch 2/3  Iteration 215/534 Training loss: 2.9116 0.9320 sec/batch\n",
      "Epoch 2/3  Iteration 216/534 Training loss: 2.9092 0.8984 sec/batch\n",
      "Epoch 2/3  Iteration 217/534 Training loss: 2.9073 0.9505 sec/batch\n",
      "Epoch 2/3  Iteration 218/534 Training loss: 2.9054 0.9295 sec/batch\n",
      "Epoch 2/3  Iteration 219/534 Training loss: 2.9032 0.9704 sec/batch\n",
      "Epoch 2/3  Iteration 220/534 Training loss: 2.9013 0.9154 sec/batch\n",
      "Epoch 2/3  Iteration 221/534 Training loss: 2.8991 1.0053 sec/batch\n",
      "Epoch 2/3  Iteration 222/534 Training loss: 2.8971 0.9883 sec/batch\n",
      "Epoch 2/3  Iteration 223/534 Training loss: 2.8949 0.9624 sec/batch\n",
      "Epoch 2/3  Iteration 224/534 Training loss: 2.8928 1.1936 sec/batch\n",
      "Epoch 2/3  Iteration 225/534 Training loss: 2.8911 1.0568 sec/batch\n",
      "Epoch 2/3  Iteration 226/534 Training loss: 2.8893 0.9210 sec/batch\n",
      "Epoch 2/3  Iteration 227/534 Training loss: 2.8876 1.0114 sec/batch\n",
      "Epoch 2/3  Iteration 228/534 Training loss: 2.8863 1.0270 sec/batch\n",
      "Epoch 2/3  Iteration 229/534 Training loss: 2.8845 2.0389 sec/batch\n",
      "Epoch 2/3  Iteration 230/534 Training loss: 2.8826 1.2596 sec/batch\n",
      "Epoch 2/3  Iteration 231/534 Training loss: 2.8809 1.2290 sec/batch\n",
      "Epoch 2/3  Iteration 232/534 Training loss: 2.8790 1.7920 sec/batch\n",
      "Epoch 2/3  Iteration 233/534 Training loss: 2.8771 1.2306 sec/batch\n",
      "Epoch 2/3  Iteration 234/534 Training loss: 2.8753 1.0753 sec/batch\n",
      "Epoch 2/3  Iteration 235/534 Training loss: 2.8735 2.0902 sec/batch\n",
      "Epoch 2/3  Iteration 236/534 Training loss: 2.8717 1.1121 sec/batch\n",
      "Epoch 2/3  Iteration 237/534 Training loss: 2.8697 1.0952 sec/batch\n",
      "Epoch 2/3  Iteration 238/534 Training loss: 2.8679 1.1543 sec/batch\n",
      "Epoch 2/3  Iteration 239/534 Training loss: 2.8661 1.1617 sec/batch\n",
      "Epoch 2/3  Iteration 240/534 Training loss: 2.8647 1.4345 sec/batch\n",
      "Epoch 2/3  Iteration 241/534 Training loss: 2.8632 1.2830 sec/batch\n",
      "Epoch 2/3  Iteration 242/534 Training loss: 2.8613 1.0093 sec/batch\n",
      "Epoch 2/3  Iteration 243/534 Training loss: 2.8593 1.1945 sec/batch\n",
      "Epoch 2/3  Iteration 244/534 Training loss: 2.8580 0.9044 sec/batch\n",
      "Epoch 2/3  Iteration 245/534 Training loss: 2.8564 0.9004 sec/batch\n",
      "Epoch 2/3  Iteration 246/534 Training loss: 2.8543 0.8670 sec/batch\n",
      "Epoch 2/3  Iteration 247/534 Training loss: 2.8523 0.9250 sec/batch\n",
      "Epoch 2/3  Iteration 248/534 Training loss: 2.8509 0.9203 sec/batch\n",
      "Epoch 2/3  Iteration 249/534 Training loss: 2.8492 0.9557 sec/batch\n",
      "Epoch 2/3  Iteration 250/534 Training loss: 2.8477 1.5444 sec/batch\n",
      "Epoch 2/3  Iteration 251/534 Training loss: 2.8462 1.5097 sec/batch\n",
      "Epoch 2/3  Iteration 252/534 Training loss: 2.8444 1.4945 sec/batch\n",
      "Epoch 2/3  Iteration 253/534 Training loss: 2.8429 1.3467 sec/batch\n",
      "Epoch 2/3  Iteration 254/534 Training loss: 2.8417 1.4178 sec/batch\n",
      "Epoch 2/3  Iteration 255/534 Training loss: 2.8402 1.0596 sec/batch\n",
      "Epoch 2/3  Iteration 256/534 Training loss: 2.8386 1.2879 sec/batch\n",
      "Epoch 2/3  Iteration 257/534 Training loss: 2.8370 1.1314 sec/batch\n",
      "Epoch 2/3  Iteration 258/534 Training loss: 2.8351 1.0829 sec/batch\n",
      "Epoch 2/3  Iteration 259/534 Training loss: 2.8335 1.0945 sec/batch\n",
      "Epoch 2/3  Iteration 260/534 Training loss: 2.8320 0.9479 sec/batch\n",
      "Epoch 2/3  Iteration 261/534 Training loss: 2.8303 1.5206 sec/batch\n",
      "Epoch 2/3  Iteration 262/534 Training loss: 2.8285 1.5311 sec/batch\n",
      "Epoch 2/3  Iteration 263/534 Training loss: 2.8266 0.8370 sec/batch\n",
      "Epoch 2/3  Iteration 264/534 Training loss: 2.8249 0.8006 sec/batch\n",
      "Epoch 2/3  Iteration 265/534 Training loss: 2.8233 0.7880 sec/batch\n",
      "Epoch 2/3  Iteration 266/534 Training loss: 2.8217 1.0238 sec/batch\n",
      "Epoch 2/3  Iteration 267/534 Training loss: 2.8200 1.0403 sec/batch\n",
      "Epoch 2/3  Iteration 268/534 Training loss: 2.8186 0.7602 sec/batch\n",
      "Epoch 2/3  Iteration 269/534 Training loss: 2.8172 0.8471 sec/batch\n",
      "Epoch 2/3  Iteration 270/534 Training loss: 2.8157 0.7777 sec/batch\n",
      "Epoch 2/3  Iteration 271/534 Training loss: 2.8142 0.8806 sec/batch\n",
      "Epoch 2/3  Iteration 272/534 Training loss: 2.8126 1.0341 sec/batch\n",
      "Epoch 2/3  Iteration 273/534 Training loss: 2.8108 0.9424 sec/batch\n",
      "Epoch 2/3  Iteration 274/534 Training loss: 2.8092 0.7816 sec/batch\n",
      "Epoch 2/3  Iteration 275/534 Training loss: 2.8078 0.7912 sec/batch\n",
      "Epoch 2/3  Iteration 276/534 Training loss: 2.8062 0.9491 sec/batch\n",
      "Epoch 2/3  Iteration 277/534 Training loss: 2.8049 0.9582 sec/batch\n",
      "Epoch 2/3  Iteration 278/534 Training loss: 2.8034 0.9798 sec/batch\n",
      "Epoch 2/3  Iteration 279/534 Training loss: 2.8021 0.9157 sec/batch\n",
      "Epoch 2/3  Iteration 280/534 Training loss: 2.8007 0.7356 sec/batch\n",
      "Epoch 2/3  Iteration 281/534 Training loss: 2.7992 1.2550 sec/batch\n",
      "Epoch 2/3  Iteration 282/534 Training loss: 2.7977 1.2084 sec/batch\n",
      "Epoch 2/3  Iteration 283/534 Training loss: 2.7962 1.3209 sec/batch\n",
      "Epoch 2/3  Iteration 284/534 Training loss: 2.7948 1.1488 sec/batch\n",
      "Epoch 2/3  Iteration 285/534 Training loss: 2.7932 1.0468 sec/batch\n",
      "Epoch 2/3  Iteration 286/534 Training loss: 2.7919 0.7313 sec/batch\n",
      "Epoch 2/3  Iteration 287/534 Training loss: 2.7907 0.6932 sec/batch\n",
      "Epoch 2/3  Iteration 288/534 Training loss: 2.7891 0.7333 sec/batch\n",
      "Epoch 2/3  Iteration 289/534 Training loss: 2.7877 0.7288 sec/batch\n",
      "Epoch 2/3  Iteration 290/534 Training loss: 2.7865 0.6854 sec/batch\n",
      "Epoch 2/3  Iteration 291/534 Training loss: 2.7851 0.7838 sec/batch\n",
      "Epoch 2/3  Iteration 292/534 Training loss: 2.7836 0.7310 sec/batch\n",
      "Epoch 2/3  Iteration 293/534 Training loss: 2.7821 0.6916 sec/batch\n",
      "Epoch 2/3  Iteration 294/534 Training loss: 2.7806 0.9906 sec/batch\n",
      "Epoch 2/3  Iteration 295/534 Training loss: 2.7792 1.0838 sec/batch\n",
      "Epoch 2/3  Iteration 296/534 Training loss: 2.7780 0.8867 sec/batch\n",
      "Epoch 2/3  Iteration 297/534 Training loss: 2.7770 0.8004 sec/batch\n",
      "Epoch 2/3  Iteration 298/534 Training loss: 2.7756 0.8523 sec/batch\n",
      "Epoch 2/3  Iteration 299/534 Training loss: 2.7746 0.9353 sec/batch\n",
      "Epoch 2/3  Iteration 300/534 Training loss: 2.7733 1.0176 sec/batch\n",
      "Epoch 2/3  Iteration 301/534 Training loss: 2.7721 0.8873 sec/batch\n",
      "Epoch 2/3  Iteration 302/534 Training loss: 2.7709 0.8932 sec/batch\n",
      "Epoch 2/3  Iteration 303/534 Training loss: 2.7697 1.0283 sec/batch\n",
      "Epoch 2/3  Iteration 304/534 Training loss: 2.7684 0.9979 sec/batch\n",
      "Epoch 2/3  Iteration 305/534 Training loss: 2.7673 1.0736 sec/batch\n",
      "Epoch 2/3  Iteration 306/534 Training loss: 2.7663 1.2617 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [128,256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002, 0.001]:\n",
    "            log_string = 'logs/4/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            writer = tf.summary.FileWriter(log_string)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlathit that if had so\n",
      "like it that it were. He could not trouble to his wife, and there was\n",
      "anything in them of the side of his weaky in the creature at his forteren\n",
      "to him.\n",
      "\n",
      "\"What is it? I can't bread to those,\" said Stepan Arkadyevitch. \"It's not\n",
      "my children, and there is an almost this arm, true it mays already,\n",
      "and tell you what I have say to you, and was not looking at the peasant,\n",
      "why is, I don't know him out, and she doesn't speak to me immediately, as\n",
      "you would say the countess and the more frest an angelembre, and time and\n",
      "things's silent, but I was not in my stand that is in my head. But if he\n",
      "say, and was so feeling with his soul. A child--in his soul of his\n",
      "soul of his soul. He should not see that any of that sense of. Here he\n",
      "had not been so composed and to speak for as in a whole picture, but\n",
      "all the setting and her excellent and society, who had been delighted\n",
      "and see to anywing had been being troed to thousand words on them,\n",
      "we liked him.\n",
      "\n",
      "That set in her money at the table, he came into the party. The capable\n",
      "of his she could not be as an old composure.\n",
      "\n",
      "\"That's all something there will be down becime by throe is\n",
      "such a silent, as in a countess, I should state it out and divorct.\n",
      "The discussion is not for me. I was that something was simply they are\n",
      "all three manshess of a sensitions of mind it all.\"\n",
      "\n",
      "\"No,\" he thought, shouted and lifting his soul. \"While it might see your\n",
      "honser and she, I could burst. And I had been a midelity. And I had a\n",
      "marnief are through the countess,\" he said, looking at him, a chosing\n",
      "which they had been carried out and still solied, and there was a sen that\n",
      "was to be completely, and that this matter of all the seconds of it, and\n",
      "a concipation were to her husband, who came up and conscaously, that he\n",
      "was not the station. All his fourse she was always at the country,,\n",
      "to speak oft, and though they were to hear the delightful throom and\n",
      "whether they came towards the morning, and his living and a coller and\n",
      "hold--the children. \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnt him oste wha sorind thans tout thint asd an sesand an hires on thime sind thit aled, ban thand and out hore as the ter hos ton ho te that, was tis tart al the hand sostint him sore an tit an son thes, win he se ther san ther hher tas tarereng,.\n",
      "\n",
      "Anl at an ades in ond hesiln, ad hhe torers teans, wast tar arering tho this sos alten sorer has hhas an siton ther him he had sin he ard ate te anling the sosin her ans and\n",
      "arins asd and ther ale te tot an tand tanginge wath and ho ald, so sot th asend sat hare sother horesinnd, he hesense wing ante her so tith tir sherinn, anded and to the toul anderin he sorit he torsith she se atere an ting ot hand and thit hhe so the te wile har\n",
      "ens ont in the sersise, and we he seres tar aterer, to ato tat or has he he wan ton here won and sen heren he sosering, to to theer oo adent har herere the wosh oute, was serild ward tous hed astend..\n",
      "\n",
      "I's sint on alt in har tor tit her asd hade shithans ored he talereng an soredendere tim tot hees. Tise sor and \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard as astice her said he celatice of to seress in the raice, and to be the some and sere allats to that said to that the sark and a cast a the wither ald the pacinesse of her had astition, he said to the sount as she west at hissele. Af the cond it he was a fact onthis astisarianing.\n",
      "\n",
      "\n",
      "\"Or a ton to to be that's a more at aspestale as the sont of anstiring as\n",
      "thours and trey.\n",
      "\n",
      "The same wo dangring the\n",
      "raterst, who sore and somethy had ast out an of his book. \"We had's beane were that, and a morted a thay he had to tere. Then to\n",
      "her homent andertersed his his ancouted to the pirsted, the soution for of the pirsice inthirgest and stenciol, with the hard and and\n",
      "a colrice of to be oneres,\n",
      "the song to this anderssad.\n",
      "The could ounterss the said to serom of\n",
      "soment a carsed of sheres of she\n",
      "torded\n",
      "har and want in their of hould, but\n",
      "her told in that in he tad a the same to her. Serghing an her has and with the seed, and the camt ont his about of the\n",
      "sail, the her then all houg ant or to hus to \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
