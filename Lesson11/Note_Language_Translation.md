# Transer Learning

## Reference

- [Adam Geitgey's Tutorial](https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa)
- [TensorFlow Tutorial: seq2seq](https://www.tensorflow.org/tutorials/seq2seq)
- [NVidia Tutorial](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/)
- [Microsoft Tutorial](https://www.youtube.com/watch?v=vxibD6VaOfI)
- [Neural Monkey Tutorial](http://neural-monkey.readthedocs.io/en/latest/machine_translation.html)
- [Attention and Memory](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp)
- [Multiligual Neural with Tensorflow](https://blog.altoros.com/enabling-multilingual-neural-machine-translation-with-tensorflow.html)
- [Quora entry](https://www.quora.com/How-can-I-build-a-machine-translation-system)
- [Attention in NLP](https://blog.heuritech.com/2016/01/20/attention-mechanism/)
- [Google's Neural Machine Translation](https://smerity.com/articles/2016/google_nmt_arch.html)

## Statistical Translation Machine 

p(English | Chinese) = p(English) x p(Chinese | English) / p(Chinese)

## Encoder - decoder model

